{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv('./Dataset/Channel_Logs/channel2Logs.csv')\n",
    "\n",
    "# Split in half without shuffling\n",
    "mid_index = len(df) // 2\n",
    "first_half = df.iloc[:mid_index]\n",
    "second_half = df.iloc[mid_index:]\n",
    "\n",
    "# Save to new CSV files\n",
    "first_half.to_csv('./Dataset/Channel_Logs/channel2Logs.csv_part1.csv', index=False)\n",
    "second_half.to_csv('./Dataset/Channel_Logs/channel2Logs.csv_part2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes AWS to True if on SageMaker Instance and set S3 BUCKET and Key accordingly\n",
    "AWS = False\n",
    "REGION = 'eu-west-1'\n",
    "BUCKET = 'ml-can-ids-logs'\n",
    "s3 = None\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import feature_selection\n",
    "from feature_selection import feature_selection_preparation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "reload(feature_selection)\n",
    "\n",
    "# Adjust pandas display optionss\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # No wrapping, long rows won't be cut off\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content (especially useful for long strings)\n",
    "\n",
    "\n",
    "# Remove this after testing/debugging\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS:\n",
    "    s3 = get_s3_client(REGION, BUCKET, True)\n",
    "    \n",
    "    # Get S3 Object \n",
    "    channel2logs = s3.get_object(Bucket = BUCKET, Key= 'channel2Logs.csv')\n",
    "    dos_attack_channel2 = s3.get_object(Bucket = BUCKET, Key= 'dos_attack_channel2.csv')\n",
    "    replay_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'replay_attack_channel2.csv') \n",
    "    spoofing_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'new_spoofing_attack_channel2.csv') \n",
    "\n",
    "    channel2logs = channel2logs['Body'].read().decode('utf-8')\n",
    "    dos_attack_channel2 = dos_attack_channel2['Body'].read().decode('utf-8')\n",
    "    replay_attack_channel2 = replay_attack_channel2['Body'].read().decode('utf-8')\n",
    "    spoofing_attack_channel2 = spoofing_attack_channel2['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Get Content\n",
    "    channel2logs = StringIO(channel2logs)\n",
    "    dos_attack_channel2 = StringIO(dos_attack_channel2)\n",
    "    replay_attack_channel2 = StringIO(replay_attack_channel2)\n",
    "    spoofing_attack_channel2 = StringIO(spoofing_attack_channel2)\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = dos_attack_channel2 # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = replay_attack_channel2 # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = spoofing_attack_channel2 # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel2_csv_path = channel2logs # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = \"\" # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = \"\" # Green Channel\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"\"\n",
    "else:\n",
    "    # Unprocessed Normal and Attack Data\n",
    "    preprocessed_normal_csv_path = './Dataset/Tw22206_L003_with_ecu_channel.csv'  # Normal Unprocessed\n",
    "    preprocessed_DoS_csv_path = './Dataset/Attack_Logs/dos_attack.csv'  # Dos Unprocessed\n",
    "    preprocessed_Fuzzy_csv_path = './Dataset/Attack_Logs/fuzzy_attack.csv'  # Fuzzy Unprocessed\n",
    "    preprocessed_Replay_csv_path = './Dataset/Attack_Logs/replay_attack.csv'  # Replay Unprocessed - Test\n",
    "    preprocessed_Spoofing_csv_path = './Dataset/Attack_Logs/spoofing_attack.csv'  # Spoofing Unprocessed\n",
    "    preprocessed_Suspension_csv_path = './Dataset/Attack_Logs/suspension_attack.csv'  # Suspension Unprocessed - Hardest Attack Type\n",
    "\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = './Dataset/Attack_Logs/dos_attack_channel2.csv'  # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = './Dataset/Attack_Logs/replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Suspension_channel2_csv_path = './Dataset/Attack_Logs/suspension_attack_channel2.csv'  # Suspension on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_new_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/new_spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_parkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB__spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel0_csv_path = './Dataset/Channel_Logs/channel0Logs.csv'\n",
    "    preprocessed_normal_channel2_csv_path = './Dataset/Channel_Logs/channel2Logs.csv'  # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = './Dataset/Channel_Logs/channel4Logs.csv'  # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = './Dataset/Channel_Logs/channel5Logs.csv'  # Green Channel\n",
    "\n",
    "\n",
    "    # Preprocessed Dataframe Data\n",
    "    processeddataframe_normal_csv_path = './Dataset/Processed_Dataframes/train_dataframe.csv'  # Normal CSV Dataframe (Turns Lists into Strings)\n",
    "    processeddataframe_DoS_csv_path = './Dataset/Processed_Dataframes/test_DoS_dataframe.csv'  # DoS CSV Dataframe (Turns Lists into Strings)\n",
    "\n",
    "    # Preprocessed Pickle Data\n",
    "    processeddataframe_normal_pickle_path = './Dataset/Processed_Dataframes/train_Normal_dataframePickle.pkl'  # Normal Pickle Dataframe\n",
    "    processeddataframe_DoS_pickle_path = './Dataset/Processed_Dataframes/test_DoS_dataframePickle.pkl'  # DoS Pickle Dataframe\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"./Resources/Models/SOA_VAE_E6_LD38_EP30_NT100000_B1024_I42.keras\"\n",
    "\n",
    "\n",
    "\n",
    "    # PRELOAD Dataframe for Debug\n",
    "    DEBUG = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############START#####################\n",
      "Average time for ID calculations: 0.000043 seconds\n",
      "Average time for payload AVERAGE calculations: 0.000014 seconds\n",
      "Average time for payload CHANGE calculations: 0.000023 seconds\n",
      "Average time for Entropy per ID calculations: 0.000019 seconds\n",
      "Compute Temporal Features completed 4.05 seconds\n",
      "Feature Selection completed in 10.27 seconds\n",
      "#############START#####################\n",
      "Raw type values: ['R' 'T']\n",
      "Normal entries in 'type' column : 35333\n",
      "Anomalies in 'type' column: 4667\n",
      "Average time for ID calculations: 0.000043 seconds\n",
      "Average time for payload AVERAGE calculations: 0.000010 seconds\n",
      "Average time for payload CHANGE calculations: 0.000021 seconds\n",
      "Average time for Entropy per ID calculations: 0.000020 seconds\n",
      "Compute Temporal Features completed 3.83 seconds\n",
      "Feature Selection completed in 9.77 seconds\n"
     ]
    }
   ],
   "source": [
    "LOAD_DATAFRAME = False\n",
    "TS_FRESH = False\n",
    "\n",
    "n_rows_train = 40000    # select how many rows to load. None if whole train datasset\n",
    "n_rows_test = 40000   # select how many rows to load. None if whole test datasset\n",
    "batch_size = 1024\n",
    "window_size = 50    # increase window size\n",
    "stride = 1         # increase stride as a buffer\n",
    "split_ratio = 0.8     # % of training data to use for training\n",
    "window_anomaly_ratio = 0.1   # For 1 anomaly per window do: 1 / window_size\n",
    "\n",
    "# Preprocess and load training data\n",
    "processeddataframe = feature_selection_preparation(preprocessed_normal_channel2_csv_path, 'training', rows=n_rows_train)\n",
    "\n",
    "#processeddataframe_test = feature_selection_preparation(preprocessed_new_Spoofing_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "processeddataframe_test = feature_selection_preparation(preprocessed_new_Spoofing_channel2_csv_path, 'test', rows=n_rows_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bit_0  bit_1  bit_2  bit_3  bit_4  bit_5  bit_6  bit_7  bit_8  bit_9  \\\n",
      "0      0      1      1      0      0      1      1      1      1      1   \n",
      "1      0      1      1      0      0      1      1      1      1      1   \n",
      "2      0      1      1      0      0      1      1      1      1      1   \n",
      "\n",
      "   bit_10  bit_11  bit_12  bit_13  bit_14  bit_15  bit_16  bit_17  bit_18  \\\n",
      "0       1       1       1       0       1       1       1       0       1   \n",
      "1       1       1       1       0       1       1       0       0       0   \n",
      "2       1       1       1       1       1       1       0       1       1   \n",
      "\n",
      "   bit_19  bit_20  bit_21  bit_22  bit_23  bit_24  bit_25  bit_26  bit_27  \\\n",
      "0       0       0       0       0       1       0       0       1       1   \n",
      "1       1       1       0       0       1       0       0       1       1   \n",
      "2       1       0       0       0       1       0       0       1       1   \n",
      "\n",
      "   bit_28  timestamp  arbitration_id  channel  dlc                     data  \\\n",
      "0       1   0.000000       218068007        2    8  fc ff ff ff 03 ff ff ff   \n",
      "1       1   0.000007       218063655        2    8  0c f3 07 ff 40 ff ff cf   \n",
      "2       1   0.000014       218099239        2    8  ff 0f fc ff ff ff ff ff   \n",
      "\n",
      "                         ecu   data[0]   data[1]   data[2]  data[3]   data[4]  \\\n",
      "0  {'GW_K', 'Coordinator_K'}  0.988235  1.000000  1.000000      1.0  0.011765   \n",
      "1  {'GW_K', 'Coordinator_K'}  0.047059  0.952941  0.027451      1.0  0.250980   \n",
      "2          {'Coordinator_K'}  1.000000  0.058824  0.988235      1.0  1.000000   \n",
      "\n",
      "   data[5]  data[6]   data[7]  payload_entropy  msg_frequency  \\\n",
      "0      1.0      1.0  1.000000         0.353759       0.047619   \n",
      "1      1.0      1.0  0.811765         0.801880       0.047619   \n",
      "2      1.0      1.0  1.000000         0.353759       0.047619   \n",
      "\n",
      "   entropy_average  entropy_bit_change  entropy_id  entropy_id_std  \\\n",
      "0         0.333333                 0.0         0.0             0.0   \n",
      "1         0.333333                 1.0         0.0             0.0   \n",
      "2         0.333333                 1.0         0.0             0.0   \n",
      "\n",
      "   entropy_id_z_score  \\\n",
      "0                 0.0   \n",
      "1                 0.0   \n",
      "2                 0.0   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                         features  \n",
      "0                                                                     [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0.9882352941176471, 1.0, 1.0, 1.0, 0.011764705882352941, 1.0, 1.0, 1.0, 0.047619047619047616, 0.0, 0.353759374819711, 0.3333333333333333, 0.0, 0.0]  \n",
      "1  [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0.047058823529411764, 0.9529411764705882, 0.027450980392156862, 1.0, 0.25098039215686274, 1.0, 1.0, 0.8117647058823529, 0.047619047619047616, 6.792544799433253e-06, 0.8018796874098554, 0.3333333333333333, 0.0, 0.0]  \n",
      "2                                                  [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1.0, 0.058823529411764705, 0.9882352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 0.047619047619047616, 1.3680759243878127e-05, 0.353759374819711, 0.3333333333333333, 0.0, 0.0]  \n",
      "------------------------------\n",
      "0    [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0.9882352941176471, 1.0, 1.0, 1.0, 0.011764705882352941, 1.0, 1.0, 1.0, 0.047619047619047616, 0.0, 0.353759374819711, 0.3333333333333333, 0.0, 0.0]\n",
      "Name: features, dtype: object\n",
      "------------------------------\n",
      "0    [1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0.8117647058823529, 0.5058823529411764, 0.30196078431372547, 0.9019607843137255, 0.25098039215686274, 0.4588235294117647, 0.996078431372549, 0.8980392156862745, 0.09090909090909091, 0.0, 1.0000000000000009, 1.0, 0.0, 0.0]\n",
      "Name: features, dtype: object\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(processeddataframe.head(3))\n",
    "print(\"------------------------------\")\n",
    "\n",
    "print(processeddataframe['features'].head(1))\n",
    "print(\"------------------------------\")\n",
    "print(processeddataframe_test['features'].head(1))\n",
    "print(\"------------------------------\")\n",
    "#only_attack_df = processeddataframe_test[processeddataframe_test['type'] == 1]\n",
    "#only_attack_df = only_attack_df['features']\n",
    "#print(only_attack_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean and STD Between Normal and Attack Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mean Difference  Std Difference  Abs Mean Difference\n",
      "37         0.145030        0.021424             0.145030\n",
      "40        -0.140575        0.090427             0.140575\n",
      "23        -0.078950       -0.017097             0.078950\n",
      "26        -0.067550       -0.002347             0.067550\n",
      "42        -0.065846       -0.156513             0.065846\n",
      "25         0.044925        0.014467             0.044925\n",
      "22         0.043650        0.014972             0.043650\n",
      "28        -0.042925        0.015410             0.042925\n",
      "16        -0.037825       -0.003627             0.037825\n",
      "41        -0.037646        0.016759             0.037646\n",
      "39        -0.037474       -0.003801             0.037474\n",
      "13         0.036450        0.008849             0.036450\n",
      "18        -0.036175        0.001240             0.036175\n",
      "17        -0.035675        0.008937             0.035675\n",
      "10         0.028625       -0.011310             0.028625\n",
      "24         0.028025        0.019529             0.028025\n",
      "14         0.024775        0.002479             0.024775\n",
      "11         0.023375       -0.006958             0.023375\n",
      "9          0.023375       -0.006957             0.023375\n",
      "4          0.022625        0.014967             0.022625\n",
      "15         0.021300       -0.001062             0.021300\n",
      "27        -0.020575        0.019255             0.020575\n",
      "34         0.019564        0.001941             0.019564\n",
      "0          0.019200        0.003785             0.019200\n",
      "33         0.018163        0.007074             0.018163\n",
      "20        -0.018050        0.001882             0.018050\n",
      "38        -0.015218        0.006875             0.015218\n",
      "2         -0.014775        0.004006             0.014775\n",
      "19         0.013375        0.004010             0.013375\n",
      "35         0.012465        0.013042             0.012465\n",
      "29         0.011155        0.005286             0.011155\n",
      "12         0.009050       -0.000527             0.009050\n",
      "21         0.008975        0.014881             0.008975\n",
      "32         0.008513        0.011043             0.008513\n",
      "8         -0.005350        0.012264             0.005350\n",
      "1         -0.004425        0.011355             0.004425\n",
      "31         0.004086        0.009903             0.004086\n",
      "7         -0.003525        0.010208             0.003525\n",
      "36         0.003473        0.014095             0.003473\n",
      "30        -0.002458        0.009266             0.002458\n",
      "6         -0.000950        0.005398             0.000950\n",
      "5         -0.000850        0.005075             0.000850\n",
      "3          0.000000        0.000000             0.000000\n",
      "\n",
      "Feature Index Mapping:\n",
      "0-28:  Arbitration ID\n",
      "29-36: Payload\n",
      "37:  Message Frequency\n",
      "38:  Timestamp\n",
      "39:  Payload Entropy\n",
      "40: Entropy Average over 2 ms\n",
      "41: Entropy per ID\n",
      "42: Entropy per ID STD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'features' column to DataFrame\n",
    "normal_features_df = pd.DataFrame(processeddataframe['features'].tolist())\n",
    "attack_features_df = pd.DataFrame(processeddataframe_test['features'].tolist())\n",
    "\n",
    "# Compute mean difference\n",
    "mean_diff = normal_features_df.mean() - attack_features_df.mean()\n",
    "\n",
    "# Compute standard deviation difference\n",
    "std_diff = normal_features_df.std() - attack_features_df.std()\n",
    "\n",
    "# Create a DataFrame with mean and std differences\n",
    "diff_df = pd.DataFrame({\n",
    "    'Mean Difference': mean_diff,\n",
    "    'Std Difference': std_diff\n",
    "})\n",
    "\n",
    "# Compute absolute mean difference and sort by it\n",
    "diff_df['Abs Mean Difference'] = diff_df['Mean Difference'].abs()\n",
    "diff_df = diff_df.sort_values(by='Abs Mean Difference', ascending=False)\n",
    "\n",
    "# Print results\n",
    "print(diff_df)\n",
    "print(\"\\nFeature Index Mapping:\")\n",
    "print(\"0-28:  Arbitration ID\")\n",
    "print(\"29-36: Payload\")\n",
    "print(\"37:  Message Frequency\")\n",
    "print(\"38:  Timestamp\")\n",
    "print(\"39:  Payload Entropy\")\n",
    "print(\"40: Entropy Average over 2 ms\")\n",
    "print(\"41: Entropy per ID\")\n",
    "print(\"42: Entropy per ID STD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mean Difference  Std of Difference  Abs Mean Difference\n",
      "37         0.145030           0.057814             0.145030\n",
      "40        -0.140575           0.146190             0.140575\n",
      "23        -0.078950           0.027866             0.078950\n",
      "26        -0.067550           0.025696             0.067550\n",
      "42        -0.065846           0.023277             0.065846\n",
      "25         0.044925           0.021967             0.044925\n",
      "22         0.043650           0.018443             0.043650\n",
      "28        -0.042925           0.016762             0.042925\n",
      "16        -0.037825           0.016278             0.037825\n",
      "41        -0.037646           0.024716             0.037646\n",
      "39        -0.037474           0.015354             0.037474\n",
      "13         0.036450           0.020931             0.036450\n",
      "18        -0.036175           0.015969             0.036175\n",
      "17        -0.035675           0.021027             0.035675\n",
      "10         0.028625           0.014750             0.028625\n",
      "24         0.028025           0.019087             0.028025\n",
      "14         0.024775           0.018555             0.024775\n",
      "9          0.023375           0.019747             0.023375\n",
      "11         0.023375           0.023644             0.023375\n",
      "4          0.022625           0.012226             0.022625\n",
      "15         0.021300           0.017286             0.021300\n",
      "27        -0.020575           0.013149             0.020575\n",
      "34         0.019564           0.017910             0.019564\n",
      "0          0.019200           0.020579             0.019200\n",
      "33         0.018163           0.012869             0.018163\n",
      "20        -0.018050           0.015561             0.018050\n",
      "38        -0.015218           0.008920             0.015218\n",
      "2         -0.014775           0.021250             0.014775\n",
      "19         0.013375           0.022484             0.013375\n",
      "35         0.012465           0.013121             0.012465\n",
      "29         0.011155           0.014230             0.011155\n",
      "12         0.009050           0.013322             0.009050\n",
      "21         0.008975           0.007851             0.008975\n",
      "32         0.008513           0.013689             0.008513\n",
      "8         -0.005350           0.012365             0.005350\n",
      "1         -0.004425           0.003586             0.004425\n",
      "31         0.004086           0.012989             0.004086\n",
      "7         -0.003525           0.006851             0.003525\n",
      "36         0.003473           0.016530             0.003473\n",
      "30        -0.002458           0.015586             0.002458\n",
      "6         -0.000950           0.006676             0.000950\n",
      "5         -0.000850           0.006038             0.000850\n",
      "3          0.000000           0.000000             0.000000\n",
      "\n",
      "Feature Index Mapping:\n",
      "0-28:  Arbitration ID\n",
      "29-36: Payload\n",
      "37:  Message Frequency\n",
      "38:  Timestamp\n",
      "39:  Payload Entropy\n",
      "40: Entropy Average over 2 ms\n",
      "41: Entropy per ID\n",
      "42: Entropy per ID STD\n"
     ]
    }
   ],
   "source": [
    "# Convert 'features' column to DataFrame\n",
    "normal_features_df = pd.DataFrame(processeddataframe['features'].tolist())\n",
    "attack_features_df = pd.DataFrame(processeddataframe_test['features'].tolist())\n",
    "\n",
    "#attack_features_df = pd.DataFrame(only_attack_df.tolist())\n",
    "\n",
    "# Define window size\n",
    "window_size = 200\n",
    "num_windows = min(len(normal_features_df) // window_size, len(attack_features_df) // window_size)\n",
    "\n",
    "# Store results for each window\n",
    "window_results = []\n",
    "\n",
    "for i in range(num_windows):\n",
    "    # Select windowed data\n",
    "    normal_window = normal_features_df.iloc[i * window_size: (i + 1) * window_size]\n",
    "    attack_window = attack_features_df.iloc[i * window_size: (i + 1) * window_size]\n",
    "\n",
    "    # Compute mean difference for the window\n",
    "    mean_diff = normal_window.mean() - attack_window.mean()\n",
    "\n",
    "    # Store results\n",
    "    window_results.append(mean_diff)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "window_diff_df = pd.DataFrame(window_results)\n",
    "\n",
    "# Compute overall statistics (mean and standard deviation of differences across windows)\n",
    "final_diff_df = pd.DataFrame({\n",
    "    'Mean Difference': window_diff_df.mean(),\n",
    "    'Std of Difference': window_diff_df.std()\n",
    "})\n",
    "\n",
    "# Compute absolute mean difference and sort\n",
    "final_diff_df['Abs Mean Difference'] = final_diff_df['Mean Difference'].abs()\n",
    "final_diff_df = final_diff_df.sort_values(by='Abs Mean Difference', ascending=False)\n",
    "\n",
    "# Print results\n",
    "print(final_diff_df)\n",
    "print(\"\\nFeature Index Mapping:\")\n",
    "print(\"0-28:  Arbitration ID\")\n",
    "print(\"29-36: Payload\")\n",
    "print(\"37:  Message Frequency\")\n",
    "print(\"38:  Timestamp\")\n",
    "print(\"39:  Payload Entropy\")\n",
    "print(\"40: Entropy Average over 2 ms\")\n",
    "print(\"41: Entropy per ID\")\n",
    "print(\"42: Entropy per ID STD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode_integers(ids, num_bits):\n",
    "    \"\"\"\n",
    "    Converts a list of CAN IDs (integers) to binary representation.\n",
    "\n",
    "    Args:\n",
    "        ids (list of int): List of CAN IDs as integers (e.g., [452948266, 452946218]).\n",
    "        num_bits (int): Number of bits to represent the IDs in binary format.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of binary representations, where each row is a binary vector.\n",
    "    \"\"\"\n",
    "    binary_ids = []\n",
    "    for id_int in ids:\n",
    "        # Convert integer to binary and pad with leading zeros\n",
    "        binary_vector = [int(bit) for bit in f\"{id_int:0{num_bits}b}\"]\n",
    "        binary_ids.append(binary_vector)\n",
    "    return np.array(binary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Integers [218068007, 218063655, 218099239]\n",
      "Binary [[0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1]\n",
      " [0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1]\n",
      " [0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "binary = [218068007, 218063655, 218099239]\n",
    "\n",
    "print(\"Original Integers\", binary)\n",
    "print(\"Binary\" , binary_encode_integers(binary, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Integers [218068007, 218063655, 218099239]\n",
      "Binary [[0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1]\n",
      " [0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1]\n",
      " [0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "binary = [218068007, 218063655, 218099239]\n",
    "\n",
    "print(\"Original Integers\", binary)\n",
    "print(\"Binary\" , binary_encode_integers(binary, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'HCRL_CarHacking'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mastype(dtypes_train)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Save to Excel\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHCRL_CarHacking/Normal_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'HCRL_CarHacking'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Prepare lists to store parsed data\n",
    "timestamps = []\n",
    "ids = []\n",
    "dlcs = []\n",
    "datas = []\n",
    "\n",
    "# Open and read the file\n",
    "with open('./Dataset/HCRL_CarHacking/Normal_dataset.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        # Use regular expressions to extract fields\n",
    "        match = re.search(r'Timestamp:\\s+([0-9.]+)\\s+ID:\\s+([0-9a-fA-F]+).*DLC:\\s+(\\d+)\\s+(.+)', line)\n",
    "        if match:\n",
    "            timestamp = float(match.group(1))\n",
    "            arbitration_id = int(match.group(2), 16)  # convert hex to int\n",
    "            dlc = int(match.group(3))\n",
    "            data = match.group(4).strip().lower()\n",
    "            # Store results\n",
    "            timestamps.append(timestamp)\n",
    "            ids.append(arbitration_id)\n",
    "            dlcs.append(dlc)\n",
    "            datas.append(data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'arbitration_id': ids,\n",
    "    'dlc': dlcs,\n",
    "    'data': datas\n",
    "})\n",
    "\n",
    "# Optionally specify dtypes\n",
    "dtypes_train = {\n",
    "    'timestamp': float,\n",
    "    'arbitration_id': int,\n",
    "    'dlc': int,\n",
    "    'data': str,\n",
    "}\n",
    "df = df.astype(dtypes_train)\n",
    "\n",
    "# Save to Excel\n",
    "df.to_csv('./Dataset/HCRL_CarHacking/Normal_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

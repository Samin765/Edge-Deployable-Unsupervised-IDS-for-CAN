{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a57642",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545506f7",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "861000fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "❌ Using CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Changes AWS to True if on SageMaker Instance and set S3 BUCKET and Key accordingly\n",
    "AWS = False\n",
    "REGION = 'eu-west-1'\n",
    "BUCKET = 'ml-can-ids-logs'\n",
    "s3 = None\n",
    "\n",
    "# Import Functions\n",
    "import setuptools.dist\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "import anomaly_detection_functions\n",
    "import clustering\n",
    "import feature_selection\n",
    "import train\n",
    "import id_embedding\n",
    "\n",
    "reload(utils)\n",
    "reload(train)\n",
    "reload(anomaly_detection_functions)\n",
    "reload(clustering)\n",
    "reload(feature_selection)\n",
    "reload(id_embedding)\n",
    "\n",
    "from utils import plot_loss_curve, plot_pca, plot_tsne, get_confusion_matrix, get_latent_representations_label, analyze_latent_variance, analyze_kl_divergence, linear_annealing, save_results_to_excel, save_trained_model, get_s3_client, check_dataset\n",
    "from anomaly_detection_functions import get_threshold_from_train, get_threshold_from_test, anomaly_detection, get_anomaly_detection_accuracy, get_mean_variances\n",
    "from clustering import visualize_anomalies, evaluate_anomaly_detector, detect_anomalies_one_class_svm_with_threshold, train_hdbscan_detector, evaluate_hdbscan_detector, detect_anomalies_hdbscan, hdbscan_set_threshold, visualize_results\n",
    "from clustering import prepare_features, train_isolation_forest, train_one_class_svm, detect_anomalies_isolation_forest, detect_anomalies_one_class_svm\n",
    "from feature_selection import feature_selection_preparation, convert_to_tensorflow, feature_selection_preparation_new\n",
    "from train import train_model, train_model_factor , train_model_btc, train_model_bernoulli, train_model_semi\n",
    "from id_embedding import train_embedding\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers, Model\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import keras\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from importlib import reload\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "if AWS:\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # No wrapping, long rows won't be cut off\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content (especially useful for long strings)\n",
    "\n",
    "# Remove this after testing/debugging\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  \n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"✅ Using GPU\")\n",
    "    device = \"/GPU:0\"\n",
    "else:\n",
    "    print(\"❌ Using CPU\")\n",
    "    device = \"/CPU:0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715700",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2753d",
   "metadata": {},
   "source": [
    "**PATH FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5bae76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS:\n",
    "    s3 = get_s3_client(REGION, BUCKET, True)\n",
    "    \n",
    "    # Get S3 Object \n",
    "    channel2logs = s3.get_object(Bucket = BUCKET, Key= 'channel2Logs.csv')\n",
    "    dos_attack_channel2 = s3.get_object(Bucket = BUCKET, Key= 'dos_attack_channel2.csv')\n",
    "    replay_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'replay_attack_channel2.csv') \n",
    "    spoofing_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'new_spoofing_attack_channel2.csv') \n",
    "\n",
    "    channel2logs = channel2logs['Body'].read().decode('utf-8')\n",
    "    dos_attack_channel2 = dos_attack_channel2['Body'].read().decode('utf-8')\n",
    "    replay_attack_channel2 = replay_attack_channel2['Body'].read().decode('utf-8')\n",
    "    spoofing_attack_channel2 = spoofing_attack_channel2['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Get Content\n",
    "    channel2logs = StringIO(channel2logs)\n",
    "    dos_attack_channel2 = StringIO(dos_attack_channel2)\n",
    "    replay_attack_channel2 = StringIO(replay_attack_channel2)\n",
    "    spoofing_attack_channel2 = StringIO(spoofing_attack_channel2)\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = dos_attack_channel2 # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = replay_attack_channel2 # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = spoofing_attack_channel2 # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel2_csv_path = channel2logs # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = \"\" # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = \"\" # Green Channel\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"\"\n",
    "else:\n",
    "    # Unprocessed Normal and Attack Data\n",
    "    preprocessed_normal_csv_path = './Dataset/Tw22206_L003_with_ecu_channel.csv'  # Normal Unprocessed\n",
    "    preprocessed_DoS_csv_path = './Dataset/Attack_Logs/dos_attack.csv'  # Dos Unprocessed\n",
    "    preprocessed_Fuzzy_csv_path = './Dataset/Attack_Logs/fuzzy_attack.csv'  # Fuzzy Unprocessed\n",
    "    preprocessed_Replay_csv_path = './Dataset/Attack_Logs/replay_attack.csv'  # Replay Unprocessed - Test\n",
    "    preprocessed_Spoofing_csv_path = './Dataset/Attack_Logs/spoofing_attack.csv'  # Spoofing Unprocessed\n",
    "    preprocessed_Suspension_csv_path = './Dataset/Attack_Logs/suspension_attack.csv'  # Suspension Unprocessed - Hardest Attack Type\n",
    "\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = './Dataset/Attack_Logs/dos_attack_channel2.csv'  # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = './Dataset/Attack_Logs/replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_new_Replay_channel2_csv_path = './Dataset/Attack_Logs/new_replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Suspension_channel2_csv_path = './Dataset/Attack_Logs/suspension_attack_channel2.csv'  # Suspension on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_new_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/new_spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Replay_channel2_csv_path = './Dataset/Attack_Logs/Replay_Attack_Channel2_50K_33s_56s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Replay_channel2_csv_path = './Dataset/Attack_Logs/Replay_Attack_Channel2_200K_33s_169s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_1000k_Replay_channel2_csv_path = './Dataset/Attack_Logs/Replay_Attack_Channel2_1000K_33s_776s.csv'  # Replay on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Replay_channel2_part1_csv_path = './Dataset/Attack_Logs/Train_Replay_Attack_Channel2_50K_33s_56s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Replay_channel2_part1_csv_path = './Dataset/Attack_Logs/Train_Replay_Attack_Channel2_200K_33s_169s.csv'  # Replay on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Replay_channel2_part2_csv_path = './Dataset/Attack_Logs/Test_Replay_Attack_Channel2_50K_33s_56s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Replay_channel2_part2_csv_path = './Dataset/Attack_Logs/Test_Replay_Attack_Channel2_200K_33s_169s.csv'  # Replay on channel 2 (Red Channel)\n",
    "\n",
    "\n",
    "    preprocessed_50K_ParkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB_Spoofing_Attack_Channel2_50K_33s_56s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_200K_ParkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB_Spoofing_Attack_Channel2_200K_33s_169s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_1000k_ParkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB_Spoofing_Attack_Channel2_1000K_33s_775s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Coordinator_K_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/Coordinator_K__Spoofing_Attack_Channel2_50K_33s_56s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Coordinator_K_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/Coordinator_K_Spoofing_Attack_Channel2_200K_33s_169s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_1000K_Coordinator_K_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/Coordinator_K_Spoofing_Attack_Channel2_1000K_33s_776s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel0_csv_path = './Dataset/Channel_Logs/channel0Logs.csv'  \n",
    "    preprocessed_normal_channel2_csv_path = './Dataset/Channel_Logs/channel2Logs.csv'  # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = './Dataset/Channel_Logs/channel4Logs.csv'  # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = './Dataset/Channel_Logs/channel5Logs.csv'  # Green Channel\n",
    "\n",
    "\n",
    "    # Preprocessed Dataframe Data\n",
    "    processeddataframe_normal_csv_path = './Dataset/Processed_Dataframes/train_dataframe.csv'  # Normal CSV Dataframe (Turns Lists into Strings)\n",
    "    processeddataframe_DoS_csv_path = './Dataset/Processed_Dataframes/test_DoS_dataframe.csv'  # DoS CSV Dataframe (Turns Lists into Strings)\n",
    "\n",
    "    # Preprocessed Pickle Data\n",
    "    processeddataframe_normal_pickle_path = './Dataset/Processed_Dataframes/train_Normal_dataframePickle.pkl'  # Normal Pickle Dataframe\n",
    "    processeddataframe_DoS_pickle_path = './Dataset/Processed_Dataframes/test_DoS_dataframePickle.pkl'  # DoS Pickle Dataframe\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"./Resources/Models/SOA_VAE_E6_LD38_EP30_NT100000_B1024_I42.keras\"\n",
    "\n",
    "\n",
    "\n",
    "    # PRELOAD Dataframe for Debug\n",
    "    DEBUG = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff978e0",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "00c046ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############START#####################\n",
      "Amount of unique IDS in training: 96\n",
      "---Training Embedding Network for Arbitration IDs---\n",
      "Epoch [0/100], Loss: 0.9542\n",
      "Epoch [10/100], Loss: 0.3505\n",
      "Epoch [20/100], Loss: 0.0772\n",
      "Epoch [30/100], Loss: 0.0212\n",
      "Epoch [40/100], Loss: 0.0077\n",
      "Epoch [50/100], Loss: 0.0025\n",
      "Epoch [60/100], Loss: 0.0009\n",
      "Epoch [70/100], Loss: 0.0003\n",
      "Epoch [80/100], Loss: 0.0001\n",
      "Epoch [90/100], Loss: 0.0000\n",
      "-- ------------------ ---\n",
      "Average time for ID calculations: 0.000066 seconds\n",
      "Average time for payload AVERAGE calculations: 0.000019 seconds\n",
      "Average time for payload CHANGE calculations: 0.000027 seconds\n",
      "Average time for Entropy per ID calculations: 0.000026 seconds\n",
      "Compute Temporal Features completed 7.00 seconds\n",
      "Feature Selection completed in 17.23 seconds\n",
      "Sliding completed (train) in 0.0042095 seconds\n",
      "-----------------------------------\n",
      "Original window that works:  (1999, 50, 24)\n",
      "Feature shape BEFORE sliding window: (50000, 24)\n",
      "Feature shape AFTER sliding window: (1999, 50, 24)\n",
      "#####################################\n",
      "#############START#####################\n",
      "Raw type values: ['R' 'T']\n",
      "Normal entries in 'type' column : 43710\n",
      "Anomalies in 'type' column: 6290\n",
      "Amount of unique IDS in test: 96\n",
      "Average time for ID calculations: 0.000063 seconds\n",
      "Average time for payload AVERAGE calculations: 0.000024 seconds\n",
      "Average time for payload CHANGE calculations: 0.000028 seconds\n",
      "Average time for Entropy per ID calculations: 0.000024 seconds\n",
      "Compute Temporal Features completed 7.06 seconds\n",
      "Feature Selection completed in 15.81 seconds\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  0\n",
      "Window Anomaly Amount  4\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  7\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  14\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  7\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  14\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  7\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  7\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  8\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  13\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  9\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  12\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  10\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  11\n",
      "Window Anomaly Amount  10\n",
      "Max Amount of Anomalies Found in a Window:  14\n",
      "Sliding Window (test) completed in 0.0235221 seconds\n",
      "-----------------------------------\n",
      "Original window that works:  (1999, 50, 24)\n",
      "Max Amount of anomalies Allowed per window:  5\n",
      "Normals in 'y' array: 800.0\n",
      "Anomalies in 'y' array: 1199.0\n",
      "Feature shape BEFORE sliding window: (50000, 24)\n",
      "Feature shape AFTER sliding window: (1999, 50, 24)\n",
      "#####################################\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "reload(feature_selection)\n",
    "LOAD_DATAFRAME = False\n",
    "BINARY = False\n",
    "BINARY_ID = False\n",
    "\n",
    "n_rows_train = 50000  # select how many rows to load. None if whole train datasset\n",
    "n_rows_test = 50000   # select how many rows to load. None if whole test datasset\n",
    "batch_size = 1024 \n",
    "window_size = 50    # increase window size\n",
    "stride = 25     # increase stride as a buffer\n",
    "split_ratio = 0.8     # % of training data to use for training\n",
    "window_anomaly = 5  # For 1 anomaly per window do: 1 / window_size\n",
    "\n",
    "if LOAD_DATAFRAME:\n",
    "    # Load training data\n",
    "    processeddataframe = pd.read_pickle(processeddataframe_normal_pickle_path)\n",
    "    train_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size)\n",
    "\n",
    "    # Load test data\n",
    "    processeddataframe_test = pd.read_pickle(processeddataframe_DoS_pickle_path)\n",
    "    test_dataset = convert_to_tensorflow(processeddataframe_test['features'] ,processeddataframe_test['type'], batch_size= batch_size )\n",
    "else:\n",
    "    # Preprocess and load training data\n",
    "    processeddataframe , embedding_model, id_to_embedding, scalers = feature_selection_preparation(preprocessed_normal_channel2_csv_path, phase = 'training', rows=n_rows_train, binary = BINARY, binary_id= BINARY_ID)\n",
    "    train_dataset, val_dataset, val_dataset2 = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size, window_size = window_size, stride = stride, split_ratio= split_ratio)\n",
    "\n",
    "    processeddataframe_test = feature_selection_preparation(preprocessed_50K_Replay_channel2_csv_path, phase = 'test', rows=n_rows_test, binary = BINARY, binary_id = BINARY_ID, embedding_model = embedding_model, id_to_embedding = id_to_embedding, scalers = scalers)\n",
    "    test_dataset, test_threshold_dataset = convert_to_tensorflow(processeddataframe_test['features'], processeddataframe_test['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly = window_anomaly)\n",
    "\n",
    "    #processeddataframe_dos_baseline = feature_selection_preparation(preprocessed_DoS_channel2_csv_path, 'test', rows=n_rows_test)\n",
    "    #dos_test_dataset, dos_threshold_dataset = convert_to_tensorflow(processeddataframe_dos_baseline['features'], processeddataframe_dos_baseline['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "467b5ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fc ff ff ff 03 ff ff ff\n",
      "Name: data, dtype: object\n",
      "0    [0.667816162109375, -0.7508273720741272, 0.47768664360046387, 0.5440722703933716, 0.6712583303451538, -0.43409672379493713, -0.16876527667045593, 0.49156415462493896, -0.12904685735702515, 0.48317334055900574, 1.1056145632181635, 1.3877915599023225, 1.1978505680767773, 1.0541955577946325, -1.6219363589487317, 0.9105799921645951, 0.8720520151810353, 1.0705433888591087, -1.2893890807146708, -2.6359245957557174, -1.1275400980447279, -1.1040536771833376, -2.2120424642267524, -0.34865150086139934]\n",
      "1    [-0.9400394558906555, 0.7717521786689758, 0.23855885863304138, -1.1254843473434448, 0.3386957347393036, 0.17023316025733948, 0.16545017063617706, 0.46266013383865356, 0.6282424926757812, -0.48477497696876526, -1.5598046651515993, 1.2618449564047536, -1.3125546255082485, 1.0541955577946325, -1.0112602367907373, 0.9105799921645951, 0.8720520151810353, 0.6318626945072333, -1.2893890807146708, -2.6358997983050587, 0.4839037397645483, -1.1040536771833376, -2.2120424642267524, -0.34865150086139934]\n",
      "2    [0.7150601744651794, -0.6158099174499512, -0.8973087668418884, 0.34999069571495056, -0.9111725091934204, 0.8637804388999939, -0.16336815059185028, -0.10567382723093033, 0.5441594123840332, 0.28008800745010376, 1.1389323035727856, -1.1311405100490581, 1.1674827633156681, 1.0541955577946325, 0.9008568014416714, 0.9105799921645951, 0.8720520151810353, 1.0705433888591087, -1.2893890807146708, -2.6358746515945315, -1.1275400980447279, -1.1040536771833376, -2.2120424642267524, -0.34865150086139934]\n",
      "3       [0.7150601744651794, -0.6158099174499512, -0.8973087668418884, 0.34999069571495056, -0.9111725091934204, 0.8637804388999939, -0.16336815059185028, -0.10567382723093033, 0.5441594123840332, 0.28008800745010376, 1.1389323035727856, -1.1311405100490581, 1.1674827633156681, 1.0541955577946325, 0.9008568014416714, 0.9105799921645951, 0.8720520151810353, 1.0705433888591087, -1.1307001180581318, -2.6350055184119348, -1.1275400980447279, -1.1040536771833376, -1.0129804531586, -0.34865150086139934]\n",
      "Name: features, dtype: object\n",
      "0    [0.667816162109375, -0.7508273720741272, 0.47768664360046387, 0.5440722703933716, 0.6712583303451538, -0.43409672379493713, -0.16876527667045593, 0.49156415462493896, -0.12904685735702515, 0.48317334055900574, 1.1056145632181635, 1.3877915599023225, 1.1978505680767773, 1.0541955577946325, -1.6219363589487317, 0.9105799921645951, 0.8720520151810353, 1.0705433888591087, -1.2893890807146708, -2.6359245957557174, -1.1275400980447279, -1.1040536771833376, -2.2120424642267524, -0.34865150086139934]\n",
      "1    [-0.9400394558906555, 0.7717521786689758, 0.23855885863304138, -1.1254843473434448, 0.3386957347393036, 0.17023316025733948, 0.16545017063617706, 0.46266013383865356, 0.6282424926757812, -0.48477497696876526, -1.5598046651515993, 1.2618449564047536, -1.3125546255082485, 1.0541955577946325, -1.0112602367907373, 0.9105799921645951, 0.8720520151810353, 0.6318626945072333, -1.2893890807146708, -2.6358997983050587, 0.4839037397645483, -1.1040536771833376, -2.2120424642267524, -0.34865150086139934]\n",
      "2    [0.7150601744651794, -0.6158099174499512, -0.8973087668418884, 0.34999069571495056, -0.9111725091934204, 0.8637804388999939, -0.16336815059185028, -0.10567382723093033, 0.5441594123840332, 0.28008800745010376, 1.1389323035727856, -1.1311405100490581, 1.1674827633156681, 1.0541955577946325, 0.9008568014416714, 0.9105799921645951, 0.8720520151810353, 1.0705433888591087, -1.2893890807146708, -2.6358746515945315, -1.1275400980447279, -1.1040536771833376, -2.2120424642267524, -0.34865150086139934]\n",
      "3       [0.7150601744651794, -0.6158099174499512, -0.8973087668418884, 0.34999069571495056, -0.9111725091934204, 0.8637804388999939, -0.16336815059185028, -0.10567382723093033, 0.5441594123840332, 0.28008800745010376, 1.1389323035727856, -1.1311405100490581, 1.1674827633156681, 1.0541955577946325, 0.9008568014416714, 0.9105799921645951, 0.8720520151810353, 1.0705433888591087, -1.1307001180581318, -2.6350055184119348, -1.1275400980447279, -1.1040536771833376, -1.0129804531586, -0.34865150086139934]\n",
      "Name: features, dtype: object\n",
      "24\n",
      "Contains NaN: False\n",
      "Contains values <0 or >1: False\n",
      "Train Dataset - Contains NaN: False\n",
      "Train Dataset - Contains values <0 or >1: True\n",
      "Validation Dataset - Contains NaN: False\n",
      "Validation Dataset - Contains values <0 or >1: True\n"
     ]
    }
   ],
   "source": [
    "# todo: check that all values normalized or there's nans in the sliding windows\n",
    "# todo: check whole processing of df\n",
    "print(processeddataframe['data'].head(1))\n",
    "print(processeddataframe['features'].head(4))\n",
    "print(processeddataframe_test['features'].head(4))\n",
    "print(np.size(processeddataframe['features'][0]))\n",
    "\n",
    "processeddataframe['has_nan'] = processeddataframe['features'].apply(lambda x: any(pd.isna(x)) if isinstance(x, list) else np.nan)\n",
    "valid_lists = [lst for lst in processeddataframe['features'] if isinstance(lst, list)]\n",
    "all_values = sum(valid_lists, [])\n",
    "has_nan = any(pd.isna(all_values))\n",
    "has_out_of_bounds = any(x < 0 or x > 1 for x in all_values if isinstance(x, (int, float)))\n",
    "\n",
    "print(f\"Contains NaN: {has_nan}\")\n",
    "print(f\"Contains values <0 or >1: {has_out_of_bounds}\")\n",
    "\n",
    "# Run checks for both datasets\n",
    "check_dataset(train_dataset, \"Train Dataset\")\n",
    "check_dataset(val_dataset, \"Validation Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70758afc",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6f1cb",
   "metadata": {},
   "source": [
    "**VAE SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ecbbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_Mean_Variance_Decoder\")\n",
    "class VAE_Mean_Variance_Decoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_Mean_Variance_Decoder, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(128, activation = 'relu', return_sequences = True), \n",
    "            #layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "            #layers.TimeDistributed(layers.Dense(input_dim))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        # Split into mean and logvar outputs\n",
    "        self.decoder_mean = layers.TimeDistributed(layers.Dense(input_dim))\n",
    "        self.decoder_logvar = layers.TimeDistributed(layers.Dense(input_dim))\n",
    "\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        #print(\"Z Shape at Decode: \" , z.shape)\n",
    "\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        #print(\"Z_FLAT Shape at Decode: \" , z.shape)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Get mean and logvar\n",
    "        mean_flat = self.decoder_mean(reconstructed_flat)\n",
    "        logvar_flat = self.decoder_logvar(reconstructed_flat)\n",
    "    \n",
    "        # Reshape back to include samples dimension\n",
    "        mean = tf.reshape(mean_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "        logvar = tf.reshape(logvar_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "        \n",
    "        return mean, logvar\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed_mean, reconstructed_logvar = self.decode(z)\n",
    "\n",
    "        #print(\"Reconstructed Mean: \" , reconstructed_mean.shape)\n",
    "        #print(\"Reconstructed Logvar: \" , reconstructed_logvar.shape)\n",
    "\n",
    "\n",
    "        return reconstructed_mean, reconstructed_logvar, mu, logvar\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_Mean_Variance_Decoder, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c54e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamples\")\n",
    "class VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(128, activation = 'relu', return_sequences = True), \n",
    "            #layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "            layers.TimeDistributed(layers.Dense(input_dim))  # Output must match (window_size, input_dim)\n",
    "        ]) \n",
    "        \n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        #print(\"AT ENCODE x shape: \" , x.shape)\n",
    "        #print(\"AT ENCODE hidden shape: \", hidden.shape)\n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        #print(\"AT DECODE: z shape \" , z.shape )\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        #print(\"AT DECODE: z flat shape \" , z_flat.shape )\n",
    "\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        #print(\"AT DECODE: reconstructed flat shape \" , reconstructed_flat.shape )\n",
    "\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        model_outputs = {}\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "\n",
    "            model_outputs['reconstructed'] = None\n",
    "            model_outputs['mu'] = mu\n",
    "            model_outputs['logvar'] = logvar\n",
    "            \n",
    "            return model_outputs\n",
    "        mu, logvar = self.encode(x)\n",
    "        #print(\"AT CALL: mu shape \" , mu.shape )\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        #print(\"RECONSTRUCTED shape: \", reconstructed.shape)\n",
    "\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        print(\"Z shape: \", z.shape)\n",
    "        print(\"z_mean shape: \", z_mean.shape)\n",
    "        print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6797623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_weak_generator\")\n",
    "class VAE_weakGenerator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_weakGenerator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences= True),\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(50, activation='relu', return_sequences= True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        model_outputs = {}\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_weakGenerator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd89b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x\")\n",
    "class VAE_2x(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(50, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(50, activation = 'relu', return_sequences = True), \n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        model_outputs = {}\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e316b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x_weak_generator\")\n",
    "class VAE_2x_weak_generator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x_weak_generator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(50, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            #layers.BatchNormalization(),\n",
    "            layers.LSTM(5, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        model_outputs = {}\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "    \"Disentangling by Factorising\"\n",
    "    (https://arxiv.org/abs/1802.05983)\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "\n",
    "        from: ..\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x_weak_generator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af9b3e",
   "metadata": {},
   "source": [
    "**Bernoulli VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34e9b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"Bernoulli_VAE\")\n",
    "class Bernoulli_VAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size,  bernoulli_prior_p=0.5 ,**kwargs):\n",
    "        super(Bernoulli_VAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "        self.bernoulli_prior_p = bernoulli_prior_p\n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = tf.keras.Sequential([\n",
    "            layers.Dense(latent_dim),\n",
    "            layers.Activation('softplus')\n",
    "        ])\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(128, activation = 'relu', return_sequences = True), \n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "       \n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        eps = tf.random.normal(shape=(tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.sqrt(logvar)  # More stable alternative to exp UNSTABLE FIX\n",
    "        #print(std)\n",
    "        #print(\"mu shape at reparm: \", mu.shape)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z shape at deocde \", z.shape)\n",
    "        reconstructed = self.decoder(z)\n",
    "        #print(\"Reconstructed X at deocde \", reconstructed.shape)\n",
    "        return reconstructed\n",
    "\n",
    "    def call(self, x, n_samples = 1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        #print(\"Reconstructed X after decode and reshape \", reconstructed.shape)\n",
    "\n",
    "        return reconstructed, mu, logvar, z\n",
    "    \n",
    "    def bernoulli_prior_logpdf(self, z):\n",
    "        \"\"\"Calculate log probability of z under a Bernoulli prior\n",
    "        \n",
    "        For continuous latent variables, we use a \"relaxed\" Bernoulli prior:\n",
    "        p(z) ~ Bernoulli(p) where z is \"pushed\" toward 0 or 1\n",
    "        \"\"\"\n",
    "        # Compute the log probability of z under a Bernoulli-like prior\n",
    "        # p(z) ~ Bernoulli(p) where p is self.bernoulli_prior_p\n",
    "        # For each dimension, we compute the probability based on how close z is to 0 or 1\n",
    "        \n",
    "        # Component that favors values close to 0\n",
    "        log_prob_0 = tf.math.log(1 - self.bernoulli_prior_p + 1e-10) - tf.square(z) / 0.1\n",
    "        \n",
    "        # Component that favors values close to 1\n",
    "        log_prob_1 = tf.math.log(self.bernoulli_prior_p + 1e-10) - tf.square(z - 1) / 0.1\n",
    "        \n",
    "        # Combine (use log-sum-exp trick for numerical stability)\n",
    "        max_val = tf.maximum(log_prob_0, log_prob_1)\n",
    "        log_prob = max_val + tf.math.log(\n",
    "            tf.exp(log_prob_0 - max_val) + tf.exp(log_prob_1 - max_val)\n",
    "        )\n",
    "        \n",
    "        return log_prob\n",
    "    #####################################################################################\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(Bernoulli_VAE, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size,  # Window size\n",
    "            \"bernoulli_prior_p\" : self.bernoulli_prior_p\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'], bernoulli_prior_p = config['bernoulli_prior_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb85df8",
   "metadata": {},
   "source": [
    "**VQ-VAE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88900688",
   "metadata": {},
   "source": [
    "**SEMI SUPERVISED: LR-SEMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "593cc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\"\"\"\n",
    "LR-SEMI is based on the paper:\n",
    "\n",
    "\"Semisupervised anomaly detection of multivariate time series based on a variational autoencoder\"\n",
    "(https://doi.org/10.1007/s10489-022-03829-1)\n",
    "\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package=\"LR_SEMIVAE\")\n",
    "class LR_SEMIVAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size,num_classes, lambda_cls, **kwargs):\n",
    "        super(LR_SEMIVAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_cls = lambda_cls \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            layers.GlobalAveragePooling1D()\n",
    "\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim + 1,)), # + 1 for the z + label concat\n",
    "            layers.RepeatVector(window_size),  \n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "\n",
    "        self.LR_classifier = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(128,)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.Dense(128, activation='tanh'),  # Linear → Tanh\n",
    "            layers.Dense(128, activation='tanh'),  # Final softmax layer? wrong\n",
    "            layers.Dense(128),  \n",
    "            layers.Dense(1, activation = 'sigmoid'),  # Final softmax layer\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        return mu, logvar, hidden\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "\n",
    "    def decode(self, z, y = None, hidden = None):\n",
    "        #print(\"AT DECODE: z shape \" , z.shape )\n",
    "        num_samples = tf.shape(z)[0]\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        #print(\"AT DECODE: z flat shape \" , z_flat.shape )\n",
    "\n",
    "        y_pred = None\n",
    "\n",
    "        if y == None:\n",
    "            #print(\"y is none\")\n",
    "            #print(\"AT DECODE: hidden shape \" , hidden.shape)\n",
    "            y_pred = self.LR_classifier(hidden) \n",
    "            #print(\"AT DECODE: y_pred shape: \", y_pred.shape)\n",
    "            y_pred = tf.cast(y_pred, tf.float32)          # ensure dtype is compatible\n",
    "            #y_pred = tf.expand_dims(y_pred, axis=-1)      # shape: (batch_size, 1)\n",
    "            y_repeated = tf.repeat(y_pred, repeats=num_samples, axis=0)  # shape: (n_samples * batch_size, num_classes)  \n",
    "            #print(\"AT DECODE: y_pred_repeated shape: \", y_repeated.shape)\n",
    "            ## LEFT OFF HERE: How should labels be generated if not provided, check the shapes should probably be: y_pred = [n_samples, batch_size, ] or \n",
    "\n",
    "        else:\n",
    "            #print(\"AT DECODE: y shape \" , y.shape )\n",
    "            y = tf.cast(y, tf.float32)          # ensure dtype is compatible\n",
    "            y = tf.expand_dims(y, axis=-1)      # shape: (batch_size, 1)\n",
    "            y_repeated = tf.repeat(y, repeats=num_samples, axis=0)  # shape: (n_samples * batch_size, num_classes)  \n",
    "\n",
    "\n",
    "     \n",
    "        #print(\"AT DECODE: y repeated shape \" , y_repeated.shape )\n",
    "        z_y_flat = tf.concat([z_flat, y_repeated], axis=-1)  # shape: (n_samples * batch_size, latent_dim + num_classes)        \n",
    "\n",
    "        #print(\"AT DECODE: z_y flat shape \" , z_y_flat.shape )\n",
    "\n",
    "        reconstructed_flat = self.decoder(z_y_flat)\n",
    "        #print(\"AT DECODE: reconstructed_flat shape \" , reconstructed_flat.shape )\n",
    "\n",
    "        z_reshaped = tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "        return z_reshaped, y_pred\n",
    "\n",
    "    def call(self, batch_x, batch_y = None, n_samples=1, latent_only = False):\n",
    "        #print(\"AT CALL batch_X shape: \", batch_x.shape)\n",
    "        model_outputs = {}\n",
    "\n",
    "        if batch_y == None:\n",
    "            y_hat = None\n",
    "            #print(\"AT CALL batch_y = None\")\n",
    "        else:\n",
    "            y_hat = batch_y\n",
    "            #print(\"AT CALL batch_y shape: \", batch_y.shape)\n",
    "\n",
    "\n",
    "        mu , logvar, hidden = self.encode(batch_x)\n",
    "        #print(\"AT CALL: mu shape \" , mu.shape )\n",
    "        #print(\"AT CALL: logvar shape \" , logvar.shape )\n",
    "\n",
    "        z= self.reparameterize(mu, logvar, n_samples)\n",
    "        #print(\"AT REPARAM: z shape \" , z.shape )\n",
    "\n",
    "        reconstructed, y_pred = self.decode(z, y_hat, hidden)\n",
    "        #print(\"Reconstruction Shape : \" , reconstructed.shape)\n",
    "\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "        model_outputs['hidden'] = hidden\n",
    "        model_outputs['y_pred'] = y_pred\n",
    "\n",
    "        return model_outputs\n",
    "    \n",
    "    def compute_loss(self , y, reconstruction_loss, hidden = None, y_pred = None, AD = True):\n",
    "        # Determine if data is labeled\n",
    "        losses = {}\n",
    "        is_labeled = y is not None\n",
    "        #print(y_pred)\n",
    "        # Mask for Normal Data\n",
    "        if is_labeled:\n",
    "            a_t = 1.0 - tf.cast(y, tf.float32)  # 1 for normal, 0 for abnormal [batch_size, 1]\n",
    "            #print(\"AT COMPUTE Loss: a_t shape BEFORE squeezed \" , a_t.shape )\n",
    "\n",
    "            # Count the number of zeros and ones in y and a_t\n",
    "            \"\"\"\n",
    "            num_zeros_y = tf.reduce_sum(tf.cast(y == 0, tf.float32))\n",
    "            num_ones_y = tf.reduce_sum(tf.cast(y == 1, tf.float32))\n",
    "            num_zeros_a_t = tf.reduce_sum(tf.cast(a_t == 0, tf.float32))\n",
    "            num_ones_a_t = tf.reduce_sum(tf.cast(a_t == 1, tf.float32))\n",
    "\n",
    "\n",
    "            print(f\"Number of zeros in y: {num_zeros_y.numpy()}\")\n",
    "            print(f\"Number of ones in y: {num_ones_y.numpy()}\")\n",
    "            print(f\"Number of zeros in a_t: {num_zeros_a_t.numpy()}\")\n",
    "            print(f\"Number of ones in y: {num_ones_a_t.numpy()}\")\"\n",
    "            \"\"\"\n",
    "            kappa = tf.reduce_mean(a_t, axis = -1 , keepdims = True) #[batch_size , ]\n",
    "\n",
    "        else:\n",
    "            if y_pred is not None:\n",
    "                y_pred = tf.cast(y_pred > 0.5, tf.float32)  # binarize probs\n",
    "\n",
    "                a_t = 1.0 - tf.cast(y_pred, tf.float32)  # 1 for normal, 0 for abnormal [batch_size, 1]\n",
    "                a_t = tf.squeeze(a_t, axis=-1)  # Now a_t shape is (1024,) [batch_size,]\n",
    "                #print(\"AT COMPUTE Loss y_pred is not none: a_t shape \" , a_t.shape )\n",
    "                \n",
    "                \"\"\"\n",
    "                num_zeros_y = tf.reduce_sum(tf.cast(y_pred == 0, tf.float32))\n",
    "                num_ones_y = tf.reduce_sum(tf.cast(y_pred == 1, tf.float32))\n",
    "                num_zeros_a_t = tf.reduce_sum(tf.cast(a_t == 0, tf.float32))\n",
    "                num_ones_a_t = tf.reduce_sum(tf.cast(a_t == 1, tf.float32))\n",
    "\n",
    "                \n",
    "                print(f\"Number of zeros in y: {num_zeros_y.numpy()}\")\n",
    "                print(f\"Number of ones in y: {num_ones_y.numpy()}\")\n",
    "                print(f\"Number of zeros in a_t: {num_zeros_a_t.numpy()}\")\n",
    "                print(f\"Number of ones in y: {num_ones_a_t.numpy()}\")\n",
    "                \"\"\"\n",
    "                kappa = tf.reduce_mean(a_t, axis = -1 , keepdims = True) #[batch_size , ]\n",
    "                #print(kappa)\n",
    "                \n",
    "        #print(\"AT COMPUTE loss: reconstruction loss before masking\" , reconstruction_loss.shape)\n",
    "        masked_recon_loss_batch = reconstruction_loss * a_t\n",
    "        #print(\"AT COMPUTE Loss: masked_recon_loss \" , masked_recon_loss.shape )\n",
    "\n",
    "        masked_recon_loss = tf.reduce_mean(masked_recon_loss_batch)\n",
    "        # 3. Classification Loss (for labeled data only)\n",
    "        classification_loss = 0.0\n",
    "        if is_labeled and self.lambda_cls > 0:\n",
    "        # Use the latent representation to predict the label\n",
    "            #z_mean = tf.reduce_mean(z, axis=0)  # Average over samples\n",
    "            y_pred = self.LR_classifier(hidden) \n",
    "            y_pred = tf.cast(y_pred, tf.float32)          # ensure dtype is compatible\n",
    "            y_logits = y_pred\n",
    "            y_logits = tf.squeeze(y_logits, axis=-1)  # Now a_t shape is (1024,) [batch_size,]\n",
    "\n",
    "            \n",
    "            #print(\"AT COMPUTE Loss: y \" , y.shape )\n",
    "            #print(\"AT COMPUTE Loss: y_logits \" , y_logits.shape )\n",
    "\n",
    "            #y_logits = self.LR_classifier(z_mean)\n",
    "            # Binary cross-entropy loss for classification\n",
    "            classification_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.binary_crossentropy(\n",
    "                    y, \n",
    "                    y_logits,\n",
    "                    from_logits=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if AD:\n",
    "            losses['classification_loss'] = classification_loss * self.lambda_cls\n",
    "            losses['masked_recon_loss'] = masked_recon_loss_batch\n",
    "            losses['kappa'] = kappa\n",
    "        else: \n",
    "            losses['classification_loss'] = classification_loss * self.lambda_cls\n",
    "            losses['masked_recon_loss'] = masked_recon_loss\n",
    "            losses['kappa'] = kappa\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LR_SEMIVAE, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'window_size': self.window_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'lambda_cls': self.lambda_cls\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863771e",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9a54",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:  200\n",
      "Input Dimension = 24, Latent Dimension = 10, Beta = 3, Validation Method = B_TCVAE, Rows in Training Data = 50000, Batch Size = 1024 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4228\\2279989477.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcosine_decay_schedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mdiscriminator_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate_disc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model_semi(vae_semi,optimizer,discriminator_optimizer, epochs,\n\u001b[0m\u001b[0;32m     49\u001b[0m                                                                     \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                                                                       \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_critic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_anneal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta_tc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                                                                       \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\Desktop\\MasterThesis\\github_USE_THIS!!\\MasterThesis\\train.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(vae, optimizer, discriminator_optimizer, epochs, n_samples, input_dim, latent_dim, batch_size, beta, gamma, n_critic, steps_anneal, patience, time, beta_tc, validation_method, model_path, train_dataset, test_dataset, val_dataset, n_rows_train, AWS, s3, BUCKET)\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0manneal_coeff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_annealing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_anneal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;31m#print(len(batch))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[1;31m# Train VAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m                 \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_only\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use multiple samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                 \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reconstructed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                 \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                 )\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             )\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4228\\4038888999.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch_x, batch_y, n_samples, latent_only)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m#print(\"AT REPARAM: z shape \" , z.shape )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mreconstructed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m#print(\"Reconstruction Shape : \" , reconstructed.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mmodel_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reconstructed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreconstructed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4228\\4038888999.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, z, y, hidden)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mz_y_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_repeated\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# shape: (n_samples * batch_size, latent_dim + num_classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m#print(\"AT DECODE: z_y flat shape \" , z_y_flat.shape )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mreconstructed_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_y_flat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;31m#print(\"AT DECODE: reconstructed_flat shape \" , reconstructed_flat.shape )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mz_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                 )\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             )\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\sequential.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# Fallback: Just apply the layer sequence.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;31m# This typically happens if `inputs` is a nested struct.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_keras_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         outputs = self._run_through_graph(\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         )\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\function.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcall_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_has_training_arg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m         ):\n\u001b[0;32m    636\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                 )\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             )\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         return super().call(\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    398\u001b[0m         self._maybe_config_dropout_masks(\n\u001b[0;32m    399\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         )\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         last_output, outputs, states = self.inner_loop(\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    575\u001b[0m                 \u001b[1;34m\"but cuDNN is not supported for this layer configuration \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m                 \u001b[1;34m\"with this backend. Pass use_cudnn='auto' to fallback \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[1;34m\"to a non-cuDNN implementation.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             )\n\u001b[1;32m--> 579\u001b[1;33m         return super().inner_loop(\n\u001b[0m\u001b[0;32m    580\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         return backend.rnn(\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[0minitial_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_new_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 )\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             final_outputs = tf.while_loop(\n\u001b[0m\u001b[0;32m    429\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mwhile_loop_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m                   \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m                   \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m                   ('after %s' % date), instructions)\n\u001b[1;32m--> 660\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m    237\u001b[0m   \u001b[1;33m...\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m   \u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m   \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m   return while_loop(\n\u001b[0m\u001b[0;32m    242\u001b[0m       \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[0;32m    486\u001b[0m                                               list(loop_vars))\n\u001b[0;32m    487\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(i, lv)\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m    407\u001b[0m                     \u001b[0mTuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_ta_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \"\"\"\n\u001b[0;32m    409\u001b[0m                 \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m                 output, new_states = step_function(\n\u001b[0m\u001b[0;32m    412\u001b[0m                     \u001b[0mcurrent_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconstants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 )\n\u001b[0;32m    414\u001b[0m                 \u001b[0mflat_new_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(inputs, states)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcell_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                 )\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             )\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, states, training)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[0mdp_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dropout_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdp_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_tm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3812\u001b[0m     \"\"\"\n\u001b[0;32m   3813\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3814\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3815\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx1_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx2_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mx2_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mx1_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, grad_a, grad_b, name)\u001b[0m\n\u001b[0;32m   3674\u001b[0m             \u001b[0mgrad_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3675\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3676\u001b[0m         )\n\u001b[0;32m   3677\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3678\u001b[1;33m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3679\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3680\u001b[0m             \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3681\u001b[0m             \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, grad_a, grad_b, name)\u001b[0m\n\u001b[0;32m   6932\u001b[0m         transpose_b, \"grad_a\", grad_a, \"grad_b\", grad_b)\n\u001b[0;32m   6933\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6934\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6935\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6936\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6937\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6938\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6939\u001b[0m       return mat_mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "reload(train)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "epochs = 100\n",
    "n_samples = 11\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "learning_rate_disc = 5e-5 # FactorVAE\n",
    "\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "print(\"steps: \", steps_anneal) \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"B_TCVAE\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 10\n",
    "beta = 3 # 50\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = True\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"LSTM_VAE\"\n",
    "#model_path s= f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "#vae = Bernoulli_VAE(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size, bernoulli_prior_p= 0.5)\n",
    "#vae = VAE_Mean_Variance_Decoder(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "vae_semi = LR_SEMIVAE(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size, num_classes= 2, lambda_cls = 1)\n",
    "\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model_semi(vae_semi,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= \"\", train_dataset = test_dataset,test_dataset = test_dataset,\n",
    "                                                                      val_dataset = test_threshold_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "reducer = None\n",
    "reducer = get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False, reducer = reducer)\n",
    "reducer = get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs, name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = True\n",
    "  latent_AD = False\n",
    "  reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "  results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "  reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010100",
   "metadata": {},
   "source": [
    "**HyperParamter Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 100\n",
    "n_samples = 8\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5 # increase this\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"B_VAE\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 2  \n",
    "beta = 20 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "latent_dims = [30, 70]  # Example values for latent dimension\n",
    "beta_values = [50,100]\n",
    "\n",
    "it = 0\n",
    "reconstruction_AD = True\n",
    "latent_AD = False\n",
    "# Iterate over all combinations\n",
    "for latent_dim, beta in itertools.product(latent_dims, beta_values):\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"BASE_LSTM_VAE\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "    print(f\"Training with: latent_dim={latent_dim}, beta={beta} validation_method={validation_method}\")\n",
    "\n",
    "    vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_weakGen = VAE_weakGenerator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_2x = VAE_2x(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    VAE_2x_weakGen = VAE_2x_weak_generator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    optimizer_weakGen = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    optimizer_2x = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    optimizer_2x_weakGen = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "    print(\"VAE--------------\")\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae, optimizer\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE WEAK GENERATOR-------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_weakGen = train_model(vae_weakGen,optimizer_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    del vae_weakGen, optimizer_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X----------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_2x = train_model(vae_2x,optimizer_2x,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_2x, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae_2x, optimizer_2x\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X WEAK GENERATOR------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, VAE_2x_weakGen = train_model(VAE_2x_weakGen,optimizer_2x_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "   \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(VAE_2x_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type='TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del VAE_2x_weakGen, optimizer_2x_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dea3cc",
   "metadata": {},
   "source": [
    "**Iterative Training on Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular HYPERPARAMETERS \n",
    "#input_dim = np.size(processeddataframe['features'][0])\n",
    "input_dim = 42\n",
    "epochs = 50\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 30  \n",
    "beta = 0 # 20\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"BEST_VAE\"\n",
    "new_model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_{model_name}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = keras.models.load_model(best_model_path)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=weight_decay)\n",
    "optimizer = vae.optimizer\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path = new_model_path)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time, show_val= show_val)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = True)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = False\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path, reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e510e",
   "metadata": {},
   "source": [
    "**Test Saved Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55000e",
   "metadata": {},
   "source": [
    "Latent Dimension = 38, Beta = 30, Gamma = 0, N_critic = 0, Beta_TC = 0, Validation Method = PLOT, Rows in Training Data = 100000, Batch Size = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./Resources/Models/BtcVAE_EPOCHS72_LD100_BETA50_NT100000_INPUT24_15-32.keras\"\n",
    "load_vae = keras.models.load_model(best_model_path)\n",
    "load_vae.trainable = False  # Freeze model weights\n",
    "reducer = None\n",
    "reducer = get_latent_representations_label(load_vae, train_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "reducer = get_latent_representations_label(load_vae, val_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "reducer = get_latent_representations_label(load_vae, test_threshold_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "reducer = get_latent_representations_label(load_vae, test_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2b60f203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon error from val 42636.04296875\n",
      "recon error from val 7855.74169921875\n",
      "recon error from val 1679.299072265625\n",
      "recon error from val 107.33773803710938\n",
      "recon error from val 71216.8984375\n",
      "recon error from val 2933.420654296875\n",
      "recon error from val 5.838082313537598\n",
      "recon error from val 13714.828125\n",
      "recon error from val 6423.71337890625\n",
      "recon error from val 2594.149658203125\n",
      "recon error from val 284.3049621582031\n",
      "recon error from val 66610.03125\n",
      "recon error from val 16350.3017578125\n",
      "recon error from val 6176.951171875\n",
      "recon error from val 598.3072509765625\n",
      "recon error from val 228.57666015625\n",
      "recon error from val 611.2199096679688\n",
      "recon error from val 754.2393798828125\n",
      "recon error from val 3668.3779296875\n",
      "recon error from val 49535.5625\n",
      "recon error from val 13359.2802734375\n",
      "recon error from val 127.39331817626953\n",
      "recon error from val 1641.744384765625\n",
      "recon error from val 49103.6796875\n",
      "recon error from val 27236.162109375\n",
      "recon error from val 4629.50537109375\n",
      "recon error from val 14.687899589538574\n",
      "recon error from val 179411.875\n",
      "recon error from val 13006.8916015625\n",
      "recon error from val 1137.585205078125\n",
      "recon error from val 293.6429443359375\n",
      "recon error from val 389784.25\n",
      "recon error from val 10212.783203125\n",
      "recon error from val 6703.9462890625\n",
      "recon error from val 2406.326416015625\n",
      "recon error from val 39750.375\n",
      "recon error from val 17117.814453125\n",
      "recon error from val 23709.1015625\n",
      "recon error from val 2179.036865234375\n",
      "recon error from val 8.669015884399414\n",
      "recon error from val 44900.03515625\n",
      "recon error from val 16010.8251953125\n",
      "recon error from val 566.8065185546875\n",
      "recon error from val 16445.388671875\n",
      "recon error from val 146419.609375\n",
      "recon error from val 19588.580078125\n",
      "recon error from val 2813.875732421875\n",
      "recon error from val 2607.79296875\n",
      "recon error from val 1469.7677001953125\n",
      "recon error from val 2266.745849609375\n",
      "recon error from val 6233.7822265625\n",
      "recon error from val 183799.28125\n",
      "recon error from val 64352.98046875\n",
      "recon error from val 7539.97998046875\n",
      "recon error from val 6508.11865234375\n",
      "recon error from val 13.754629135131836\n",
      "recon error from val 4396.0693359375\n",
      "recon error from val 57662.01171875\n",
      "recon error from val 15522.375\n",
      "recon error from val 64.72488403320312\n",
      "recon error from val 108064.6875\n",
      "recon error from val 8455.4755859375\n",
      "recon error from val 2832.331787109375\n",
      "recon error from val 3289.21337890625\n",
      "recon error from val 662745.125\n",
      "recon error from val 45015.3828125\n",
      "recon error from val 12163.9404296875\n",
      "recon error from val 1703.3638916015625\n",
      "recon error from val 8128.09814453125\n",
      "recon error from val 4469.326171875\n",
      "recon error from val 10512.083984375\n",
      "recon error from val 770.2706909179688\n",
      "recon error from val 1308.908447265625\n",
      "recon error from val 1749.7904052734375\n",
      "recon error from val 492.7146911621094\n",
      "recon error from val 48689.52734375\n",
      "recon error from val 13030.8720703125\n",
      "recon error from val 19918.5\n",
      "recon error from val 2412.884521484375\n",
      "recon error from val 47467.30078125\n",
      "recon error from val 35789.1015625\n",
      "recon error from val 5069.79345703125\n",
      "recon error from val 5.525006294250488\n",
      "recon error from val 5458.58447265625\n",
      "recon error from val 17932.255859375\n",
      "recon error from val 7962.9599609375\n",
      "recon error from val 228.48951721191406\n",
      "recon error from val 794481.625\n",
      "recon error from val 361.5321350097656\n",
      "recon error from val 545.2045288085938\n",
      "recon error from val 321273.84375\n",
      "recon error from val 267191.375\n",
      "recon error from val 16674.2421875\n",
      "recon error from val 2368.69384765625\n",
      "recon error from val 36261.84765625\n",
      "recon error from val 70131.71875\n",
      "recon error from val 7295.7353515625\n",
      "recon error from val 14060.19140625\n",
      "recon error from val 558345.75\n",
      "recon error from val 78094.4453125\n",
      "recon error from val 31224.689453125\n",
      "recon error from val 26924.23046875\n",
      "recon error from val 348.7178955078125\n",
      "recon error from val 170629.78125\n",
      "recon error from val 108341.34375\n",
      "recon error from val 14832.791015625\n",
      "recon error from val 962.7975463867188\n",
      "recon error from val 295048.90625\n",
      "recon error from val 32186.65234375\n",
      "recon error from val 9195.865234375\n",
      "recon error from val 2184.78369140625\n",
      "recon error from val 2214649.75\n",
      "recon error from val 229030.546875\n",
      "recon error from val 48782.5703125\n",
      "recon error from val 3124.787353515625\n",
      "recon error from val 444.0266418457031\n",
      "recon error from val 289136.6875\n",
      "recon error from val 69088.375\n",
      "recon error from val 16615.505859375\n",
      "recon error from val 481.8753967285156\n",
      "recon error from val 458902.9375\n",
      "recon error from val 35471.546875\n",
      "recon error from val 1951.7396240234375\n",
      "recon error from val 2266.642333984375\n",
      "recon error from val 237325.234375\n",
      "recon error from val 64698.84375\n",
      "recon error from val 6829.83935546875\n",
      "recon error from val 45774.80859375\n",
      "recon error from val 209321.265625\n",
      "recon error from val 24774.642578125\n",
      "recon error from val 66554.1875\n",
      "recon error from val 13300.1142578125\n",
      "recon error from val 128679.671875\n",
      "recon error from val 95342.5625\n",
      "recon error from val 49121.64453125\n",
      "recon error from val 80.0294189453125\n",
      "recon error from val 173901.453125\n",
      "recon error from val 113835.234375\n",
      "recon error from val 32503.80078125\n",
      "recon error from val 1.393441915512085\n",
      "recon error from val 25462.50390625\n",
      "recon error from val 40971.828125\n",
      "recon error from val 204.0730743408203\n",
      "recon error from val 554452.6875\n",
      "recon error from val 37889.65234375\n",
      "recon error from val 28722.00390625\n",
      "recon error from val 692.6825561523438\n",
      "recon error from val 818154.5\n",
      "recon error from val 3877.300048828125\n",
      "recon error from val 871.6275634765625\n",
      "recon error from val 1146.6685791015625\n",
      "recon error from val 526085.375\n",
      "recon error from val 54047.21875\n",
      "recon error from val 9661.400390625\n",
      "recon error from val 103347.84375\n",
      "recon error from val 644.9369506835938\n",
      "recon error from val 6412.9833984375\n",
      "recon error from val 9775.763671875\n",
      "recon error from val 2698773.75\n",
      "recon error from val 508970.53125\n",
      "recon error from val 143285.28125\n",
      "recon error from val 56483.24609375\n",
      "recon error from val 52157.375\n",
      "recon error from val 353574.375\n",
      "recon error from val 263884.59375\n",
      "recon error from val 36120.71875\n",
      "recon error from val 455.8289794921875\n",
      "recon error from val 1300777.25\n",
      "recon error from val 351080.59375\n",
      "recon error from val 3187.327392578125\n",
      "recon error from val 33696.1015625\n",
      "recon error from val 3749773.25\n",
      "recon error from val 266927.0\n",
      "recon error from val 18049.72265625\n",
      "recon error from val 11053.98046875\n",
      "recon error from val 169783.59375\n",
      "recon error from val 85600.625\n",
      "recon error from val 67877.8125\n",
      "recon error from val 10590.7529296875\n",
      "recon error from val 22697.60546875\n",
      "recon error from val 667277.9375\n",
      "recon error from val 69662.625\n",
      "recon error from val 728.0731201171875\n",
      "recon error from val 1655794.875\n",
      "recon error from val 308381.15625\n",
      "recon error from val 95245.484375\n",
      "recon error from val 6518.798828125\n",
      "recon error from val 518894.53125\n",
      "recon error from val 108707.703125\n",
      "recon error from val 7018.73828125\n",
      "recon error from val 10454.0478515625\n",
      "recon error from val 6060165.0\n",
      "recon error from val 996320.375\n",
      "recon error from val 108500.1953125\n",
      "recon error from val 109624.734375\n",
      "recon error from val 12433.2578125\n",
      "recon error from val 563705.625\n",
      "recon error from val 195831.71875\n",
      "recon error from val 55659.796875\n",
      "recon error from val 880.8380126953125\n",
      "########## Reconstruction Thhresholds ##########\n",
      "Normal Reconstruction threshold: 978036.4779765\n",
      "Normal Recon Probability threshold: 11736459.1796218\n",
      "########################################\n",
      "Get Thresold from Train completed in 1.1004 seconds\n",
      "Normal reconstruction:  67745299300352.0\n",
      "Normal Probability:  812944094920704.0\n",
      "Normal reconstruction:  66169688031232.0\n",
      "Normal Probability:  794035702726656.0\n",
      "Normal reconstruction:  53781584674816.0\n",
      "Normal Probability:  645379167092736.0\n",
      "Normal reconstruction:  47841145782272.0\n",
      "Normal Probability:  574094789574656.0\n",
      "Normal reconstruction:  69057160151040.0\n",
      "Normal Probability:  828685955366912.0\n",
      "Normal reconstruction:  71734929653760.0\n",
      "Normal Probability:  860818618974208.0\n",
      "Normal reconstruction:  54181436063744.0\n",
      "Normal Probability:  650177115324416.0\n",
      "Normal reconstruction:  68082877857792.0\n",
      "Normal Probability:  816995658366976.0\n",
      "Normal reconstruction:  69469871276032.0\n",
      "Normal Probability:  833639327727616.0\n",
      "Normal reconstruction:  64996373430272.0\n",
      "Normal Probability:  779954652446720.0\n",
      "Normal reconstruction:  48477379756032.0\n",
      "Normal Probability:  581728087310336.0\n",
      "Normal reconstruction:  68198011502592.0\n",
      "Normal Probability:  818375013957632.0\n",
      "Normal reconstruction:  70105740345344.0\n",
      "Normal Probability:  841270343761920.0\n",
      "Normal reconstruction:  65158755909632.0\n",
      "Normal Probability:  781905104470016.0\n",
      "Normal reconstruction:  46873599541248.0\n",
      "Normal Probability:  562483244826624.0\n",
      "Normal reconstruction:  76635697053696.0\n",
      "Normal Probability:  919628800851968.0\n",
      "Normal reconstruction:  49948481552384.0\n",
      "Normal Probability:  599380470005760.0\n",
      "Normal reconstruction:  48978557140992.0\n",
      "Normal Probability:  587742316593152.0\n",
      "Normal reconstruction:  56940847693824.0\n",
      "Normal Probability:  683290709196800.0\n",
      "Normal reconstruction:  69416993685504.0\n",
      "Normal Probability:  833004008112128.0\n",
      "Normal reconstruction:  68787629981696.0\n",
      "Normal Probability:  825449831727104.0\n",
      "Normal reconstruction:  49619702644736.0\n",
      "Normal Probability:  595435005673472.0\n",
      "Normal reconstruction:  74615678304256.0\n",
      "Normal Probability:  895387535671296.0\n",
      "Normal reconstruction:  51003676164096.0\n",
      "Normal Probability:  612044650840064.0\n",
      "Normal reconstruction:  77455624765440.0\n",
      "Normal Probability:  929467698511872.0\n",
      "Normal reconstruction:  71614351802368.0\n",
      "Normal Probability:  859372557172736.0\n",
      "Normal reconstruction:  55110352764928.0\n",
      "Normal Probability:  661324635832320.0\n",
      "Normal reconstruction:  73033964322816.0\n",
      "Normal Probability:  876408008081408.0\n",
      "Normal reconstruction:  58642451660800.0\n",
      "Normal Probability:  703709453484032.0\n",
      "Normal reconstruction:  63159784177664.0\n",
      "Normal Probability:  757917712121856.0\n",
      "Normal reconstruction:  58163223068672.0\n",
      "Normal Probability:  697958693601280.0\n",
      "Normal reconstruction:  59321610141696.0\n",
      "Normal Probability:  711858818383872.0\n",
      "Normal reconstruction:  49277149642752.0\n",
      "Normal Probability:  591325594386432.0\n",
      "Normal reconstruction:  48162861481984.0\n",
      "Normal Probability:  577954153234432.0\n",
      "Normal reconstruction:  68153182781440.0\n",
      "Normal Probability:  817838545698816.0\n",
      "Normal reconstruction:  66471900217344.0\n",
      "Normal Probability:  797661460430848.0\n",
      "Normal reconstruction:  48946718179328.0\n",
      "Normal Probability:  587361205354496.0\n",
      "Normal reconstruction:  47672174051328.0\n",
      "Normal Probability:  572065685962752.0\n",
      "Normal reconstruction:  68606532517888.0\n",
      "Normal Probability:  823279463956480.0\n",
      "Normal reconstruction:  48165021548544.0\n",
      "Normal Probability:  577979453276160.0\n",
      "Normal reconstruction:  47389012393984.0\n",
      "Normal Probability:  568668769484800.0\n",
      "Normal reconstruction:  47996980953088.0\n",
      "Normal Probability:  575964509634560.0\n",
      "Normal reconstruction:  68995847815168.0\n",
      "Normal Probability:  827951113306112.0\n",
      "Normal reconstruction:  73807729524736.0\n",
      "Normal Probability:  885692989177856.0\n",
      "Normal reconstruction:  71732731838464.0\n",
      "Normal Probability:  860792916279296.0\n",
      "Normal reconstruction:  41191617855488.0\n",
      "Normal Probability:  494299330379776.0\n",
      "Normal reconstruction:  68230328614912.0\n",
      "Normal Probability:  818764513804288.0\n",
      "Normal reconstruction:  69579959173120.0\n",
      "Normal Probability:  834959895953408.0\n",
      "Normal reconstruction:  73085059334144.0\n",
      "Normal Probability:  877019906703360.0\n",
      "Normal reconstruction:  73945688571904.0\n",
      "Normal Probability:  887347491110912.0\n",
      "Normal reconstruction:  68643845046272.0\n",
      "Normal Probability:  823726341881856.0\n",
      "Normal reconstruction:  48655025307648.0\n",
      "Normal Probability:  583860404355072.0\n",
      "Normal reconstruction:  47700049395712.0\n",
      "Normal Probability:  572400559194112.0\n",
      "Normal reconstruction:  68498550161408.0\n",
      "Normal Probability:  821982585159680.0\n",
      "Normal reconstruction:  65211344093184.0\n",
      "Normal Probability:  782536531771392.0\n",
      "Normal reconstruction:  47279792717824.0\n",
      "Normal Probability:  567356522758144.0\n",
      "Normal reconstruction:  50202429882368.0\n",
      "Normal Probability:  602429359915008.0\n",
      "Normal reconstruction:  68795842428928.0\n",
      "Normal Probability:  825550226587648.0\n",
      "Normal reconstruction:  70318508998656.0\n",
      "Normal Probability:  843820816138240.0\n",
      "Normal reconstruction:  70613599256576.0\n",
      "Normal Probability:  847362285109248.0\n",
      "Normal reconstruction:  47341822279680.0\n",
      "Normal Probability:  568101229821952.0\n",
      "Normal reconstruction:  49387384340480.0\n",
      "Normal Probability:  592647639007232.0\n",
      "Normal reconstruction:  47770614366208.0\n",
      "Normal Probability:  573248412581888.0\n",
      "Normal reconstruction:  46735120400384.0\n",
      "Normal Probability:  560821595799552.0\n",
      "Normal reconstruction:  47179334942720.0\n",
      "Normal Probability:  566152321302528.0\n",
      "Normal reconstruction:  69181340909568.0\n",
      "Normal Probability:  830176577454080.0\n",
      "Normal reconstruction:  48169685614592.0\n",
      "Normal Probability:  578037234008064.0\n",
      "Normal reconstruction:  68750426505216.0\n",
      "Normal Probability:  825005302611968.0\n",
      "Normal reconstruction:  49114368704512.0\n",
      "Normal Probability:  589372592226304.0\n",
      "Normal reconstruction:  46917232885760.0\n",
      "Normal Probability:  563006190649344.0\n",
      "Normal reconstruction:  72489107456000.0\n",
      "Normal Probability:  869869792788480.0\n",
      "Normal reconstruction:  46957032636416.0\n",
      "Normal Probability:  563484207087616.0\n",
      "Normal reconstruction:  58011783528448.0\n",
      "Normal Probability:  696140982910976.0\n",
      "Normal reconstruction:  73445878530048.0\n",
      "Normal Probability:  881349367955456.0\n",
      "Normal reconstruction:  70169602818048.0\n",
      "Normal Probability:  842033572872192.0\n",
      "Normal reconstruction:  69492709261312.0\n",
      "Normal Probability:  833910984409088.0\n",
      "Normal reconstruction:  69905755930624.0\n",
      "Normal Probability:  838869188608000.0\n",
      "Normal reconstruction:  67683433316352.0\n",
      "Normal Probability:  812201870884864.0\n",
      "Normal reconstruction:  73871583608832.0\n",
      "Normal Probability:  886459976384512.0\n",
      "Normal reconstruction:  68059561721856.0\n",
      "Normal Probability:  816714740662272.0\n",
      "Normal reconstruction:  76075774246912.0\n",
      "Normal Probability:  912908586319872.0\n",
      "Normal reconstruction:  56871633289216.0\n",
      "Normal Probability:  682459364589568.0\n",
      "Normal reconstruction:  69209530826752.0\n",
      "Normal Probability:  830515879870464.0\n",
      "Normal reconstruction:  72193660682240.0\n",
      "Normal Probability:  866324028850176.0\n",
      "Normal reconstruction:  69748800880640.0\n",
      "Normal Probability:  836985979666432.0\n",
      "Normal reconstruction:  62559013044224.0\n",
      "Normal Probability:  750707737100288.0\n",
      "Normal reconstruction:  78049487880192.0\n",
      "Normal Probability:  936594659868672.0\n",
      "Normal reconstruction:  51559031373824.0\n",
      "Normal Probability:  618709701885952.0\n",
      "Normal reconstruction:  49604468932608.0\n",
      "Normal Probability:  595253811740672.0\n",
      "Normal reconstruction:  75668129841152.0\n",
      "Normal Probability:  908018229182464.0\n",
      "Normal reconstruction:  67793865146368.0\n",
      "Normal Probability:  813526398533632.0\n",
      "Normal reconstruction:  68945058988032.0\n",
      "Normal Probability:  827341295058944.0\n",
      "Normal reconstruction:  68118915317760.0\n",
      "Normal Probability:  817427235471360.0\n",
      "Normal reconstruction:  66906862125056.0\n",
      "Normal Probability:  802883134029824.0\n",
      "Normal reconstruction:  75880017690624.0\n",
      "Normal Probability:  910558836555776.0\n",
      "Normal reconstruction:  46652438085632.0\n",
      "Normal Probability:  559828988592128.0\n",
      "Normal reconstruction:  46452042629120.0\n",
      "Normal Probability:  557425350410240.0\n",
      "Normal reconstruction:  46766602846208.0\n",
      "Normal Probability:  561199016050688.0\n",
      "Normal reconstruction:  81353743794176.0\n",
      "Normal Probability:  976244321550336.0\n",
      "Normal reconstruction:  75818151706624.0\n",
      "Normal Probability:  909818156023808.0\n",
      "Normal reconstruction:  71067825602560.0\n",
      "Normal Probability:  852814075002880.0\n",
      "Normal reconstruction:  55110046580736.0\n",
      "Normal Probability:  661320676409344.0\n",
      "Normal reconstruction:  45683633553408.0\n",
      "Normal Probability:  548203585863680.0\n",
      "Normal reconstruction:  65600160268288.0\n",
      "Normal Probability:  787202476867584.0\n",
      "Normal reconstruction:  69279764447232.0\n",
      "Normal Probability:  831357357916160.0\n",
      "Normal reconstruction:  41147422474240.0\n",
      "Normal Probability:  493768834809856.0\n",
      "Normal reconstruction:  75570880708608.0\n",
      "Normal Probability:  906849796751360.0\n",
      "Normal reconstruction:  79905844887552.0\n",
      "Normal Probability:  958871313055744.0\n",
      "Normal reconstruction:  56226876489728.0\n",
      "Normal Probability:  674720907264000.0\n",
      "Normal reconstruction:  67367040188416.0\n",
      "Normal Probability:  808402703876096.0\n",
      "Normal reconstruction:  50023140163584.0\n",
      "Normal Probability:  600276775993344.0\n",
      "Normal reconstruction:  47482373406720.0\n",
      "Normal Probability:  569788011118592.0\n",
      "Normal reconstruction:  68713193668608.0\n",
      "Normal Probability:  824557686489088.0\n",
      "Normal reconstruction:  73733221908480.0\n",
      "Normal Probability:  884798360911872.0\n",
      "Normal reconstruction:  41044037074944.0\n",
      "Normal Probability:  492529568972800.0\n",
      "Normal reconstruction:  46120541618176.0\n",
      "Normal Probability:  553446264537088.0\n",
      "Normal reconstruction:  50170548977664.0\n",
      "Normal Probability:  602047510478848.0\n",
      "Normal reconstruction:  73229184008192.0\n",
      "Normal Probability:  878749704781824.0\n",
      "Normal reconstruction:  65310849761280.0\n",
      "Normal Probability:  783730666897408.0\n",
      "Normal reconstruction:  51203299868672.0\n",
      "Normal Probability:  614438222692352.0\n",
      "Normal reconstruction:  59574874800128.0\n",
      "Normal Probability:  714898245943296.0\n",
      "Normal reconstruction:  54153602662400.0\n",
      "Normal Probability:  649842242093056.0\n",
      "Normal reconstruction:  68198112165888.0\n",
      "Normal Probability:  818376758788096.0\n",
      "Normal reconstruction:  68524663898112.0\n",
      "Normal Probability:  822296386207744.0\n",
      "Normal reconstruction:  71096925683712.0\n",
      "Normal Probability:  853163376640000.0\n",
      "Normal reconstruction:  70022324027392.0\n",
      "Normal Probability:  840266663591936.0\n",
      "Normal reconstruction:  75610995032064.0\n",
      "Normal Probability:  907333718769664.0\n",
      "Normal reconstruction:  49208727961600.0\n",
      "Normal Probability:  590504584544256.0\n",
      "Normal reconstruction:  76103943192576.0\n",
      "Normal Probability:  913247486083072.0\n",
      "Normal reconstruction:  77890741862400.0\n",
      "Normal Probability:  934688365477888.0\n",
      "Normal reconstruction:  70933834366976.0\n",
      "Normal Probability:  851205475532800.0\n",
      "Normal reconstruction:  48917437743104.0\n",
      "Normal Probability:  587009018036224.0\n",
      "Normal reconstruction:  46679885611008.0\n",
      "Normal Probability:  560158224678912.0\n",
      "Normal reconstruction:  54151174160384.0\n",
      "Normal Probability:  649813318172672.0\n",
      "Normal reconstruction:  49352349319168.0\n",
      "Normal Probability:  592227940171776.0\n",
      "Normal reconstruction:  47349065842688.0\n",
      "Normal Probability:  568190014849024.0\n",
      "Normal reconstruction:  48326418366464.0\n",
      "Normal Probability:  579917154615296.0\n",
      "Normal reconstruction:  70168571019264.0\n",
      "Normal Probability:  842021761712128.0\n",
      "Normal reconstruction:  74629376901120.0\n",
      "Normal Probability:  895552556367872.0\n",
      "Normal reconstruction:  73871348727808.0\n",
      "Normal Probability:  886454473457664.0\n",
      "Normal reconstruction:  67531570151424.0\n",
      "Normal Probability:  810379798118400.0\n",
      "Normal reconstruction:  55547457961984.0\n",
      "Normal Probability:  666570133078016.0\n",
      "Normal reconstruction:  71773810851840.0\n",
      "Normal Probability:  861286770409472.0\n",
      "Normal reconstruction:  75844424826880.0\n",
      "Normal Probability:  910133769011200.0\n",
      "Normal reconstruction:  50237645258752.0\n",
      "Normal Probability:  602852011540480.0\n",
      "Normal reconstruction:  50672921739264.0\n",
      "Normal Probability:  608075698405376.0\n",
      "Normal reconstruction:  77627826110464.0\n",
      "Normal Probability:  931533577781248.0\n",
      "Normal reconstruction:  48705361149952.0\n",
      "Normal Probability:  584464988110848.0\n",
      "Normal reconstruction:  68171151179776.0\n",
      "Normal Probability:  818052622974976.0\n",
      "Normal reconstruction:  72239823192064.0\n",
      "Normal Probability:  866876670345216.0\n",
      "Normal reconstruction:  58725956059136.0\n",
      "Normal Probability:  704710516408320.0\n",
      "Normal reconstruction:  71528729280512.0\n",
      "Normal Probability:  858344113831936.0\n",
      "Normal reconstruction:  49856735346688.0\n",
      "Normal Probability:  598280220180480.0\n",
      "Normal reconstruction:  69321124478976.0\n",
      "Normal Probability:  831854299054080.0\n",
      "Normal reconstruction:  69921136443392.0\n",
      "Normal Probability:  839053603766272.0\n",
      "Normal reconstruction:  69950936973312.0\n",
      "Normal Probability:  839412233535488.0\n",
      "Normal reconstruction:  49106902843392.0\n",
      "Normal Probability:  589283740090368.0\n",
      "Normal reconstruction:  74207790628864.0\n",
      "Normal Probability:  890492279586816.0\n",
      "Normal reconstruction:  68438789718016.0\n",
      "Normal Probability:  821265459838976.0\n",
      "Normal reconstruction:  70497937129472.0\n",
      "Normal Probability:  845974004039680.0\n",
      "Normal reconstruction:  50195593166848.0\n",
      "Normal Probability:  602346212032512.0\n",
      "Normal reconstruction:  77574491340800.0\n",
      "Normal Probability:  930894231633920.0\n",
      "Normal reconstruction:  50763132829696.0\n",
      "Normal Probability:  609157023531008.0\n",
      "Normal reconstruction:  58498868051968.0\n",
      "Normal Probability:  701986232074240.0\n",
      "Normal reconstruction:  68325551898624.0\n",
      "Normal Probability:  819906169798656.0\n",
      "Normal reconstruction:  49446888931328.0\n",
      "Normal Probability:  593362751062016.0\n",
      "Normal reconstruction:  49220295852032.0\n",
      "Normal Probability:  590644170981376.0\n",
      "Normal reconstruction:  49608835203072.0\n",
      "Normal Probability:  595306626416640.0\n",
      "Normal reconstruction:  74648762974208.0\n",
      "Normal Probability:  895784484601856.0\n",
      "Normal reconstruction:  68996019781632.0\n",
      "Normal Probability:  827952119939072.0\n",
      "Normal reconstruction:  75505474732032.0\n",
      "Normal Probability:  906064086171648.0\n",
      "Normal reconstruction:  49307487043584.0\n",
      "Normal Probability:  591691203477504.0\n",
      "Normal reconstruction:  69882494320640.0\n",
      "Normal Probability:  838589545971712.0\n",
      "Normal reconstruction:  70137948405760.0\n",
      "Normal Probability:  841654877552640.0\n",
      "Normal reconstruction:  72184609374208.0\n",
      "Normal Probability:  866215379599360.0\n",
      "Normal reconstruction:  70057019310080.0\n",
      "Normal Probability:  840684550488064.0\n",
      "Normal reconstruction:  79707563360256.0\n",
      "Normal Probability:  956491766956032.0\n",
      "Normal reconstruction:  69906565431296.0\n",
      "Normal Probability:  838878315413504.0\n",
      "Normal reconstruction:  67756586172416.0\n",
      "Normal Probability:  813078648193024.0\n",
      "Normal reconstruction:  78656185565184.0\n",
      "Normal Probability:  943875434741760.0\n",
      "Normal reconstruction:  53716195475456.0\n",
      "Normal Probability:  644594194710528.0\n",
      "Normal reconstruction:  60283556986880.0\n",
      "Normal Probability:  723402616733696.0\n",
      "Normal reconstruction:  49419697258496.0\n",
      "Normal Probability:  593036669091840.0\n",
      "Normal reconstruction:  60025959612416.0\n",
      "Normal Probability:  720311246913536.0\n",
      "Normal reconstruction:  75696676274176.0\n",
      "Normal Probability:  908360215953408.0\n",
      "Normal reconstruction:  68887005626368.0\n",
      "Normal Probability:  826644906377216.0\n",
      "Normal reconstruction:  66028549701632.0\n",
      "Normal Probability:  792343955374080.0\n",
      "Normal reconstruction:  68751219228672.0\n",
      "Normal Probability:  825013557002240.0\n",
      "Normal reconstruction:  67443288440832.0\n",
      "Normal Probability:  809320216264704.0\n",
      "Normal reconstruction:  76617963536384.0\n",
      "Normal Probability:  919414723575808.0\n",
      "Normal reconstruction:  48085585625088.0\n",
      "Normal Probability:  577027178496000.0\n",
      "Normal reconstruction:  56582532497408.0\n",
      "Normal Probability:  678990373191680.0\n",
      "Normal reconstruction:  48742858227712.0\n",
      "Normal Probability:  584915020152832.0\n",
      "Normal reconstruction:  51413254144000.0\n",
      "Normal Probability:  616959771148288.0\n",
      "Normal reconstruction:  48152006623232.0\n",
      "Normal Probability:  577824431800320.0\n",
      "Normal reconstruction:  47537444618240.0\n",
      "Normal Probability:  570450174279680.0\n",
      "Normal reconstruction:  71738368983040.0\n",
      "Normal Probability:  860859488272384.0\n",
      "Normal reconstruction:  72640781877248.0\n",
      "Normal Probability:  871689718071296.0\n",
      "Normal reconstruction:  48563681755136.0\n",
      "Normal Probability:  582763778408448.0\n",
      "Normal reconstruction:  70678233481216.0\n",
      "Normal Probability:  848138197794816.0\n",
      "Normal reconstruction:  68467982073856.0\n",
      "Normal Probability:  821616841850880.0\n",
      "Normal reconstruction:  51819984191488.0\n",
      "Normal Probability:  621839793520640.0\n",
      "Normal reconstruction:  75129715425280.0\n",
      "Normal Probability:  901558397042688.0\n",
      "Normal reconstruction:  71828471021568.0\n",
      "Normal Probability:  861941417377792.0\n",
      "Normal reconstruction:  69581821444096.0\n",
      "Normal Probability:  834980565483520.0\n",
      "Normal reconstruction:  74981161566208.0\n",
      "Normal Probability:  899775247417344.0\n",
      "Normal reconstruction:  74420743831552.0\n",
      "Normal Probability:  893048187781120.0\n",
      "Normal reconstruction:  51329410007040.0\n",
      "Normal Probability:  615952534208512.0\n",
      "Normal reconstruction:  51438625488896.0\n",
      "Normal Probability:  617263908519936.0\n",
      "Normal reconstruction:  69182083301376.0\n",
      "Normal Probability:  830184362082304.0\n",
      "Normal reconstruction:  75213593116672.0\n",
      "Normal Probability:  902563956260864.0\n",
      "Normal reconstruction:  46732285050880.0\n",
      "Normal Probability:  560787202506752.0\n",
      "Normal reconstruction:  72361843884032.0\n",
      "Normal Probability:  868342864805888.0\n",
      "Normal reconstruction:  48158457462784.0\n",
      "Normal Probability:  577902680735744.0\n",
      "Normal reconstruction:  67111795818496.0\n",
      "Normal Probability:  805341868589056.0\n",
      "Normal reconstruction:  68503541383168.0\n",
      "Normal Probability:  822043184463872.0\n",
      "Normal reconstruction:  70711871799296.0\n",
      "Normal Probability:  848543132680192.0\n",
      "Normal reconstruction:  53209615826944.0\n",
      "Normal Probability:  638515071156224.0\n",
      "Normal reconstruction:  74551421566976.0\n",
      "Normal Probability:  894616924585984.0\n",
      "Normal reconstruction:  48743407681536.0\n",
      "Normal Probability:  584921663930368.0\n",
      "Normal reconstruction:  70758176915456.0\n",
      "Normal Probability:  849098726965248.0\n",
      "Normal reconstruction:  75195524055040.0\n",
      "Normal Probability:  902345919561728.0\n",
      "Normal reconstruction:  48970457939968.0\n",
      "Normal Probability:  587643867889664.0\n",
      "Normal reconstruction:  47051316396032.0\n",
      "Normal Probability:  564615125663744.0\n",
      "Normal reconstruction:  48669223026688.0\n",
      "Normal Probability:  584031330631680.0\n",
      "Normal reconstruction:  71395140698112.0\n",
      "Normal Probability:  856743500316672.0\n",
      "Normal reconstruction:  60395582652416.0\n",
      "Normal Probability:  724746941497344.0\n",
      "Normal reconstruction:  69375834980352.0\n",
      "Normal Probability:  832509482893312.0\n",
      "Normal reconstruction:  76833072611328.0\n",
      "Normal Probability:  921996938444800.0\n",
      "Normal reconstruction:  60264858779648.0\n",
      "Normal Probability:  723178674454528.0\n",
      "Normal reconstruction:  72262623428608.0\n",
      "Normal Probability:  867151212707840.0\n",
      "Normal reconstruction:  49189484494848.0\n",
      "Normal Probability:  590272589201408.0\n",
      "Normal reconstruction:  69763720019968.0\n",
      "Normal Probability:  837163549720576.0\n",
      "Normal reconstruction:  73877682126848.0\n",
      "Normal Probability:  886533460590592.0\n",
      "Normal reconstruction:  73998939455488.0\n",
      "Normal Probability:  887986367496192.0\n",
      "Normal reconstruction:  50296738807808.0\n",
      "Normal Probability:  603560010055680.0\n",
      "Normal reconstruction:  52391915290624.0\n",
      "Normal Probability:  628702882824192.0\n",
      "Normal reconstruction:  49158790578176.0\n",
      "Normal Probability:  589904564191232.0\n",
      "Normal reconstruction:  48392768061440.0\n",
      "Normal Probability:  580711924891648.0\n",
      "Normal reconstruction:  74878468227072.0\n",
      "Normal Probability:  898540914081792.0\n",
      "Normal reconstruction:  49249957969920.0\n",
      "Normal Probability:  590998774218752.0\n",
      "Normal reconstruction:  74506936778752.0\n",
      "Normal Probability:  894084617076736.0\n",
      "Normal reconstruction:  50111031803904.0\n",
      "Normal Probability:  601331391791104.0\n",
      "Normal reconstruction:  70669609992192.0\n",
      "Normal Probability:  848034984361984.0\n",
      "Normal reconstruction:  74955995742208.0\n",
      "Normal Probability:  899472116678656.0\n",
      "Normal reconstruction:  50398782029824.0\n",
      "Normal Probability:  604785350803456.0\n",
      "Normal reconstruction:  70195355844608.0\n",
      "Normal Probability:  842345360654336.0\n",
      "Normal reconstruction:  75061524430848.0\n",
      "Normal Probability:  900737789853696.0\n",
      "Normal reconstruction:  48863587074048.0\n",
      "Normal Probability:  586363900526592.0\n",
      "Normal reconstruction:  70048395821056.0\n",
      "Normal Probability:  840579122462720.0\n",
      "Normal reconstruction:  71278320943104.0\n",
      "Normal Probability:  855338911793152.0\n",
      "Normal reconstruction:  53675271651328.0\n",
      "Normal Probability:  644102286737408.0\n",
      "Normal reconstruction:  72637862641664.0\n",
      "Normal Probability:  871653546393600.0\n",
      "Normal reconstruction:  68168236138496.0\n",
      "Normal Probability:  818018934325248.0\n",
      "Normal reconstruction:  73950025482240.0\n",
      "Normal Probability:  887399299153920.0\n",
      "Normal reconstruction:  47298889383936.0\n",
      "Normal Probability:  567587041705984.0\n",
      "Normal reconstruction:  57588876050432.0\n",
      "Normal Probability:  691068156772352.0\n",
      "Normal reconstruction:  67740597485568.0\n",
      "Normal Probability:  812887052386304.0\n",
      "Normal reconstruction:  49764850728960.0\n",
      "Normal Probability:  597178426851328.0\n",
      "Normal reconstruction:  49899232034816.0\n",
      "Normal Probability:  598791589724160.0\n",
      "Normal reconstruction:  78103502127104.0\n",
      "Normal Probability:  937241052446720.0\n",
      "Normal reconstruction:  54437041143808.0\n",
      "Normal Probability:  653244191735808.0\n",
      "Normal reconstruction:  70497542864896.0\n",
      "Normal Probability:  845969440636928.0\n",
      "Normal reconstruction:  49427490275328.0\n",
      "Normal Probability:  593128675344384.0\n",
      "Normal reconstruction:  69202417287168.0\n",
      "Normal Probability:  830429510762496.0\n",
      "Normal reconstruction:  49029723455488.0\n",
      "Normal Probability:  588356228481024.0\n",
      "Normal reconstruction:  48707663822848.0\n",
      "Normal Probability:  584490153934848.0\n",
      "Normal reconstruction:  49656193089536.0\n",
      "Normal Probability:  595872689684480.0\n",
      "Normal reconstruction:  71233542553600.0\n",
      "Normal Probability:  854803181731840.0\n",
      "Normal reconstruction:  68745263316992.0\n",
      "Normal Probability:  824943830892544.0\n",
      "Normal reconstruction:  57893973917696.0\n",
      "Normal Probability:  694727200473088.0\n",
      "Normal reconstruction:  49644503564288.0\n",
      "Normal Probability:  595734780968960.0\n",
      "Normal reconstruction:  49669560336384.0\n",
      "Normal Probability:  596035160244224.0\n",
      "Normal reconstruction:  60251483144192.0\n",
      "Normal Probability:  723018754031616.0\n",
      "Normal reconstruction:  73468804595712.0\n",
      "Normal Probability:  881625923584000.0\n",
      "Normal reconstruction:  79478495641600.0\n",
      "Normal Probability:  953743591866368.0\n",
      "Normal reconstruction:  51747326263296.0\n",
      "Normal Probability:  620968988901376.0\n",
      "Normal reconstruction:  48097312899072.0\n",
      "Normal Probability:  577167301804032.0\n",
      "Normal reconstruction:  47770756972544.0\n",
      "Normal Probability:  573250761392128.0\n",
      "Normal reconstruction:  75956379189248.0\n",
      "Normal Probability:  911476416053248.0\n",
      "Normal reconstruction:  69812705296384.0\n",
      "Normal Probability:  837753235308544.0\n",
      "Normal reconstruction:  69453266026496.0\n",
      "Normal Probability:  833439141986304.0\n",
      "Normal reconstruction:  57374488395776.0\n",
      "Normal Probability:  688493994967040.0\n",
      "Normal reconstruction:  72320823590912.0\n",
      "Normal Probability:  867849144893440.0\n",
      "Normal reconstruction:  73536039288832.0\n",
      "Normal Probability:  882433041891328.0\n",
      "Normal reconstruction:  72063360434176.0\n",
      "Normal Probability:  864761331843072.0\n",
      "Normal reconstruction:  74092363382784.0\n",
      "Normal Probability:  889108025049088.0\n",
      "Normal reconstruction:  53030451937280.0\n",
      "Normal Probability:  636365708460032.0\n",
      "Normal reconstruction:  49154290089984.0\n",
      "Normal Probability:  589850809991168.0\n",
      "Normal reconstruction:  78245814861824.0\n",
      "Normal Probability:  938949644124160.0\n",
      "Normal reconstruction:  72506866139136.0\n",
      "Normal Probability:  870082662105088.0\n",
      "Normal reconstruction:  79525891276800.0\n",
      "Normal Probability:  954309655134208.0\n",
      "Normal reconstruction:  73812519419904.0\n",
      "Normal Probability:  885751373889536.0\n",
      "Normal reconstruction:  50378414489600.0\n",
      "Normal Probability:  604540806103040.0\n",
      "Normal reconstruction:  69073270472704.0\n",
      "Normal Probability:  828880034201600.0\n",
      "Normal reconstruction:  76407426252800.0\n",
      "Normal Probability:  916889886785536.0\n",
      "Normal reconstruction:  54373254168576.0\n",
      "Normal Probability:  652478479597568.0\n",
      "Normal reconstruction:  54470104842240.0\n",
      "Normal Probability:  653640670904320.0\n",
      "Normal reconstruction:  62076559032320.0\n",
      "Normal Probability:  744916510572544.0\n",
      "Normal reconstruction:  71250848251904.0\n",
      "Normal Probability:  855011017883648.0\n",
      "Normal reconstruction:  59589114462208.0\n",
      "Normal Probability:  715070849941504.0\n",
      "Normal reconstruction:  70624160514048.0\n",
      "Normal Probability:  847488382664704.0\n",
      "Normal reconstruction:  49716544929792.0\n",
      "Normal Probability:  596597868068864.0\n",
      "Normal reconstruction:  50986106224640.0\n",
      "Normal Probability:  611832519720960.0\n",
      "Normal reconstruction:  47608508710912.0\n",
      "Normal Probability:  571301718654976.0\n",
      "Normal reconstruction:  50454369140736.0\n",
      "Normal Probability:  605452480020480.0\n",
      "Normal reconstruction:  69739682463744.0\n",
      "Normal Probability:  836875854020608.0\n",
      "Normal reconstruction:  68803144712192.0\n",
      "Normal Probability:  825636394369024.0\n",
      "Normal reconstruction:  49211219378176.0\n",
      "Normal Probability:  590534649315328.0\n",
      "Normal reconstruction:  49172896022528.0\n",
      "Normal Probability:  590074752270336.0\n",
      "Normal reconstruction:  72954515816448.0\n",
      "Normal Probability:  875453854253056.0\n",
      "Normal reconstruction:  48485483151360.0\n",
      "Normal Probability:  581826133360640.0\n",
      "Normal reconstruction:  70099201425408.0\n",
      "Normal Probability:  841190014451712.0\n",
      "Normal reconstruction:  77425266393088.0\n",
      "Normal Probability:  929103297380352.0\n",
      "Normal reconstruction:  49060878745600.0\n",
      "Normal Probability:  588730763051008.0\n",
      "Normal reconstruction:  71621784109056.0\n",
      "Normal Probability:  859462013288448.0\n",
      "Normal reconstruction:  72871837696000.0\n",
      "Normal Probability:  874461649698816.0\n",
      "Normal reconstruction:  60536792285184.0\n",
      "Normal Probability:  726441910075392.0\n",
      "Normal reconstruction:  49700891787264.0\n",
      "Normal Probability:  596411842297856.0\n",
      "Normal reconstruction:  72527267233792.0\n",
      "Normal Probability:  870326871261184.0\n",
      "Normal reconstruction:  72381011853312.0\n",
      "Normal Probability:  868571840249856.0\n",
      "Normal reconstruction:  47139967205376.0\n",
      "Normal Probability:  565678130069504.0\n",
      "Normal reconstruction:  47452434464768.0\n",
      "Normal Probability:  569429112913920.0\n",
      "Normal reconstruction:  69371305132032.0\n",
      "Normal Probability:  832455258931200.0\n",
      "Normal reconstruction:  50370319482880.0\n",
      "Normal Probability:  604443431141376.0\n",
      "Normal reconstruction:  70541633388544.0\n",
      "Normal Probability:  846499265118208.0\n",
      "Normal reconstruction:  69923841769472.0\n",
      "Normal Probability:  839087426633728.0\n",
      "Normal reconstruction:  51328504037376.0\n",
      "Normal Probability:  615943608729600.0\n",
      "Normal reconstruction:  70126313406464.0\n",
      "Normal Probability:  841515760877568.0\n",
      "Normal reconstruction:  69717247131648.0\n",
      "Normal Probability:  836606210605056.0\n",
      "Normal reconstruction:  72549815812096.0\n",
      "Normal Probability:  870598259507200.0\n",
      "Normal reconstruction:  56334783348736.0\n",
      "Normal Probability:  676016443883520.0\n",
      "Normal reconstruction:  59890592645120.0\n",
      "Normal Probability:  718687078187008.0\n",
      "Normal reconstruction:  48934848299008.0\n",
      "Normal Probability:  587218129256448.0\n",
      "Normal reconstruction:  71019456888832.0\n",
      "Normal Probability:  852233784655872.0\n",
      "Normal reconstruction:  68431000895488.0\n",
      "Normal Probability:  821173050933248.0\n",
      "Normal reconstruction:  69093579292672.0\n",
      "Normal Probability:  829123639377920.0\n",
      "Normal reconstruction:  50072842665984.0\n",
      "Normal Probability:  600873709338624.0\n",
      "Normal reconstruction:  48929060159488.0\n",
      "Normal Probability:  587148067602432.0\n",
      "Normal reconstruction:  73509439012864.0\n",
      "Normal Probability:  882113603698688.0\n",
      "Normal reconstruction:  47996112732160.0\n",
      "Normal Probability:  575953101127680.0\n",
      "Normal reconstruction:  50755385950208.0\n",
      "Normal Probability:  609065419931648.0\n",
      "Normal reconstruction:  75689202024448.0\n",
      "Normal Probability:  908268276809728.0\n",
      "Normal reconstruction:  69707419877376.0\n",
      "Normal Probability:  836489172746240.0\n",
      "Normal reconstruction:  67473856528384.0\n",
      "Normal Probability:  809684416069632.0\n",
      "Normal reconstruction:  72702790467584.0\n",
      "Normal Probability:  872434626461696.0\n",
      "Normal reconstruction:  41871950741504.0\n",
      "Normal Probability:  502463559892992.0\n",
      "Normal reconstruction:  48415283085312.0\n",
      "Normal Probability:  580983648681984.0\n",
      "Normal reconstruction:  68032315523072.0\n",
      "Normal Probability:  816387450732544.0\n",
      "Normal reconstruction:  77283792519168.0\n",
      "Normal Probability:  927406785298432.0\n",
      "Normal reconstruction:  58997423996928.0\n",
      "Normal Probability:  707968181993472.0\n",
      "Normal reconstruction:  74170050281472.0\n",
      "Normal Probability:  890039563190272.0\n",
      "Normal reconstruction:  70475497603072.0\n",
      "Normal Probability:  845706172563456.0\n",
      "Normal reconstruction:  50032166305792.0\n",
      "Normal Probability:  600386767421440.0\n",
      "Normal reconstruction:  73432565809152.0\n",
      "Normal Probability:  881189984403456.0\n",
      "Normal reconstruction:  61188096393216.0\n",
      "Normal Probability:  734258415009792.0\n",
      "Normal reconstruction:  49711897640960.0\n",
      "Normal Probability:  596542704582656.0\n",
      "Normal reconstruction:  49313606533120.0\n",
      "Normal Probability:  591762405982208.0\n",
      "Normal reconstruction:  73759344033792.0\n",
      "Normal Probability:  885111557980160.0\n",
      "Normal reconstruction:  70005102215168.0\n",
      "Normal Probability:  840063457951744.0\n",
      "Normal reconstruction:  71251217350656.0\n",
      "Normal Probability:  855014306217984.0\n",
      "Normal reconstruction:  71967763857408.0\n",
      "Normal Probability:  863612025438208.0\n",
      "Normal reconstruction:  69255240351744.0\n",
      "Normal Probability:  831063421091840.0\n",
      "Normal reconstruction:  58844084436992.0\n",
      "Normal Probability:  706127788507136.0\n",
      "Normal reconstruction:  49720940560384.0\n",
      "Normal Probability:  596651890704384.0\n",
      "Normal reconstruction:  50749279043584.0\n",
      "Normal Probability:  608992069943296.0\n",
      "Normal reconstruction:  49532217851904.0\n",
      "Normal Probability:  594387100762112.0\n",
      "Normal reconstruction:  60310195011584.0\n",
      "Normal Probability:  723722927341568.0\n",
      "Normal reconstruction:  73401561513984.0\n",
      "Normal Probability:  880819409256448.0\n",
      "Normal reconstruction:  69130501750784.0\n",
      "Normal Probability:  829564947267584.0\n",
      "Normal reconstruction:  76948172701696.0\n",
      "Normal Probability:  923376025600000.0\n",
      "Normal reconstruction:  48387453878272.0\n",
      "Normal Probability:  580649178103808.0\n",
      "Normal reconstruction:  80359282704384.0\n",
      "Normal Probability:  964312365531136.0\n",
      "Normal reconstruction:  60484493508608.0\n",
      "Normal Probability:  725813368455168.0\n",
      "Normal reconstruction:  75937672593408.0\n",
      "Normal Probability:  911252406665216.0\n",
      "Normal reconstruction:  68813106184192.0\n",
      "Normal Probability:  825757190324224.0\n",
      "Normal reconstruction:  73152495353856.0\n",
      "Normal Probability:  877830313345024.0\n",
      "Normal reconstruction:  49693149102080.0\n",
      "Normal Probability:  596318024105984.0\n",
      "Normal reconstruction:  58637422690304.0\n",
      "Normal Probability:  703649256833024.0\n",
      "Normal reconstruction:  70327002464256.0\n",
      "Normal Probability:  843924096679936.0\n",
      "Normal reconstruction:  41149490266112.0\n",
      "Normal Probability:  493793597980672.0\n",
      "Normal reconstruction:  72725917859840.0\n",
      "Normal Probability:  872712926920704.0\n",
      "Normal reconstruction:  76290094792704.0\n",
      "Normal Probability:  915480600641536.0\n",
      "Normal reconstruction:  43356470116352.0\n",
      "Normal Probability:  520277238743040.0\n",
      "Normal reconstruction:  72212484718592.0\n",
      "Normal Probability:  866549984395264.0\n",
      "Normal reconstruction:  63094655025152.0\n",
      "Normal Probability:  757134820114432.0\n",
      "Normal reconstruction:  77961633988608.0\n",
      "Normal Probability:  935538567675904.0\n",
      "Normal reconstruction:  52833181237248.0\n",
      "Normal Probability:  633998980153344.0\n",
      "Normal reconstruction:  51678149607424.0\n",
      "Normal Probability:  620137577185280.0\n",
      "Normal reconstruction:  51126237921280.0\n",
      "Normal Probability:  613514469179392.0\n",
      "Normal reconstruction:  48730887684096.0\n",
      "Normal Probability:  584770534768640.0\n",
      "Normal reconstruction:  69005012369408.0\n",
      "Normal Probability:  828058353270784.0\n",
      "Normal reconstruction:  69908880687104.0\n",
      "Normal Probability:  838906836680704.0\n",
      "Normal reconstruction:  72977659985920.0\n",
      "Normal Probability:  875732825800704.0\n",
      "Normal reconstruction:  72098844246016.0\n",
      "Normal Probability:  865185526972416.0\n",
      "Normal reconstruction:  59470570848256.0\n",
      "Normal Probability:  713646866956288.0\n",
      "Normal reconstruction:  49716607844352.0\n",
      "Normal Probability:  596599814225920.0\n",
      "Normal reconstruction:  69096137818112.0\n",
      "Normal Probability:  829153771257856.0\n",
      "Normal reconstruction:  51182324154368.0\n",
      "Normal Probability:  614187839520768.0\n",
      "Normal reconstruction:  51053735182336.0\n",
      "Normal Probability:  612644335648768.0\n",
      "Normal reconstruction:  50554441039872.0\n",
      "Normal Probability:  606654332665856.0\n",
      "Normal reconstruction:  74723471917056.0\n",
      "Normal Probability:  896681260351488.0\n",
      "Normal reconstruction:  59999518720000.0\n",
      "Normal Probability:  719994828619776.0\n",
      "Normal reconstruction:  49274670809088.0\n",
      "Normal Probability:  591294992744448.0\n",
      "Normal reconstruction:  49324054544384.0\n",
      "Normal Probability:  591888436428800.0\n",
      "Normal reconstruction:  70350423457792.0\n",
      "Normal Probability:  844205349928960.0\n",
      "Normal reconstruction:  67947519279104.0\n",
      "Normal Probability:  815369946136576.0\n",
      "Normal reconstruction:  78724124901376.0\n",
      "Normal Probability:  944688928391168.0\n",
      "Normal reconstruction:  76221450813440.0\n",
      "Normal Probability:  914655228723200.0\n",
      "Normal reconstruction:  48978833965056.0\n",
      "Normal Probability:  587745537818624.0\n",
      "Normal reconstruction:  72223918391296.0\n",
      "Normal Probability:  866686752260096.0\n",
      "Normal reconstruction:  69585155915776.0\n",
      "Normal Probability:  835021636108288.0\n",
      "Normal reconstruction:  62940480798720.0\n",
      "Normal Probability:  755285501149184.0\n",
      "Normal reconstruction:  79974941851648.0\n",
      "Normal Probability:  959698496913408.0\n",
      "Normal reconstruction:  78010011090944.0\n",
      "Normal Probability:  936119260676096.0\n",
      "Normal reconstruction:  79390734024704.0\n",
      "Normal Probability:  952689445830656.0\n",
      "Normal reconstruction:  50084473470976.0\n",
      "Normal Probability:  601014436626432.0\n",
      "Normal reconstruction:  50259250118656.0\n",
      "Normal Probability:  603111454408704.0\n",
      "Normal reconstruction:  67612784459776.0\n",
      "Normal Probability:  811352876646400.0\n",
      "Normal reconstruction:  48540600500224.0\n",
      "Normal Probability:  582486685908992.0\n",
      "Normal reconstruction:  68468707688448.0\n",
      "Normal Probability:  821625163350016.0\n",
      "Normal reconstruction:  62142283776000.0\n",
      "Normal Probability:  745707120099328.0\n",
      "Normal reconstruction:  73777597644800.0\n",
      "Normal Probability:  885332883013632.0\n",
      "Normal reconstruction:  61038296825856.0\n",
      "Normal Probability:  732458689495040.0\n",
      "Normal reconstruction:  47621662048256.0\n",
      "Normal Probability:  571460632444928.0\n",
      "Normal reconstruction:  43630668546048.0\n",
      "Normal Probability:  523569230512128.0\n",
      "Normal reconstruction:  69873728225280.0\n",
      "Normal Probability:  838485191688192.0\n",
      "Normal reconstruction:  54957780762624.0\n",
      "Normal Probability:  659494174457856.0\n",
      "Normal reconstruction:  51883548868608.0\n",
      "Normal Probability:  622602284433408.0\n",
      "Normal reconstruction:  70383155806208.0\n",
      "Normal Probability:  844599950049280.0\n",
      "Normal reconstruction:  70826476961792.0\n",
      "Normal Probability:  849918126194688.0\n",
      "Normal reconstruction:  69598326030336.0\n",
      "Normal Probability:  835180482789376.0\n",
      "Normal reconstruction:  49722781859840.0\n",
      "Normal Probability:  596672560234496.0\n",
      "Normal reconstruction:  69509868158976.0\n",
      "Normal Probability:  834117746819072.0\n",
      "Normal reconstruction:  42299362902016.0\n",
      "Normal Probability:  507592321269760.0\n",
      "Normal reconstruction:  70357205647360.0\n",
      "Normal Probability:  844288095158272.0\n",
      "Normal reconstruction:  68068575281152.0\n",
      "Normal Probability:  816822987259904.0\n",
      "Normal reconstruction:  54180261658624.0\n",
      "Normal Probability:  650163223789568.0\n",
      "Normal reconstruction:  69618202836992.0\n",
      "Normal Probability:  835418048167936.0\n",
      "Normal reconstruction:  76722317819904.0\n",
      "Normal Probability:  920667914502144.0\n",
      "Normal reconstruction:  53408912375808.0\n",
      "Normal Probability:  640906629742592.0\n",
      "Normal reconstruction:  72986308640768.0\n",
      "Normal Probability:  875836173451264.0\n",
      "Normal reconstruction:  69548363481088.0\n",
      "Normal Probability:  834580663762944.0\n",
      "Normal reconstruction:  50803633029120.0\n",
      "Normal Probability:  609642489053184.0\n",
      "Normal reconstruction:  76744077869056.0\n",
      "Normal Probability:  920928095567872.0\n",
      "Normal reconstruction:  76477915725824.0\n",
      "Normal Probability:  917736330887168.0\n",
      "Normal reconstruction:  49447169949696.0\n",
      "Normal Probability:  593366911811584.0\n",
      "Normal reconstruction:  72850320916480.0\n",
      "Normal Probability:  874203817443328.0\n",
      "Normal reconstruction:  51124660862976.0\n",
      "Normal Probability:  613497155092480.0\n",
      "Normal reconstruction:  51854423621632.0\n",
      "Normal Probability:  622253989429248.0\n",
      "Normal reconstruction:  61640410136576.0\n",
      "Normal Probability:  739685710168064.0\n",
      "Normal reconstruction:  49980232433664.0\n",
      "Normal Probability:  599761782571008.0\n",
      "Normal reconstruction:  62390242639872.0\n",
      "Normal Probability:  748681921822720.0\n",
      "Normal reconstruction:  61631249776640.0\n",
      "Normal Probability:  739575517413376.0\n",
      "Normal reconstruction:  71441445814272.0\n",
      "Normal Probability:  857296611573760.0\n",
      "Normal reconstruction:  68226847342592.0\n",
      "Normal Probability:  818721094369280.0\n",
      "Normal reconstruction:  69640990490624.0\n",
      "Normal Probability:  835691919441920.0\n",
      "Normal reconstruction:  68591156199424.0\n",
      "Normal Probability:  823092901314560.0\n",
      "Normal reconstruction:  50106434846720.0\n",
      "Normal Probability:  601277100720128.0\n",
      "Normal reconstruction:  49276189147136.0\n",
      "Normal Probability:  591314387206144.0\n",
      "Normal reconstruction:  59400416919552.0\n",
      "Normal Probability:  712805456019456.0\n",
      "Normal reconstruction:  50154514153472.0\n",
      "Normal Probability:  601853968515072.0\n",
      "Normal reconstruction:  48703003951104.0\n",
      "Normal Probability:  584435997081600.0\n",
      "Normal reconstruction:  60654014693376.0\n",
      "Normal Probability:  727848511864832.0\n",
      "Normal reconstruction:  72523693686784.0\n",
      "Normal Probability:  870285330874368.0\n",
      "Normal reconstruction:  78808371691520.0\n",
      "Normal Probability:  945700460298240.0\n",
      "Normal reconstruction:  70886648446976.0\n",
      "Normal Probability:  850639882027008.0\n",
      "Normal reconstruction:  69451621859328.0\n",
      "Normal Probability:  833419076435968.0\n",
      "Normal reconstruction:  73193045884928.0\n",
      "Normal Probability:  878318329004032.0\n",
      "Normal reconstruction:  70555315208192.0\n",
      "Normal Probability:  846663748943872.0\n",
      "Normal reconstruction:  61270703210496.0\n",
      "Normal Probability:  735247800991744.0\n",
      "Normal reconstruction:  74924077088768.0\n",
      "Normal Probability:  899087918432256.0\n",
      "Normal reconstruction:  70885339824128.0\n",
      "Normal Probability:  850625587838976.0\n",
      "Normal reconstruction:  50931546718208.0\n",
      "Normal Probability:  611179818909696.0\n",
      "Normal reconstruction:  60671064539136.0\n",
      "Normal Probability:  728052858355712.0\n",
      "Normal reconstruction:  49654448259072.0\n",
      "Normal Probability:  595853228113920.0\n",
      "Normal reconstruction:  41337076318208.0\n",
      "Normal Probability:  496044865486848.0\n",
      "Normal reconstruction:  74812022063104.0\n",
      "Normal Probability:  897744533192704.0\n",
      "Normal reconstruction:  46262707552256.0\n",
      "Normal Probability:  555152977166336.0\n",
      "Normal reconstruction:  74426213203968.0\n",
      "Normal Probability:  893115430862848.0\n",
      "Normal reconstruction:  72340293550080.0\n",
      "Normal Probability:  868083354828800.0\n",
      "Normal reconstruction:  76042538582016.0\n",
      "Normal Probability:  912510429429760.0\n",
      "Normal reconstruction:  52792320327680.0\n",
      "Normal Probability:  633507609051136.0\n",
      "Normal reconstruction:  78118014418944.0\n",
      "Normal Probability:  937415736819712.0\n",
      "Normal reconstruction:  51467188699136.0\n",
      "Normal Probability:  617606499270656.0\n",
      "Normal reconstruction:  74951063240704.0\n",
      "Normal Probability:  899412255571968.0\n",
      "Normal reconstruction:  70068176158720.0\n",
      "Normal Probability:  840818768216064.0\n",
      "Normal reconstruction:  71410751897600.0\n",
      "Normal Probability:  856928586563584.0\n",
      "Normal reconstruction:  50083101933568.0\n",
      "Normal Probability:  600996652777472.0\n",
      "Normal reconstruction:  48889109413888.0\n",
      "Normal Probability:  586669312966656.0\n",
      "Normal reconstruction:  76049928945664.0\n",
      "Normal Probability:  912599482892288.0\n",
      "Normal reconstruction:  70593651146752.0\n",
      "Normal Probability:  847123981533184.0\n",
      "Normal reconstruction:  42430686560256.0\n",
      "Normal Probability:  509168104505344.0\n",
      "Normal reconstruction:  47411703578624.0\n",
      "Normal Probability:  568940627492864.0\n",
      "Normal reconstruction:  71703086497792.0\n",
      "Normal Probability:  860436031340544.0\n",
      "Normal reconstruction:  74090618552320.0\n",
      "Normal Probability:  889087892389888.0\n",
      "Normal reconstruction:  56456682405888.0\n",
      "Normal Probability:  677479685554176.0\n",
      "Normal reconstruction:  73741937672192.0\n",
      "Normal Probability:  884904392916992.0\n",
      "Normal reconstruction:  76671239585792.0\n",
      "Normal Probability:  920054271049728.0\n",
      "Normal reconstruction:  43766400417792.0\n",
      "Normal Probability:  525197694205952.0\n",
      "Normal reconstruction:  69144380702720.0\n",
      "Normal Probability:  829731914121216.0\n",
      "Normal reconstruction:  73965846396928.0\n",
      "Normal Probability:  887589552783360.0\n",
      "Normal reconstruction:  62539404673024.0\n",
      "Normal Probability:  750472184987648.0\n",
      "Normal reconstruction:  50931383140352.0\n",
      "Normal Probability:  611176597684224.0\n",
      "Normal reconstruction:  78058950230016.0\n",
      "Normal Probability:  936707604086784.0\n",
      "Normal reconstruction:  51444744978432.0\n",
      "Normal Probability:  617338936229888.0\n",
      "Normal reconstruction:  50317286703104.0\n",
      "Normal Probability:  603808581287936.0\n",
      "Normal reconstruction:  70304642629632.0\n",
      "Normal Probability:  843655057244160.0\n",
      "Normal reconstruction:  71639660232704.0\n",
      "Normal Probability:  859675889238016.0\n",
      "Normal reconstruction:  81655104536576.0\n",
      "Normal Probability:  979860751122432.0\n",
      "Normal reconstruction:  68765681188864.0\n",
      "Normal Probability:  825189516443648.0\n",
      "Normal reconstruction:  65764832837632.0\n",
      "Normal Probability:  789178363150336.0\n",
      "Normal reconstruction:  48601040420864.0\n",
      "Normal Probability:  583212602490880.0\n",
      "Normal reconstruction:  50508135923712.0\n",
      "Normal Probability:  606096456679424.0\n",
      "Normal reconstruction:  49166759755776.0\n",
      "Normal Probability:  590001804935168.0\n",
      "Normal reconstruction:  74640315645952.0\n",
      "Normal Probability:  895683619979264.0\n",
      "Normal reconstruction:  76175380578304.0\n",
      "Normal Probability:  914104801820672.0\n",
      "Normal reconstruction:  70451506184192.0\n",
      "Normal Probability:  845418141319168.0\n",
      "Normal reconstruction:  81151964217344.0\n",
      "Normal Probability:  973823570608128.0\n",
      "Normal reconstruction:  70820940480512.0\n",
      "Normal Probability:  849850614677504.0\n",
      "Normal reconstruction:  71298386493440.0\n",
      "Normal Probability:  855580772139008.0\n",
      "Normal reconstruction:  56924343107584.0\n",
      "Normal Probability:  683091731415040.0\n",
      "Normal reconstruction:  68994253979648.0\n",
      "Normal Probability:  827932389933056.0\n",
      "Normal reconstruction:  73454543962112.0\n",
      "Normal Probability:  881454661763072.0\n",
      "Normal reconstruction:  47786204594176.0\n",
      "Normal Probability:  573434371244032.0\n",
      "Normal reconstruction:  74021177655296.0\n",
      "Normal Probability:  888253192339456.0\n",
      "Normal reconstruction:  76511436603392.0\n",
      "Normal Probability:  918137105022976.0\n",
      "Normal reconstruction:  78200860311552.0\n",
      "Normal Probability:  938409686204416.0\n",
      "Normal reconstruction:  68840859893760.0\n",
      "Normal Probability:  826089983180800.0\n",
      "Normal reconstruction:  49556133773312.0\n",
      "Normal Probability:  594674058264576.0\n",
      "Normal reconstruction:  68521895657472.0\n",
      "Normal Probability:  822262898884608.0\n",
      "Normal reconstruction:  64649504489472.0\n",
      "Normal Probability:  775793567334400.0\n",
      "Normal reconstruction:  75737822396416.0\n",
      "Normal Probability:  908853667430400.0\n",
      "Normal reconstruction:  72825767460864.0\n",
      "Normal Probability:  873909679292416.0\n",
      "Normal reconstruction:  55145928851456.0\n",
      "Normal Probability:  661750978445312.0\n",
      "Normal reconstruction:  86130393350144.0\n",
      "Normal Probability:  1033564820865024.0\n",
      "Normal reconstruction:  51178150821888.0\n",
      "Normal Probability:  614138984267776.0\n",
      "Normal reconstruction:  50936445665280.0\n",
      "Normal Probability:  611237196988416.0\n",
      "Normal reconstruction:  61154302885888.0\n",
      "Normal Probability:  733852406382592.0\n",
      "Normal reconstruction:  67822868758528.0\n",
      "Normal Probability:  813874760646656.0\n",
      "Normal reconstruction:  49330098536448.0\n",
      "Normal Probability:  591961182437376.0\n",
      "Normal reconstruction:  49539406888960.0\n",
      "Normal Probability:  594472798781440.0\n",
      "Normal reconstruction:  70396233646080.0\n",
      "Normal Probability:  844755172851712.0\n",
      "Normal reconstruction:  50618131546112.0\n",
      "Normal Probability:  607416689360896.0\n",
      "Normal reconstruction:  49498608893952.0\n",
      "Normal Probability:  593982501421056.0\n",
      "Normal reconstruction:  51330374696960.0\n",
      "Normal Probability:  615963942715392.0\n",
      "Normal reconstruction:  49991586414592.0\n",
      "Normal Probability:  599899489959936.0\n",
      "Normal reconstruction:  53417921740800.0\n",
      "Normal Probability:  641014272360448.0\n",
      "Normal reconstruction:  74509612744704.0\n",
      "Normal Probability:  894115688480768.0\n",
      "Normal reconstruction:  52715585536000.0\n",
      "Normal Probability:  632587345199104.0\n",
      "Normal reconstruction:  69654684893184.0\n",
      "Normal Probability:  835855329525760.0\n",
      "Normal reconstruction:  48538796949504.0\n",
      "Normal Probability:  582464539983872.0\n",
      "Normal reconstruction:  71439021506560.0\n",
      "Normal Probability:  857268492959744.0\n",
      "Normal reconstruction:  76484408508416.0\n",
      "Normal Probability:  917812902100992.0\n",
      "Normal reconstruction:  51942847938560.0\n",
      "Normal Probability:  623314980569088.0\n",
      "Normal reconstruction:  48700713861120.0\n",
      "Normal Probability:  584409489080320.0\n",
      "Normal reconstruction:  54654343839744.0\n",
      "Normal Probability:  655852310626304.0\n",
      "Normal reconstruction:  44037885132800.0\n",
      "Normal Probability:  528454353158144.0\n",
      "Normal reconstruction:  73593073434624.0\n",
      "Normal Probability:  883115740364800.0\n",
      "Normal reconstruction:  50249817128960.0\n",
      "Normal Probability:  602997771993088.0\n",
      "Normal reconstruction:  64331785961472.0\n",
      "Normal Probability:  771982454947840.0\n",
      "Normal reconstruction:  58043416969216.0\n",
      "Normal Probability:  696520617754624.0\n",
      "Normal reconstruction:  75924267597824.0\n",
      "Normal Probability:  911090741411840.0\n",
      "Normal reconstruction:  52099278700544.0\n",
      "Normal Probability:  625191881277440.0\n",
      "Normal reconstruction:  49791773966336.0\n",
      "Normal Probability:  597501488922624.0\n",
      "Normal reconstruction:  70999651385344.0\n",
      "Normal Probability:  851996420603904.0\n",
      "Normal reconstruction:  73971458375680.0\n",
      "Normal Probability:  887657198518272.0\n",
      "Normal reconstruction:  48599979261952.0\n",
      "Normal Probability:  583199918915584.0\n",
      "Normal reconstruction:  72183418191872.0\n",
      "Normal Probability:  866201152520192.0\n",
      "Normal reconstruction:  76023496441856.0\n",
      "Normal Probability:  912282393509888.0\n",
      "Normal reconstruction:  49065601531904.0\n",
      "Normal Probability:  588787805585408.0\n",
      "Normal reconstruction:  49964533153792.0\n",
      "Normal Probability:  599573005336576.0\n",
      "Normal reconstruction:  49875278364672.0\n",
      "Normal Probability:  598504095350784.0\n",
      "Normal reconstruction:  52794782384128.0\n",
      "Normal Probability:  633538948890624.0\n",
      "Normal reconstruction:  49683594477568.0\n",
      "Normal Probability:  596202798186496.0\n",
      "Normal reconstruction:  50173786980352.0\n",
      "Normal Probability:  602084219027456.0\n",
      "Normal reconstruction:  77081367019520.0\n",
      "Normal Probability:  924975162720256.0\n",
      "Normal reconstruction:  76520244641792.0\n",
      "Normal Probability:  918243338354688.0\n",
      "Normal reconstruction:  50270469881856.0\n",
      "Normal Probability:  603245739245568.0\n",
      "Normal reconstruction:  72007987232768.0\n",
      "Normal Probability:  864096551436288.0\n",
      "Normal reconstruction:  41434543554560.0\n",
      "Normal Probability:  497214371659776.0\n",
      "Normal reconstruction:  79708662267904.0\n",
      "Normal Probability:  956503511007232.0\n",
      "Normal reconstruction:  53120591724544.0\n",
      "Normal Probability:  637445490081792.0\n",
      "Normal reconstruction:  51798123479040.0\n",
      "Normal Probability:  621579142692864.0\n",
      "Normal reconstruction:  70912149815296.0\n",
      "Normal Probability:  850944153616384.0\n",
      "Normal reconstruction:  53098143809536.0\n",
      "Normal Probability:  637177121734656.0\n",
      "Normal reconstruction:  49422079623168.0\n",
      "Normal Probability:  593065458794496.0\n",
      "Normal reconstruction:  50935199956992.0\n",
      "Normal Probability:  611223842324480.0\n",
      "Normal reconstruction:  56489834184704.0\n",
      "Normal Probability:  677877104246784.0\n",
      "Normal reconstruction:  42579475300352.0\n",
      "Normal Probability:  510953300951040.0\n",
      "Normal reconstruction:  72656460185600.0\n",
      "Normal Probability:  871877153128448.0\n",
      "Normal reconstruction:  70907208925184.0\n",
      "Normal Probability:  850886842646528.0\n",
      "Normal reconstruction:  52572073230336.0\n",
      "Normal Probability:  630866137055232.0\n",
      "Normal reconstruction:  65009774231552.0\n",
      "Normal Probability:  780116250591232.0\n",
      "Normal reconstruction:  52407748788224.0\n",
      "Normal Probability:  628892935127040.0\n",
      "Normal reconstruction:  52780660162560.0\n",
      "Normal Probability:  633367284416512.0\n",
      "Normal reconstruction:  50963700252672.0\n",
      "Normal Probability:  611565560659968.0\n",
      "Normal reconstruction:  68962847031296.0\n",
      "Normal Probability:  827554231484416.0\n",
      "Normal reconstruction:  50043738390528.0\n",
      "Normal Probability:  600524810354688.0\n",
      "Normal reconstruction:  61167418474496.0\n",
      "Normal Probability:  734010783301632.0\n",
      "Normal reconstruction:  74440062795776.0\n",
      "Normal Probability:  893280652886016.0\n",
      "Normal reconstruction:  76471775264768.0\n",
      "Normal Probability:  917660766306304.0\n",
      "Normal reconstruction:  71629040254976.0\n",
      "Normal Probability:  859548986376192.0\n",
      "Normal reconstruction:  69628810231808.0\n",
      "Normal Probability:  835545823444992.0\n",
      "Normal reconstruction:  69299276349440.0\n",
      "Normal Probability:  831593111355392.0\n",
      "Normal reconstruction:  53275210547200.0\n",
      "Normal Probability:  639301519933440.0\n",
      "Normal reconstruction:  63194286522368.0\n",
      "Normal Probability:  758331438268416.0\n",
      "Normal reconstruction:  49234267078656.0\n",
      "Normal Probability:  590810802290688.0\n",
      "Normal reconstruction:  51006884806656.0\n",
      "Normal Probability:  612083104219136.0\n",
      "Normal reconstruction:  48721224007680.0\n",
      "Normal Probability:  584655510175744.0\n",
      "Normal reconstruction:  80112733126656.0\n",
      "Normal Probability:  961351321124864.0\n",
      "Normal reconstruction:  76866752872448.0\n",
      "Normal Probability:  922400262717440.0\n",
      "Normal reconstruction:  50140618424320.0\n",
      "Normal Probability:  601688813600768.0\n",
      "Normal reconstruction:  73347119448064.0\n",
      "Normal Probability:  880165768921088.0\n",
      "Normal reconstruction:  74701661536256.0\n",
      "Normal Probability:  896419737108480.0\n",
      "Normal reconstruction:  71944175091712.0\n",
      "Normal Probability:  863330503753728.0\n",
      "Normal reconstruction:  52154916143104.0\n",
      "Normal Probability:  625859346038784.0\n",
      "Normal reconstruction:  42961484120064.0\n",
      "Normal Probability:  515537306124288.0\n",
      "Normal reconstruction:  58679910989824.0\n",
      "Normal Probability:  704159149981696.0\n",
      "Normal reconstruction:  74947498082304.0\n",
      "Normal Probability:  899369842769920.0\n",
      "Normal reconstruction:  78450136186880.0\n",
      "Normal Probability:  941403077083136.0\n",
      "Normal reconstruction:  72896022052864.0\n",
      "Normal Probability:  874752835059712.0\n",
      "Normal reconstruction:  52266090364928.0\n",
      "Normal Probability:  627192463622144.0\n",
      "Normal reconstruction:  72975378284544.0\n",
      "Normal Probability:  875704841404416.0\n",
      "Normal reconstruction:  50399117574144.0\n",
      "Normal Probability:  604789444444160.0\n",
      "Normal reconstruction:  63771691188224.0\n",
      "Normal Probability:  765261636435968.0\n",
      "Normal reconstruction:  83544789483520.0\n",
      "Normal Probability:  1002537171812352.0\n",
      "Normal reconstruction:  76702998855680.0\n",
      "Normal Probability:  920436254703616.0\n",
      "Normal reconstruction:  69573936152576.0\n",
      "Normal Probability:  834887217053696.0\n",
      "Normal reconstruction:  69314224848896.0\n",
      "Normal Probability:  831770614300672.0\n",
      "Normal reconstruction:  70685095362560.0\n",
      "Normal Probability:  848221144350720.0\n",
      "Normal reconstruction:  47927619747840.0\n",
      "Normal Probability:  575131017543680.0\n",
      "Normal reconstruction:  79135082807296.0\n",
      "Normal Probability:  949621027241984.0\n",
      "Normal reconstruction:  50046963810304.0\n",
      "Normal Probability:  600561921556480.0\n",
      "Normal reconstruction:  73098908925952.0\n",
      "Normal Probability:  877187544645632.0\n",
      "Normal reconstruction:  72527074295808.0\n",
      "Normal Probability:  870325193539584.0\n",
      "Normal reconstruction:  70702073905152.0\n",
      "Normal Probability:  848425893494784.0\n",
      "Normal reconstruction:  77399253319680.0\n",
      "Normal Probability:  928792247795712.0\n",
      "Normal reconstruction:  50026164256768.0\n",
      "Normal Probability:  600312679235584.0\n",
      "Normal reconstruction:  75203635838976.0\n",
      "Normal Probability:  902444502482944.0\n",
      "Normal reconstruction:  74091549687808.0\n",
      "Normal Probability:  889099904876544.0\n",
      "Normal reconstruction:  50480570957824.0\n",
      "Normal Probability:  605768227225600.0\n",
      "Normal reconstruction:  73267738050560.0\n",
      "Normal Probability:  879213494140928.0\n",
      "Normal reconstruction:  77033442902016.0\n",
      "Normal Probability:  924401113497600.0\n",
      "Normal reconstruction:  71653233000448.0\n",
      "Normal Probability:  859839232212992.0\n",
      "Normal reconstruction:  74944360742912.0\n",
      "Normal Probability:  899332328914944.0\n",
      "Normal reconstruction:  75165836771328.0\n",
      "Normal Probability:  901988296425472.0\n",
      "Normal reconstruction:  50038566813696.0\n",
      "Normal Probability:  600462600437760.0\n",
      "Normal reconstruction:  50177020788736.0\n",
      "Normal Probability:  602124215910400.0\n",
      "Normal reconstruction:  42587658387456.0\n",
      "Normal Probability:  511051984535552.0\n",
      "Normal reconstruction:  75238935101440.0\n",
      "Normal Probability:  902867221217280.0\n",
      "Normal reconstruction:  73957977882624.0\n",
      "Normal Probability:  887496137244672.0\n",
      "Normal reconstruction:  58012710469632.0\n",
      "Normal Probability:  696153465159680.0\n",
      "Normal reconstruction:  72328507555840.0\n",
      "Normal Probability:  867942895976448.0\n",
      "Normal reconstruction:  55216741285888.0\n",
      "Normal Probability:  662601247752192.0\n",
      "Normal reconstruction:  49960879915008.0\n",
      "Normal Probability:  599531800494080.0\n",
      "Normal reconstruction:  53083811872768.0\n",
      "Normal Probability:  637007269199872.0\n",
      "Normal reconstruction:  78230992191488.0\n",
      "Normal Probability:  938770597675008.0\n",
      "Normal reconstruction:  64647155679232.0\n",
      "Normal Probability:  775764710522880.0\n",
      "Normal reconstruction:  61087227576320.0\n",
      "Normal Probability:  733045824946176.0\n",
      "Normal reconstruction:  69487944531968.0\n",
      "Normal Probability:  833857095991296.0\n",
      "Normal reconstruction:  73621930246144.0\n",
      "Normal Probability:  883462693191680.0\n",
      "Normal reconstruction:  60680132624384.0\n",
      "Normal Probability:  728161708933120.0\n",
      "Normal reconstruction:  72788228440064.0\n",
      "Normal Probability:  873458707726336.0\n",
      "Normal reconstruction:  75839651708928.0\n",
      "Normal Probability:  910075384299520.0\n",
      "Normal reconstruction:  83254585589760.0\n",
      "Normal Probability:  999054423097344.0\n",
      "Normal reconstruction:  73047402872832.0\n",
      "Normal Probability:  876568062722048.0\n",
      "Normal reconstruction:  50224034742272.0\n",
      "Normal Probability:  602688937000960.0\n",
      "Normal reconstruction:  51451481030656.0\n",
      "Normal Probability:  617418326016000.0\n",
      "Normal reconstruction:  76937779216384.0\n",
      "Normal Probability:  923253149270016.0\n",
      "Normal reconstruction:  77729412153344.0\n",
      "Normal Probability:  932752945840128.0\n",
      "Normal reconstruction:  51810312126464.0\n",
      "Normal Probability:  621723896512512.0\n",
      "Normal reconstruction:  49626891681792.0\n",
      "Normal Probability:  595522649849856.0\n",
      "Normal reconstruction:  43031688380416.0\n",
      "Normal Probability:  516380864544768.0\n",
      "Normal reconstruction:  78756060332032.0\n",
      "Normal Probability:  945070844936192.0\n",
      "Normal reconstruction:  42592716718080.0\n",
      "Normal Probability:  511113355591680.0\n",
      "Normal reconstruction:  73756022145024.0\n",
      "Normal Probability:  885073641472000.0\n",
      "Normal reconstruction:  75833486082048.0\n",
      "Normal Probability:  910002772508672.0\n",
      "Normal reconstruction:  52225577582592.0\n",
      "Normal Probability:  626706125684736.0\n",
      "Normal reconstruction:  73102356643840.0\n",
      "Normal Probability:  877230091665408.0\n",
      "Normal reconstruction:  59889439211520.0\n",
      "Normal Probability:  718673253761024.0\n",
      "Normal reconstruction:  81449340370944.0\n",
      "Normal Probability:  977392419995648.0\n",
      "Normal reconstruction:  53163042275328.0\n",
      "Normal Probability:  637956054319104.0\n",
      "Normal reconstruction:  51692364103680.0\n",
      "Normal Probability:  620308570570752.0\n",
      "Normal reconstruction:  50654483578880.0\n",
      "Normal Probability:  607853635174400.0\n",
      "Normal reconstruction:  76856745263104.0\n",
      "Normal Probability:  922280540504064.0\n",
      "Normal reconstruction:  50360802607104.0\n",
      "Normal Probability:  604330956685312.0\n",
      "Normal reconstruction:  71448148312064.0\n",
      "Normal Probability:  857378014625792.0\n",
      "Normal reconstruction:  48294013173760.0\n",
      "Normal Probability:  579528862728192.0\n",
      "Normal reconstruction:  43062298411008.0\n",
      "Normal Probability:  516748554010624.0\n",
      "Normal reconstruction:  44312758845440.0\n",
      "Normal Probability:  531752888041472.0\n",
      "Normal reconstruction:  71051149049856.0\n",
      "Normal Probability:  852613486608384.0\n",
      "Normal reconstruction:  77651876249600.0\n",
      "Normal Probability:  931822078787584.0\n",
      "Normal reconstruction:  73481479782400.0\n",
      "Normal Probability:  881778327814144.0\n",
      "Normal reconstruction:  66300764225536.0\n",
      "Normal Probability:  795609137152000.0\n",
      "Normal reconstruction:  50192808148992.0\n",
      "Normal Probability:  602311785185280.0\n",
      "Normal reconstruction:  80605849059328.0\n",
      "Normal Probability:  967268980752384.0\n",
      "Normal reconstruction:  48944272900096.0\n",
      "Normal Probability:  587330469494784.0\n",
      "Normal reconstruction:  47950289960960.0\n",
      "Normal Probability:  575402741334016.0\n",
      "Normal reconstruction:  49451443945472.0\n",
      "Normal Probability:  593416639479808.0\n",
      "Normal reconstruction:  48973549142016.0\n",
      "Normal Probability:  587682925248512.0\n",
      "Normal reconstruction:  77759434981376.0\n",
      "Normal Probability:  933114394181632.0\n",
      "Normal reconstruction:  71604511965184.0\n",
      "Normal Probability:  859254579789824.0\n",
      "Normal reconstruction:  69739636326400.0\n",
      "Normal Probability:  836876256673792.0\n",
      "Normal reconstruction:  79567557492736.0\n",
      "Normal Probability:  954809280626688.0\n",
      "Normal reconstruction:  74691586818048.0\n",
      "Normal Probability:  896298672717824.0\n",
      "Normal reconstruction:  73157973114880.0\n",
      "Normal Probability:  877895811596288.0\n",
      "Normal reconstruction:  72735858360320.0\n",
      "Normal Probability:  872829495017472.0\n",
      "Normal reconstruction:  76382587584512.0\n",
      "Normal Probability:  916589843054592.0\n",
      "Normal reconstruction:  55748906188800.0\n",
      "Normal Probability:  668985850855424.0\n",
      "Normal reconstruction:  76469560672256.0\n",
      "Normal Probability:  917635130720256.0\n",
      "Normal reconstruction:  71930686210048.0\n",
      "Normal Probability:  863168704282624.0\n",
      "Normal reconstruction:  70408757837824.0\n",
      "Normal Probability:  844906905993216.0\n",
      "Normal reconstruction:  75397932777472.0\n",
      "Normal Probability:  904774388023296.0\n",
      "Normal reconstruction:  64248206065664.0\n",
      "Normal Probability:  770980452499456.0\n",
      "Normal reconstruction:  51374419083264.0\n",
      "Normal Probability:  616491821039616.0\n",
      "Normal reconstruction:  72695039393792.0\n",
      "Normal Probability:  872340137181184.0\n",
      "Normal reconstruction:  81590923296768.0\n",
      "Normal Probability:  979091549323264.0\n",
      "Normal reconstruction:  50078542725120.0\n",
      "Normal Probability:  600942026162176.0\n",
      "Normal reconstruction:  65079001219072.0\n",
      "Normal Probability:  780947796525056.0\n",
      "Normal reconstruction:  62986165157888.0\n",
      "Normal Probability:  755834720092160.0\n",
      "Normal reconstruction:  73990274023424.0\n",
      "Normal Probability:  887883758043136.0\n",
      "Normal reconstruction:  77392114614272.0\n",
      "Normal Probability:  928706080014336.0\n",
      "Normal reconstruction:  72824156848128.0\n",
      "Normal Probability:  873889882177536.0\n",
      "Normal reconstruction:  73948221931520.0\n",
      "Normal Probability:  887377690099712.0\n",
      "Normal reconstruction:  48477803380736.0\n",
      "Normal Probability:  581733254692864.0\n",
      "Normal reconstruction:  81536347013120.0\n",
      "Normal Probability:  978436902354944.0\n",
      "Normal reconstruction:  52521120825344.0\n",
      "Normal Probability:  630253433126912.0\n",
      "Normal reconstruction:  71762167463936.0\n",
      "Normal Probability:  861146311557120.0\n",
      "Normal reconstruction:  48438498557952.0\n",
      "Normal Probability:  581262620229632.0\n",
      "Normal reconstruction:  49667391881216.0\n",
      "Normal Probability:  596009323331584.0\n",
      "Normal reconstruction:  50728890531840.0\n",
      "Normal Probability:  608747860787200.0\n",
      "Normal reconstruction:  78844912467968.0\n",
      "Normal Probability:  946138144309248.0\n",
      "Normal reconstruction:  50813988765696.0\n",
      "Normal Probability:  609768989261824.0\n",
      "Normal reconstruction:  73437330538496.0\n",
      "Normal Probability:  881247832244224.0\n",
      "Normal reconstruction:  72853735079936.0\n",
      "Normal Probability:  874244820959232.0\n",
      "Normal reconstruction:  53198408646656.0\n",
      "Normal Probability:  638380383666176.0\n",
      "Normal reconstruction:  73911563714560.0\n",
      "Normal Probability:  886938529693696.0\n",
      "Normal reconstruction:  51032268734464.0\n",
      "Normal Probability:  612387308699648.0\n",
      "Normal reconstruction:  70112455426048.0\n",
      "Normal Probability:  841349733548032.0\n",
      "Normal reconstruction:  56783049588736.0\n",
      "Normal Probability:  681397568143360.0\n",
      "Normal reconstruction:  70127030632448.0\n",
      "Normal Probability:  841523746832384.0\n",
      "Normal reconstruction:  48724994686976.0\n",
      "Normal Probability:  584699265155072.0\n",
      "Normal reconstruction:  72816439328768.0\n",
      "Normal Probability:  873795996876800.0\n",
      "Normal reconstruction:  81514805067776.0\n",
      "Normal Probability:  978176385744896.0\n",
      "Normal reconstruction:  74452570210304.0\n",
      "Normal Probability:  893430439870464.0\n",
      "Normal reconstruction:  48567565680640.0\n",
      "Normal Probability:  582811559919616.0\n",
      "Normal reconstruction:  50443795300352.0\n",
      "Normal Probability:  605325711376384.0\n",
      "Normal reconstruction:  51683841277952.0\n",
      "Normal Probability:  620205558464512.0\n",
      "Normal reconstruction:  53611094605824.0\n",
      "Normal Probability:  643333084938240.0\n",
      "Normal reconstruction:  68800749764608.0\n",
      "Normal Probability:  825609081061376.0\n",
      "Normal reconstruction:  74412179062784.0\n",
      "Normal Probability:  892946316525568.0\n",
      "Normal reconstruction:  74343786741760.0\n",
      "Normal Probability:  892126715969536.0\n",
      "Normal reconstruction:  70619882323968.0\n",
      "Normal Probability:  847439191867392.0\n",
      "Normal reconstruction:  64304942415872.0\n",
      "Normal Probability:  771657044066304.0\n",
      "Normal reconstruction:  49294539227136.0\n",
      "Normal Probability:  591534302953472.0\n",
      "Normal reconstruction:  78546764562432.0\n",
      "Normal Probability:  942561845837824.0\n",
      "Normal reconstruction:  49367373316096.0\n",
      "Normal Probability:  592407859036160.0\n",
      "Normal reconstruction:  54646974447616.0\n",
      "Normal Probability:  655764062470144.0\n",
      "Normal reconstruction:  64909828161536.0\n",
      "Normal Probability:  778917619171328.0\n",
      "Normal reconstruction:  53883455930368.0\n",
      "Normal Probability:  646600682635264.0\n",
      "Normal reconstruction:  70694582878208.0\n",
      "Normal Probability:  848335162310656.0\n",
      "Normal reconstruction:  59625483272192.0\n",
      "Normal Probability:  715506520686592.0\n",
      "Normal reconstruction:  73050263388160.0\n",
      "Normal Probability:  876603630419968.0\n",
      "Normal reconstruction:  73864142913536.0\n",
      "Normal Probability:  886370587377664.0\n",
      "Normal reconstruction:  73004117655552.0\n",
      "Normal Probability:  876048908550144.0\n",
      "Normal reconstruction:  75103215812608.0\n",
      "Normal Probability:  901238153543680.0\n",
      "Normal reconstruction:  76176882139136.0\n",
      "Normal Probability:  914122988322816.0\n",
      "Normal reconstruction:  76576238600192.0\n",
      "Normal Probability:  918914896756736.0\n",
      "Normal reconstruction:  72765252042752.0\n",
      "Normal Probability:  873183024513024.0\n",
      "Normal reconstruction:  70473039740928.0\n",
      "Normal Probability:  845677919731712.0\n",
      "Normal reconstruction:  50163796148224.0\n",
      "Normal Probability:  601965838991360.0\n",
      "Normal reconstruction:  65035258822656.0\n",
      "Normal Probability:  780424011841536.0\n",
      "Normal reconstruction:  52850654707712.0\n",
      "Normal Probability:  634207822938112.0\n",
      "Normal reconstruction:  50522295894016.0\n",
      "Normal Probability:  606267382956032.0\n",
      "Normal reconstruction:  72912933486592.0\n",
      "Normal Probability:  874955906482176.0\n",
      "Normal reconstruction:  70522364755968.0\n",
      "Normal Probability:  846268679061504.0\n",
      "Normal reconstruction:  72510028644352.0\n",
      "Normal Probability:  870119639089152.0\n",
      "Normal reconstruction:  71998197727232.0\n",
      "Normal Probability:  863979714904064.0\n",
      "Normal reconstruction:  70912107872256.0\n",
      "Normal Probability:  850945428684800.0\n",
      "Normal reconstruction:  51384166645760.0\n",
      "Normal Probability:  616609529987072.0\n",
      "Normal reconstruction:  49679450505216.0\n",
      "Normal Probability:  596152265211904.0\n",
      "Normal reconstruction:  50849661321216.0\n",
      "Normal Probability:  610194862112768.0\n",
      "Normal reconstruction:  51359839682560.0\n",
      "Normal Probability:  616317539319808.0\n",
      "Normal reconstruction:  50222340243456.0\n",
      "Normal Probability:  602668200361984.0\n",
      "Normal reconstruction:  62053695881216.0\n",
      "Normal Probability:  744644853891072.0\n",
      "Normal reconstruction:  60541741563904.0\n",
      "Normal Probability:  726501704073216.0\n",
      "Normal reconstruction:  74310953730048.0\n",
      "Normal Probability:  891731511869440.0\n",
      "Normal reconstruction:  80177677729792.0\n",
      "Normal Probability:  962131797213184.0\n",
      "Normal reconstruction:  49311941394432.0\n",
      "Normal Probability:  591742810193920.0\n",
      "Normal reconstruction:  73220451467264.0\n",
      "Normal Probability:  878646088695808.0\n",
      "Normal reconstruction:  71941784338432.0\n",
      "Normal Probability:  863301042962432.0\n",
      "Normal reconstruction:  66764176097280.0\n",
      "Normal Probability:  801170448711680.0\n",
      "Normal reconstruction:  53955061088256.0\n",
      "Normal Probability:  647460414291968.0\n",
      "Normal reconstruction:  49478488817664.0\n",
      "Normal Probability:  593742117470208.0\n",
      "Normal reconstruction:  54706302877696.0\n",
      "Normal Probability:  656476423061504.0\n",
      "Normal reconstruction:  50345568894976.0\n",
      "Normal Probability:  604147078397952.0\n",
      "Normal reconstruction:  60553510780928.0\n",
      "Normal Probability:  726641558945792.0\n",
      "Normal reconstruction:  70669316390912.0\n",
      "Normal Probability:  848030756503552.0\n",
      "Attack reconstruction:  49326344634368.0\n",
      "Attack Probability:  591916286607360.0\n",
      "Attack reconstruction:  43838894768128.0\n",
      "Attack Probability:  526065982242816.0\n",
      "Attack reconstruction:  54475880398848.0\n",
      "Attack Probability:  653709994360832.0\n",
      "Attack reconstruction:  50958419623936.0\n",
      "Attack Probability:  611500196626432.0\n",
      "Attack reconstruction:  64183571841024.0\n",
      "Attack Probability:  770202794983424.0\n",
      "Attack reconstruction:  55577535315968.0\n",
      "Attack Probability:  666931648528384.0\n",
      "Attack reconstruction:  81103234793472.0\n",
      "Attack Probability:  973238918184960.0\n",
      "Attack reconstruction:  50729821667328.0\n",
      "Attack Probability:  608758329769984.0\n",
      "Attack reconstruction:  51981305511936.0\n",
      "Attack Probability:  623776555335680.0\n",
      "Attack reconstruction:  74867093274624.0\n",
      "Attack Probability:  898405689720832.0\n",
      "Attack reconstruction:  52337368367104.0\n",
      "Attack Probability:  628048839835648.0\n",
      "Attack reconstruction:  54023608598528.0\n",
      "Attack Probability:  648284175597568.0\n",
      "Attack reconstruction:  49541768282112.0\n",
      "Attack Probability:  594500984504320.0\n",
      "Attack reconstruction:  73638741016576.0\n",
      "Attack Probability:  883666100158464.0\n",
      "Attack reconstruction:  70208349798400.0\n",
      "Attack Probability:  842499711041536.0\n",
      "Attack reconstruction:  72221678632960.0\n",
      "Attack Probability:  866660110041088.0\n",
      "Attack reconstruction:  62191885615104.0\n",
      "Attack Probability:  746302107287552.0\n",
      "Attack reconstruction:  71607330537472.0\n",
      "Attack Probability:  859286523609088.0\n",
      "Attack reconstruction:  69887921750016.0\n",
      "Attack Probability:  838653769154560.0\n",
      "Attack reconstruction:  76011274240000.0\n",
      "Attack Probability:  912135894859776.0\n",
      "Attack reconstruction:  67463806976000.0\n",
      "Attack Probability:  809566103142400.0\n",
      "Attack reconstruction:  44256383205376.0\n",
      "Attack Probability:  531075424059392.0\n",
      "Attack reconstruction:  73674157719552.0\n",
      "Attack Probability:  884089355763712.0\n",
      "Attack reconstruction:  80976734584832.0\n",
      "Attack Probability:  971720915681280.0\n",
      "Attack reconstruction:  53253052039168.0\n",
      "Attack Probability:  639036909682688.0\n",
      "Attack reconstruction:  54258988744704.0\n",
      "Attack Probability:  651108653465600.0\n",
      "Attack reconstruction:  61531475673088.0\n",
      "Attack Probability:  738377154428928.0\n",
      "Attack reconstruction:  64393358344192.0\n",
      "Attack Probability:  772718773403648.0\n",
      "Attack reconstruction:  80525536526336.0\n",
      "Attack Probability:  966306572533760.0\n",
      "Attack reconstruction:  77618976129024.0\n",
      "Attack Probability:  931428820844544.0\n",
      "Attack reconstruction:  51875298672640.0\n",
      "Attack Probability:  622501956681728.0\n",
      "Attack reconstruction:  54909701455872.0\n",
      "Attack Probability:  658916568465408.0\n",
      "Attack reconstruction:  79165558620160.0\n",
      "Attack Probability:  949987710074880.0\n",
      "Attack reconstruction:  49861185503232.0\n",
      "Attack Probability:  598334578360320.0\n",
      "Attack reconstruction:  44375266557952.0\n",
      "Attack Probability:  532503030923264.0\n",
      "Attack reconstruction:  69343060688896.0\n",
      "Attack Probability:  832117030256640.0\n",
      "Attack reconstruction:  68548764368896.0\n",
      "Attack Probability:  822585222758400.0\n",
      "Attack reconstruction:  72923981283328.0\n",
      "Attack Probability:  875087574073344.0\n",
      "Attack reconstruction:  75472893378560.0\n",
      "Attack Probability:  905675995611136.0\n",
      "Attack reconstruction:  70791983005696.0\n",
      "Attack Probability:  849502520999936.0\n",
      "Attack reconstruction:  52136075329536.0\n",
      "Attack Probability:  625631712772096.0\n",
      "Attack reconstruction:  64014654636032.0\n",
      "Attack Probability:  768176509943808.0\n",
      "Attack reconstruction:  48226296135680.0\n",
      "Attack Probability:  578715369078784.0\n",
      "Attack reconstruction:  69909358837760.0\n",
      "Attack Probability:  838911802736640.0\n",
      "Attack reconstruction:  53100442288128.0\n",
      "Attack Probability:  637205307457536.0\n",
      "Attack reconstruction:  65120432553984.0\n",
      "Attack Probability:  781444066574336.0\n",
      "Attack reconstruction:  72447726452736.0\n",
      "Attack Probability:  869372113453056.0\n",
      "Attack reconstruction:  70189831946240.0\n",
      "Attack Probability:  842277043830784.0\n",
      "Attack reconstruction:  76641418084352.0\n",
      "Attack Probability:  919697319002112.0\n",
      "Attack reconstruction:  69038839431168.0\n",
      "Attack Probability:  828465502748672.0\n",
      "Attack reconstruction:  73697327054848.0\n",
      "Attack Probability:  884365173194752.0\n",
      "Attack reconstruction:  66045977034752.0\n",
      "Attack Probability:  792551590199296.0\n",
      "Attack reconstruction:  81727431114752.0\n",
      "Attack Probability:  980730347782144.0\n",
      "Attack reconstruction:  55548057747456.0\n",
      "Attack Probability:  666577582161920.0\n",
      "Attack reconstruction:  51734508470272.0\n",
      "Attack Probability:  620813631881216.0\n",
      "Attack reconstruction:  83152462675968.0\n",
      "Attack Probability:  997830625853440.0\n",
      "Attack reconstruction:  55277302841344.0\n",
      "Attack Probability:  663328774946816.0\n",
      "Attack reconstruction:  73594281394176.0\n",
      "Attack Probability:  883129363464192.0\n",
      "Attack reconstruction:  75852050071552.0\n",
      "Attack Probability:  910223694888960.0\n",
      "Attack reconstruction:  100143093252096.0\n",
      "Attack Probability:  1201717756559360.0\n",
      "Attack reconstruction:  73947626340352.0\n",
      "Attack Probability:  887371985846272.0\n",
      "Attack reconstruction:  73105217159168.0\n",
      "Attack Probability:  877261163069440.0\n",
      "Attack reconstruction:  74179202252800.0\n",
      "Attack Probability:  890150762577920.0\n",
      "Attack reconstruction:  82431403098112.0\n",
      "Attack Probability:  989176736514048.0\n",
      "Attack reconstruction:  67293031694336.0\n",
      "Attack Probability:  807515323367424.0\n",
      "Attack reconstruction:  75879405322240.0\n",
      "Attack Probability:  910552192778240.0\n",
      "Attack reconstruction:  56166390431744.0\n",
      "Attack Probability:  673997742145536.0\n",
      "Attack reconstruction:  48489836838912.0\n",
      "Attack Probability:  581878478274560.0\n",
      "Attack reconstruction:  64389432475648.0\n",
      "Attack Probability:  772673072267264.0\n",
      "Attack reconstruction:  70942038425600.0\n",
      "Attack Probability:  851304662433792.0\n",
      "Attack reconstruction:  50324156973056.0\n",
      "Attack Probability:  603890051448832.0\n",
      "Attack reconstruction:  78344381005824.0\n",
      "Attack Probability:  940132102307840.0\n",
      "Attack reconstruction:  82835750780928.0\n",
      "Attack Probability:  994029714014208.0\n",
      "Attack reconstruction:  55675719778304.0\n",
      "Attack Probability:  668108402458624.0\n",
      "Attack reconstruction:  62851309895680.0\n",
      "Attack Probability:  754215651639296.0\n",
      "Attack reconstruction:  54428484763648.0\n",
      "Attack Probability:  653142454697984.0\n",
      "Attack reconstruction:  78753862516736.0\n",
      "Attack Probability:  945045343567872.0\n",
      "Attack reconstruction:  76449771945984.0\n",
      "Attack Probability:  917397162688512.0\n",
      "Attack reconstruction:  61414467174400.0\n",
      "Attack Probability:  736973840973824.0\n",
      "Attack reconstruction:  54053627232256.0\n",
      "Attack Probability:  648642671149056.0\n",
      "Attack reconstruction:  76571817803776.0\n",
      "Attack Probability:  918861343883264.0\n",
      "Attack reconstruction:  43665560961024.0\n",
      "Attack Probability:  523986848972800.0\n",
      "Attack reconstruction:  54944732282880.0\n",
      "Attack Probability:  659335059341312.0\n",
      "Attack reconstruction:  64787853606912.0\n",
      "Attack Probability:  777454310391808.0\n",
      "Attack reconstruction:  51353829244928.0\n",
      "Attack Probability:  616245263073280.0\n",
      "Attack reconstruction:  78740684013568.0\n",
      "Attack Probability:  944888644370432.0\n",
      "Attack reconstruction:  71483447574528.0\n",
      "Attack Probability:  857801605775360.0\n",
      "Attack reconstruction:  75156139540480.0\n",
      "Attack Probability:  901875486425088.0\n",
      "Attack reconstruction:  72205094354944.0\n",
      "Attack Probability:  866461803347968.0\n",
      "Attack reconstruction:  72145409409024.0\n",
      "Attack Probability:  865744745136128.0\n",
      "Attack reconstruction:  81360278519808.0\n",
      "Attack Probability:  976323241574400.0\n",
      "Attack reconstruction:  72602882146304.0\n",
      "Attack Probability:  871233109360640.0\n",
      "Attack reconstruction:  68268853297152.0\n",
      "Attack Probability:  819225618808832.0\n",
      "Attack reconstruction:  50703733096448.0\n",
      "Attack Probability:  608444461613056.0\n",
      "Attack reconstruction:  75043103047680.0\n",
      "Attack Probability:  900517337235456.0\n",
      "Attack reconstruction:  64169499951104.0\n",
      "Attack Probability:  770033412210688.0\n",
      "Attack reconstruction:  71141645352960.0\n",
      "Attack Probability:  853699308027904.0\n",
      "Attack reconstruction:  50837170683904.0\n",
      "Attack Probability:  610045142237184.0\n",
      "Attack reconstruction:  77560515919872.0\n",
      "Attack Probability:  930725587058688.0\n",
      "Attack reconstruction:  66500719280128.0\n",
      "Attack Probability:  798009486999552.0\n",
      "Attack reconstruction:  52344578375680.0\n",
      "Attack Probability:  628135410270208.0\n",
      "Attack reconstruction:  50895953854464.0\n",
      "Attack Probability:  610751597248512.0\n",
      "Attack reconstruction:  72342029991936.0\n",
      "Attack Probability:  868105500753920.0\n",
      "Attack reconstruction:  82094441103360.0\n",
      "Attack Probability:  985134501199872.0\n",
      "Attack reconstruction:  55398895714304.0\n",
      "Attack Probability:  664787855867904.0\n",
      "Attack reconstruction:  52983920328704.0\n",
      "Attack Probability:  635807362711552.0\n",
      "Attack reconstruction:  77275957559296.0\n",
      "Attack Probability:  927311155167232.0\n",
      "Attack reconstruction:  69771580145664.0\n",
      "Attack Probability:  837258106109952.0\n",
      "Attack reconstruction:  72258949218304.0\n",
      "Attack Probability:  867107994599424.0\n",
      "Attack reconstruction:  53648902062080.0\n",
      "Attack Probability:  643785063137280.0\n",
      "Attack reconstruction:  52042307469312.0\n",
      "Attack Probability:  624507505082368.0\n",
      "Attack reconstruction:  52284176203776.0\n",
      "Attack Probability:  627410164776960.0\n",
      "Attack reconstruction:  74261880373248.0\n",
      "Attack Probability:  891143436894208.0\n",
      "Attack reconstruction:  64512736624640.0\n",
      "Attack Probability:  774152352956416.0\n",
      "Attack reconstruction:  76443665039360.0\n",
      "Attack Probability:  917326228619264.0\n",
      "Attack reconstruction:  81229701447680.0\n",
      "Attack Probability:  974756518035456.0\n",
      "Attack reconstruction:  66374047105024.0\n",
      "Attack Probability:  796487592181760.0\n",
      "Attack reconstruction:  65122248687616.0\n",
      "Attack Probability:  781466413826048.0\n",
      "Attack reconstruction:  73651005161472.0\n",
      "Attack Probability:  883812598808576.0\n",
      "Attack reconstruction:  56295080067072.0\n",
      "Attack Probability:  675541178908672.0\n",
      "Attack reconstruction:  75809352056832.0\n",
      "Attack Probability:  909711251603456.0\n",
      "Attack reconstruction:  71792911712256.0\n",
      "Attack Probability:  861515812962304.0\n",
      "Attack reconstruction:  76311754178560.0\n",
      "Attack Probability:  915740244836352.0\n",
      "Attack reconstruction:  70947298082816.0\n",
      "Attack Probability:  851367140786176.0\n",
      "Attack reconstruction:  61262901805056.0\n",
      "Attack Probability:  735155324977152.0\n",
      "Attack reconstruction:  65470606606336.0\n",
      "Attack Probability:  785647765815296.0\n",
      "Attack reconstruction:  76270557724672.0\n",
      "Attack Probability:  915247598665728.0\n",
      "Attack reconstruction:  51096571609088.0\n",
      "Attack Probability:  613160000159744.0\n",
      "Attack reconstruction:  70817475985408.0\n",
      "Attack Probability:  849810886230016.0\n",
      "Attack reconstruction:  75723930861568.0\n",
      "Attack Probability:  908688244080640.0\n",
      "Attack reconstruction:  56477934944256.0\n",
      "Attack Probability:  677736242741248.0\n",
      "Attack reconstruction:  76790206824448.0\n",
      "Attack Probability:  921483085873152.0\n",
      "Attack reconstruction:  58874384089088.0\n",
      "Attack Probability:  706491518550016.0\n",
      "Attack reconstruction:  72994495922176.0\n",
      "Attack Probability:  875933749739520.0\n",
      "Attack reconstruction:  65522251071488.0\n",
      "Attack Probability:  786265704235008.0\n",
      "Attack reconstruction:  52734682202112.0\n",
      "Attack Probability:  632815649554432.0\n",
      "Attack reconstruction:  62636892880896.0\n",
      "Attack Probability:  751644039970816.0\n",
      "Attack reconstruction:  74887662141440.0\n",
      "Attack Probability:  898651173945344.0\n",
      "Attack reconstruction:  72321997996032.0\n",
      "Attack Probability:  867864445714432.0\n",
      "Attack reconstruction:  71559498694656.0\n",
      "Attack Probability:  858714152108032.0\n",
      "Attack reconstruction:  76652742705152.0\n",
      "Attack Probability:  919832677580800.0\n",
      "Attack reconstruction:  55130602864640.0\n",
      "Attack Probability:  661568576552960.0\n",
      "Attack reconstruction:  79258663780352.0\n",
      "Attack Probability:  951102388305920.0\n",
      "Attack reconstruction:  68275916505088.0\n",
      "Attack Probability:  819310914174976.0\n",
      "Attack reconstruction:  77040086679552.0\n",
      "Attack Probability:  924481442807808.0\n",
      "Attack reconstruction:  48224773603328.0\n",
      "Attack Probability:  578696914141184.0\n",
      "Attack reconstruction:  66517093842944.0\n",
      "Attack Probability:  798205377773568.0\n",
      "Attack reconstruction:  49566313349120.0\n",
      "Attack Probability:  594794854219776.0\n",
      "Attack reconstruction:  62903851941888.0\n",
      "Attack Probability:  754845938089984.0\n",
      "Attack reconstruction:  52923140669440.0\n",
      "Attack Probability:  635077218271232.0\n",
      "Attack reconstruction:  68207834562560.0\n",
      "Attack Probability:  818495407259648.0\n",
      "Attack reconstruction:  69172402847744.0\n",
      "Attack Probability:  830070277013504.0\n",
      "Attack reconstruction:  49189446746112.0\n",
      "Attack Probability:  590271716786176.0\n",
      "Attack reconstruction:  44377099468800.0\n",
      "Attack Probability:  532524539314176.0\n",
      "Attack reconstruction:  71187380043776.0\n",
      "Attack Probability:  854247654555648.0\n",
      "Attack reconstruction:  59465139224576.0\n",
      "Attack Probability:  713580093636608.0\n",
      "Attack reconstruction:  67199926534144.0\n",
      "Attack Probability:  806399302959104.0\n",
      "Attack reconstruction:  72697295929344.0\n",
      "Attack Probability:  872366980726784.0\n",
      "Attack reconstruction:  56887588421632.0\n",
      "Attack Probability:  682650960396288.0\n",
      "Attack reconstruction:  70780138291200.0\n",
      "Attack Probability:  849362196365312.0\n",
      "Attack reconstruction:  71220993196032.0\n",
      "Attack Probability:  854652253896704.0\n",
      "Attack reconstruction:  71760271638528.0\n",
      "Attack Probability:  861121682604032.0\n",
      "Attack reconstruction:  73631602311168.0\n",
      "Attack Probability:  883579865268224.0\n",
      "Attack reconstruction:  83181218824192.0\n",
      "Attack Probability:  998173954801664.0\n",
      "Attack reconstruction:  67228275834880.0\n",
      "Attack Probability:  806739008028672.0\n",
      "Attack reconstruction:  51828028866560.0\n",
      "Attack Probability:  621936832937984.0\n",
      "Attack reconstruction:  52247966777344.0\n",
      "Attack Probability:  626975366447104.0\n",
      "Attack reconstruction:  64530298175488.0\n",
      "Attack Probability:  774362940571648.0\n",
      "Attack reconstruction:  61061663293440.0\n",
      "Attack Probability:  732739472982016.0\n",
      "Attack reconstruction:  73908275380224.0\n",
      "Attack Probability:  886898465701888.0\n",
      "Attack reconstruction:  54141669867520.0\n",
      "Attack Probability:  649698964668416.0\n",
      "Attack reconstruction:  73227732779008.0\n",
      "Attack Probability:  878732927565824.0\n",
      "Attack reconstruction:  69820682862592.0\n",
      "Attack Probability:  837848462786560.0\n",
      "Attack reconstruction:  85485879820288.0\n",
      "Attack Probability:  1025829853200384.0\n",
      "Attack reconstruction:  52329021702144.0\n",
      "Attack Probability:  627948176539648.0\n",
      "Attack reconstruction:  68589830799360.0\n",
      "Attack Probability:  823077936037888.0\n",
      "Attack reconstruction:  71613127065600.0\n",
      "Attack Probability:  859356518154240.0\n",
      "Attack reconstruction:  76867784671232.0\n",
      "Attack Probability:  922413483163648.0\n",
      "Attack reconstruction:  50654915592192.0\n",
      "Attack Probability:  607860077625344.0\n",
      "Attack reconstruction:  67400879833088.0\n",
      "Attack Probability:  808808779612160.0\n",
      "Attack reconstruction:  74303680806912.0\n",
      "Attack Probability:  891645679632384.0\n",
      "Attack reconstruction:  51662752317440.0\n",
      "Attack Probability:  619951954067456.0\n",
      "Attack reconstruction:  52972482461696.0\n",
      "Attack Probability:  635669453996032.0\n",
      "Attack reconstruction:  71451570864128.0\n",
      "Attack Probability:  857419622121472.0\n",
      "Attack reconstruction:  49430992519168.0\n",
      "Attack Probability:  593170349948928.0\n",
      "Attack reconstruction:  57548602343424.0\n",
      "Attack Probability:  690583362338816.0\n",
      "Attack reconstruction:  73236255604736.0\n",
      "Attack Probability:  878834865930240.0\n",
      "Attack reconstruction:  70272640090112.0\n",
      "Attack Probability:  843272402501632.0\n",
      "Attack reconstruction:  74225415094272.0\n",
      "Attack Probability:  890705618665472.0\n",
      "Attack reconstruction:  52440774737920.0\n",
      "Attack Probability:  629288810315776.0\n",
      "Attack reconstruction:  50120422850560.0\n",
      "Attack Probability:  601443597811712.0\n",
      "Attack reconstruction:  50558178164736.0\n",
      "Attack Probability:  606699362713600.0\n",
      "Attack reconstruction:  47555345907712.0\n",
      "Attack Probability:  570665727950848.0\n",
      "Attack reconstruction:  51620133994496.0\n",
      "Attack Probability:  619442262245376.0\n",
      "Attack reconstruction:  55017184690176.0\n",
      "Attack Probability:  660204991545344.0\n",
      "Attack reconstruction:  75253480947712.0\n",
      "Attack Probability:  903042375352320.0\n",
      "Attack reconstruction:  49762191540224.0\n",
      "Attack Probability:  597146080378880.0\n",
      "Attack reconstruction:  73758765219840.0\n",
      "Attack Probability:  885103639134208.0\n",
      "Attack reconstruction:  67078291718144.0\n",
      "Attack Probability:  804940289146880.0\n",
      "Attack reconstruction:  75285684813824.0\n",
      "Attack Probability:  903428788191232.0\n",
      "Attack reconstruction:  48983544168448.0\n",
      "Attack Probability:  587802714570752.0\n",
      "Attack reconstruction:  76566289711104.0\n",
      "Attack Probability:  918793765257216.0\n",
      "Attack reconstruction:  52324324081664.0\n",
      "Attack Probability:  627892207747072.0\n",
      "Attack reconstruction:  72437584625664.0\n",
      "Attack Probability:  869251116171264.0\n",
      "Attack reconstruction:  71745994227712.0\n",
      "Attack Probability:  860951293198336.0\n",
      "Attack reconstruction:  69453555433472.0\n",
      "Attack Probability:  833441289469952.0\n",
      "Attack reconstruction:  76532643004416.0\n",
      "Attack Probability:  918392051597312.0\n",
      "Attack reconstruction:  63373685293056.0\n",
      "Attack Probability:  760486169673728.0\n",
      "Attack reconstruction:  51570255331328.0\n",
      "Attack Probability:  618843449851904.0\n",
      "Attack reconstruction:  81196851658752.0\n",
      "Attack Probability:  974363260092416.0\n",
      "Attack reconstruction:  78206698782720.0\n",
      "Attack Probability:  938481224253440.0\n",
      "Attack reconstruction:  73386671734784.0\n",
      "Attack Probability:  880641168113664.0\n",
      "Attack reconstruction:  77348200251392.0\n",
      "Attack Probability:  928178604343296.0\n",
      "Attack reconstruction:  76425847635968.0\n",
      "Attack Probability:  917108393246720.0\n",
      "Attack reconstruction:  65401178292224.0\n",
      "Attack Probability:  784814676377600.0\n",
      "Attack reconstruction:  55294520459264.0\n",
      "Attack Probability:  663534799159296.0\n",
      "Attack reconstruction:  79126459318272.0\n",
      "Attack Probability:  949517411155968.0\n",
      "Attack reconstruction:  74943278612480.0\n",
      "Attack Probability:  899321188843520.0\n",
      "Attack reconstruction:  50461600120832.0\n",
      "Attack Probability:  605540593958912.0\n",
      "Attack reconstruction:  62273838120960.0\n",
      "Attack Probability:  747286460104704.0\n",
      "Attack reconstruction:  65464986238976.0\n",
      "Attack Probability:  785580120080384.0\n",
      "Attack reconstruction:  73080655314944.0\n",
      "Attack Probability:  876968903966720.0\n",
      "Attack reconstruction:  65420241403904.0\n",
      "Attack Probability:  785043047841792.0\n",
      "Attack reconstruction:  84981355380736.0\n",
      "Attack Probability:  1019777640300544.0\n",
      "Attack reconstruction:  65744498851840.0\n",
      "Attack Probability:  788932744708096.0\n",
      "Attack reconstruction:  76570064584704.0\n",
      "Attack Probability:  918839466393600.0\n",
      "Attack reconstruction:  56345696927744.0\n",
      "Attack Probability:  676147507494912.0\n",
      "Attack reconstruction:  75370049044480.0\n",
      "Attack Probability:  904441796493312.0\n",
      "Attack reconstruction:  91805093724160.0\n",
      "Attack Probability:  1101661057581056.0\n",
      "Attack reconstruction:  57220691656704.0\n",
      "Attack Probability:  686646890594304.0\n",
      "Attack reconstruction:  84928280657920.0\n",
      "Attack Probability:  1019139367895040.0\n",
      "Attack reconstruction:  71581149691904.0\n",
      "Attack Probability:  858974668718080.0\n",
      "Attack reconstruction:  70631861256192.0\n",
      "Attack Probability:  847582200856576.0\n",
      "Attack reconstruction:  78113677508608.0\n",
      "Attack Probability:  937364465647616.0\n",
      "Attack reconstruction:  74656220446720.0\n",
      "Attack Probability:  895873940717568.0\n",
      "Attack reconstruction:  76937208791040.0\n",
      "Attack Probability:  923247243689984.0\n",
      "Attack reconstruction:  66719427067904.0\n",
      "Attack Probability:  800633242255360.0\n",
      "Attack reconstruction:  75509039890432.0\n",
      "Attack Probability:  906110122852352.0\n",
      "Attack reconstruction:  56257855619072.0\n",
      "Attack Probability:  675094435201024.0\n",
      "Attack reconstruction:  80130558918656.0\n",
      "Attack Probability:  961566069489664.0\n",
      "Attack reconstruction:  54608579788800.0\n",
      "Attack Probability:  655303091683328.0\n",
      "Attack reconstruction:  53740052676608.0\n",
      "Attack Probability:  644880816668672.0\n",
      "Attack reconstruction:  54151610368000.0\n",
      "Attack Probability:  649818888208384.0\n",
      "Attack reconstruction:  65732725440512.0\n",
      "Attack Probability:  788792554291200.0\n",
      "Attack reconstruction:  72760831246336.0\n",
      "Attack Probability:  873130276945920.0\n",
      "Attack reconstruction:  83255550279680.0\n",
      "Attack Probability:  999066905346048.0\n",
      "Attack reconstruction:  85284897161216.0\n",
      "Attack Probability:  1023418765934592.0\n",
      "Attack reconstruction:  68513741930496.0\n",
      "Attack Probability:  822164383072256.0\n",
      "Attack reconstruction:  53994877616128.0\n",
      "Attack Probability:  647937424097280.0\n",
      "Attack reconstruction:  64032295878656.0\n",
      "Attack Probability:  768387231776768.0\n",
      "Attack reconstruction:  52208204775424.0\n",
      "Attack Probability:  626498155315200.0\n",
      "Attack reconstruction:  68171285397504.0\n",
      "Attack Probability:  818055374438400.0\n",
      "Attack reconstruction:  74922634248192.0\n",
      "Attack Probability:  899069866147840.0\n",
      "Attack reconstruction:  56122367016960.0\n",
      "Attack Probability:  673469259841536.0\n",
      "Attack reconstruction:  44052997210112.0\n",
      "Attack Probability:  528635849080832.0\n",
      "Attack reconstruction:  70624236011520.0\n",
      "Attack Probability:  847490597257216.0\n",
      "Attack reconstruction:  69171324911616.0\n",
      "Attack Probability:  830055714390016.0\n",
      "Attack reconstruction:  73767036387328.0\n",
      "Attack Probability:  885203228688384.0\n",
      "Attack reconstruction:  87801336954880.0\n",
      "Attack Probability:  1053617217863680.0\n",
      "Attack reconstruction:  79893782069248.0\n",
      "Attack Probability:  958725485494272.0\n",
      "Attack reconstruction:  69499285929984.0\n",
      "Attack Probability:  833992186134528.0\n",
      "Attack reconstruction:  51694691942400.0\n",
      "Attack Probability:  620335548334080.0\n",
      "Attack reconstruction:  64874465984512.0\n",
      "Attack Probability:  778493759586304.0\n",
      "Attack reconstruction:  70993821302784.0\n",
      "Attack Probability:  851925486534656.0\n",
      "Attack reconstruction:  74535634206720.0\n",
      "Attack Probability:  894428013133824.0\n",
      "Attack reconstruction:  78680688689152.0\n",
      "Attack Probability:  944170243981312.0\n",
      "Attack reconstruction:  71268992811008.0\n",
      "Attack Probability:  855229054582784.0\n",
      "Attack reconstruction:  52786385387520.0\n",
      "Attack Probability:  633436272328704.0\n",
      "Attack reconstruction:  73353360572416.0\n",
      "Attack Probability:  880239857106944.0\n",
      "Attack reconstruction:  81713883512832.0\n",
      "Attack Probability:  980567675895808.0\n",
      "Attack reconstruction:  51651289284608.0\n",
      "Attack Probability:  619817467904000.0\n",
      "Attack reconstruction:  74541615284224.0\n",
      "Attack Probability:  894498007678976.0\n",
      "Attack reconstruction:  77582863171584.0\n",
      "Attack Probability:  930994425167872.0\n",
      "Attack reconstruction:  95979399282688.0\n",
      "Attack Probability:  1151752388739072.0\n",
      "Attack reconstruction:  75049218342912.0\n",
      "Attack Probability:  900590754332672.0\n",
      "Attack reconstruction:  64215360471040.0\n",
      "Attack Probability:  770584241766400.0\n",
      "Attack reconstruction:  73771222302720.0\n",
      "Attack Probability:  885254768295936.0\n",
      "Attack reconstruction:  65646486355968.0\n",
      "Attack Probability:  787757534281728.0\n",
      "Attack reconstruction:  50435448635392.0\n",
      "Attack Probability:  605225249406976.0\n",
      "Attack reconstruction:  67315601244160.0\n",
      "Attack Probability:  807787584028672.0\n",
      "Attack reconstruction:  73686438641664.0\n",
      "Attack Probability:  884238001897472.0\n",
      "Attack reconstruction:  56533865988096.0\n",
      "Attack Probability:  678407197163520.0\n",
      "Attack reconstruction:  72807807451136.0\n",
      "Attack Probability:  873693857185792.0\n",
      "Attack reconstruction:  67730422104064.0\n",
      "Attack Probability:  812764712927232.0\n",
      "Attack reconstruction:  72785317593088.0\n",
      "Attack Probability:  873421395197952.0\n",
      "Attack reconstruction:  71810603286528.0\n",
      "Attack Probability:  861726333468672.0\n",
      "Attack reconstruction:  61193876144128.0\n",
      "Attack Probability:  734326933159936.0\n",
      "Attack reconstruction:  56863517310976.0\n",
      "Attack Probability:  682363398914048.0\n",
      "Attack reconstruction:  75644658515968.0\n",
      "Attack Probability:  907735700865024.0\n",
      "Attack reconstruction:  75586844229632.0\n",
      "Attack Probability:  907044076912640.0\n",
      "Attack reconstruction:  82986888331264.0\n",
      "Attack Probability:  995841854668800.0\n",
      "Attack reconstruction:  72040065269760.0\n",
      "Attack Probability:  864479944376320.0\n",
      "Attack reconstruction:  54087634649088.0\n",
      "Attack Probability:  649052169437184.0\n",
      "Attack reconstruction:  54799173156864.0\n",
      "Attack Probability:  657589759115264.0\n",
      "Attack reconstruction:  63859113066496.0\n",
      "Attack Probability:  766309742673920.0\n",
      "Attack reconstruction:  54513532665856.0\n",
      "Attack Probability:  654161905451008.0\n",
      "Attack reconstruction:  79592823980032.0\n",
      "Attack Probability:  955114424631296.0\n",
      "Attack reconstruction:  60456739799040.0\n",
      "Attack Probability:  725478763659264.0\n",
      "Attack reconstruction:  73648429858816.0\n",
      "Attack Probability:  883780252336128.0\n",
      "Attack reconstruction:  52618504175616.0\n",
      "Attack Probability:  631421530013696.0\n",
      "Attack reconstruction:  74511575678976.0\n",
      "Attack Probability:  894139713454080.0\n",
      "Attack reconstruction:  52857629835264.0\n",
      "Attack Probability:  634291037929472.0\n",
      "Attack reconstruction:  66992493035520.0\n",
      "Attack Probability:  803910033866752.0\n",
      "Attack reconstruction:  72590626390016.0\n",
      "Attack Probability:  871087684452352.0\n",
      "Attack reconstruction:  50998852714496.0\n",
      "Attack Probability:  611986870108160.0\n",
      "Attack reconstruction:  77352772042752.0\n",
      "Attack Probability:  928233633611776.0\n",
      "Attack reconstruction:  87665978376192.0\n",
      "Attack Probability:  1051990901653504.0\n",
      "Attack reconstruction:  49998704148480.0\n",
      "Attack Probability:  599984583999488.0\n",
      "Attack reconstruction:  72666761396224.0\n",
      "Attack Probability:  872001237417984.0\n",
      "Attack reconstruction:  73143645372416.0\n",
      "Attack Probability:  877723677360128.0\n",
      "Attack reconstruction:  65240398036992.0\n",
      "Attack Probability:  782885430755328.0\n",
      "Attack reconstruction:  56806697074688.0\n",
      "Attack Probability:  681680566222848.0\n",
      "Attack reconstruction:  73649839144960.0\n",
      "Attack Probability:  883798237511680.0\n",
      "Attack reconstruction:  56534155395072.0\n",
      "Attack Probability:  678409009102848.0\n",
      "Attack reconstruction:  103291748876288.0\n",
      "Attack Probability:  1239500583862272.0\n",
      "Attack reconstruction:  53536775733248.0\n",
      "Attack Probability:  642441208135680.0\n",
      "Attack reconstruction:  64075669176320.0\n",
      "Attack Probability:  768909741391872.0\n",
      "Attack reconstruction:  73044819181568.0\n",
      "Attack Probability:  876538870366208.0\n",
      "Attack reconstruction:  73097256370176.0\n",
      "Attack Probability:  877169827905536.0\n",
      "Attack reconstruction:  50815351914496.0\n",
      "Attack Probability:  609783216340992.0\n",
      "Attack reconstruction:  67796633387008.0\n",
      "Attack Probability:  813559416094720.0\n",
      "Attack reconstruction:  77233695752192.0\n",
      "Attack Probability:  926804550352896.0\n",
      "Attack reconstruction:  72795014823936.0\n",
      "Attack Probability:  873539305472000.0\n",
      "Attack reconstruction:  69800084635648.0\n",
      "Attack Probability:  837600361316352.0\n",
      "Attack reconstruction:  51905539604480.0\n",
      "Attack Probability:  622865820942336.0\n",
      "Attack reconstruction:  78119062994944.0\n",
      "Attack Probability:  937428755939328.0\n",
      "Attack reconstruction:  68037076058112.0\n",
      "Attack Probability:  816444157722624.0\n",
      "Attack reconstruction:  52993227489280.0\n",
      "Attack Probability:  635918159446016.0\n",
      "Attack reconstruction:  43905747779584.0\n",
      "Attack Probability:  526869174681600.0\n",
      "Attack reconstruction:  70622189191168.0\n",
      "Attack Probability:  847466169630720.0\n",
      "Attack reconstruction:  56818793447424.0\n",
      "Attack Probability:  681825454260224.0\n",
      "Attack reconstruction:  50717020651520.0\n",
      "Attack Probability:  608604851798016.0\n",
      "Attack reconstruction:  76483015999488.0\n",
      "Attack Probability:  917795789340672.0\n",
      "Attack reconstruction:  92120522162176.0\n",
      "Attack Probability:  1105447138361344.0\n",
      "Attack reconstruction:  50496924549120.0\n",
      "Attack Probability:  605964319326208.0\n",
      "Attack reconstruction:  75009515061248.0\n",
      "Attack Probability:  900114214289408.0\n",
      "Attack reconstruction:  73516275728384.0\n",
      "Attack Probability:  882195543621632.0\n",
      "Attack reconstruction:  76127968165888.0\n",
      "Attack Probability:  913536725286912.0\n",
      "Attack reconstruction:  53782616473600.0\n",
      "Attack Probability:  645391582232576.0\n",
      "Attack reconstruction:  78455815274496.0\n",
      "Attack Probability:  941470051729408.0\n",
      "Attack reconstruction:  71516012150784.0\n",
      "Attack Probability:  858191306948608.0\n",
      "Attack reconstruction:  74906150633472.0\n",
      "Attack Probability:  898874646462464.0\n",
      "Attack reconstruction:  73281562476544.0\n",
      "Attack Probability:  879379722797056.0\n",
      "Attack reconstruction:  75355201208320.0\n",
      "Attack Probability:  904262280282112.0\n",
      "Attack reconstruction:  82770831343616.0\n",
      "Attack Probability:  993251251191808.0\n",
      "Attack reconstruction:  53058134343680.0\n",
      "Attack Probability:  636696555159552.0\n",
      "Attack reconstruction:  73006466465792.0\n",
      "Attack Probability:  876078973321216.0\n",
      "Attack reconstruction:  86114975088640.0\n",
      "Attack Probability:  1033379197747200.0\n",
      "Attack reconstruction:  51074396323840.0\n",
      "Attack Probability:  612892437118976.0\n",
      "Attack reconstruction:  90747516747776.0\n",
      "Attack Probability:  1088970570072064.0\n",
      "Attack reconstruction:  72696876498944.0\n",
      "Attack Probability:  872361880453120.0\n",
      "Attack reconstruction:  51266935848960.0\n",
      "Attack Probability:  615204069048320.0\n",
      "Attack reconstruction:  78945324105728.0\n",
      "Attack Probability:  947344828792832.0\n",
      "Attack reconstruction:  89514760470528.0\n",
      "Attack Probability:  1074176957874176.0\n",
      "Attack reconstruction:  76291705405440.0\n",
      "Attack Probability:  915500867518464.0\n",
      "Attack reconstruction:  65241140428800.0\n",
      "Attack Probability:  782893014056960.0\n",
      "Attack reconstruction:  77603314597888.0\n",
      "Attack Probability:  931240312045568.0\n",
      "Attack reconstruction:  73297685381120.0\n",
      "Attack Probability:  879574875373568.0\n",
      "Attack reconstruction:  75430740623360.0\n",
      "Attack Probability:  905168988143616.0\n",
      "Attack reconstruction:  69911049142272.0\n",
      "Attack Probability:  838931532742656.0\n",
      "Attack reconstruction:  64275259326464.0\n",
      "Attack Probability:  771301568413696.0\n",
      "Attack reconstruction:  57247732334592.0\n",
      "Attack Probability:  686973240999936.0\n",
      "Attack reconstruction:  76573797515264.0\n",
      "Attack Probability:  918885570183168.0\n",
      "Attack reconstruction:  69775283716096.0\n",
      "Attack Probability:  837303203266560.0\n",
      "Attack reconstruction:  73598031101952.0\n",
      "Attack Probability:  883177077866496.0\n",
      "Attack reconstruction:  70376377810944.0\n",
      "Attack Probability:  844517137711104.0\n",
      "Attack reconstruction:  79864707153920.0\n",
      "Attack Probability:  958377391816704.0\n",
      "Attack reconstruction:  56304223649792.0\n",
      "Attack Probability:  675650096594944.0\n",
      "Attack reconstruction:  54915980328960.0\n",
      "Attack Probability:  658991059304448.0\n",
      "Attack reconstruction:  77191819821056.0\n",
      "Attack Probability:  926301099655168.0\n",
      "Attack reconstruction:  61598139940864.0\n",
      "Attack Probability:  739178836918272.0\n",
      "Attack reconstruction:  68505504317440.0\n",
      "Attack Probability:  822067544981504.0\n",
      "Attack reconstruction:  74925251493888.0\n",
      "Attack Probability:  899103487688704.0\n",
      "Attack reconstruction:  54102633480192.0\n",
      "Attack Probability:  649231148777472.0\n",
      "Attack reconstruction:  84097665859584.0\n",
      "Attack Probability:  1009169473732608.0\n",
      "Attack reconstruction:  53747954745344.0\n",
      "Attack Probability:  644975977037824.0\n",
      "Attack reconstruction:  81330440241152.0\n",
      "Attack Probability:  975965953982464.0\n",
      "Attack reconstruction:  54160955277312.0\n",
      "Attack Probability:  649931161337856.0\n",
      "Attack reconstruction:  74438057918464.0\n",
      "Attack Probability:  893257366110208.0\n",
      "Attack reconstruction:  55603242205184.0\n",
      "Attack Probability:  667238604472320.0\n",
      "Attack reconstruction:  54550752919552.0\n",
      "Attack Probability:  654609118920704.0\n",
      "Attack reconstruction:  61052926558208.0\n",
      "Attack Probability:  732633642303488.0\n",
      "Attack reconstruction:  71896301305856.0\n",
      "Attack Probability:  862755984769024.0\n",
      "Attack reconstruction:  68900167352320.0\n",
      "Attack Probability:  826802142445568.0\n",
      "Attack reconstruction:  52213766422528.0\n",
      "Attack Probability:  626564660199424.0\n",
      "Attack reconstruction:  80574375002112.0\n",
      "Attack Probability:  966891963154432.0\n",
      "Attack reconstruction:  72861008003072.0\n",
      "Attack Probability:  874332062482432.0\n",
      "Attack reconstruction:  75094491660288.0\n",
      "Attack Probability:  901134134804480.0\n",
      "Attack reconstruction:  62377336766464.0\n",
      "Attack Probability:  748526699020288.0\n",
      "Attack reconstruction:  76137598287872.0\n",
      "Attack Probability:  913650139267072.0\n",
      "Attack reconstruction:  52727912595456.0\n",
      "Attack Probability:  632736125550592.0\n",
      "Attack reconstruction:  72654312701952.0\n",
      "Attack Probability:  871851450433536.0\n",
      "Attack reconstruction:  54678511419392.0\n",
      "Attack Probability:  656141751156736.0\n",
      "Attack reconstruction:  77377350664192.0\n",
      "Attack Probability:  928527838871552.0\n",
      "Attack reconstruction:  74737933877248.0\n",
      "Attack Probability:  896854535438336.0\n",
      "Attack reconstruction:  67923041320960.0\n",
      "Attack Probability:  815076881727488.0\n",
      "Attack reconstruction:  77717290614784.0\n",
      "Attack Probability:  932607051169792.0\n",
      "Attack reconstruction:  63929212469248.0\n",
      "Attack Probability:  767150214086656.0\n",
      "Attack reconstruction:  54123701469184.0\n",
      "Attack Probability:  649485021609984.0\n",
      "Attack reconstruction:  49885533437952.0\n",
      "Attack Probability:  598627105898496.0\n",
      "Attack reconstruction:  54338126872576.0\n",
      "Attack Probability:  652057304367104.0\n",
      "Attack reconstruction:  77760793935872.0\n",
      "Attack Probability:  933131171397632.0\n",
      "Attack reconstruction:  57170666192896.0\n",
      "Attack Probability:  686048682180608.0\n",
      "Attack reconstruction:  51102468800512.0\n",
      "Attack Probability:  613228786745344.0\n",
      "Attack reconstruction:  57090664038400.0\n",
      "Attack Probability:  685088622772224.0\n",
      "Attack reconstruction:  50517661188096.0\n",
      "Attack Probability:  606211951034368.0\n",
      "Attack reconstruction:  66209315815424.0\n",
      "Attack Probability:  794510632157184.0\n",
      "Attack reconstruction:  45496664064000.0\n",
      "Attack Probability:  545960404975616.0\n",
      "Attack reconstruction:  72765377871872.0\n",
      "Attack Probability:  873184299581440.0\n",
      "Attack reconstruction:  76066639052800.0\n",
      "Attack Probability:  912798393565184.0\n",
      "Attack reconstruction:  52989788160000.0\n",
      "Attack Probability:  635878162563072.0\n",
      "Attack reconstruction:  72513384087552.0\n",
      "Attack Probability:  870160172843008.0\n",
      "Attack reconstruction:  57248281788416.0\n",
      "Attack Probability:  686979817668608.0\n",
      "Attack reconstruction:  77302004187136.0\n",
      "Attack Probability:  927623546929152.0\n",
      "Attack reconstruction:  57261317685248.0\n",
      "Attack Probability:  687136651083776.0\n",
      "Attack reconstruction:  45369257885696.0\n",
      "Attack Probability:  544430792638464.0\n",
      "Attack reconstruction:  74828732170240.0\n",
      "Attack Probability:  897943779409920.0\n",
      "Attack reconstruction:  58843992162304.0\n",
      "Attack Probability:  706126714765312.0\n",
      "Attack reconstruction:  72775284817920.0\n",
      "Attack Probability:  873304625774592.0\n",
      "Attack reconstruction:  54418070306816.0\n",
      "Attack Probability:  653016558469120.0\n",
      "Attack reconstruction:  81371393425408.0\n",
      "Attack Probability:  976456318451712.0\n",
      "Attack reconstruction:  68825772982272.0\n",
      "Attack Probability:  825908990574592.0\n",
      "Attack reconstruction:  48775368278016.0\n",
      "Attack Probability:  585304452890624.0\n",
      "Attack reconstruction:  81263171993600.0\n",
      "Attack Probability:  975157090844672.0\n",
      "Attack reconstruction:  59774947295232.0\n",
      "Attack Probability:  717297991811072.0\n",
      "Attack reconstruction:  67202086600704.0\n",
      "Attack Probability:  806425408307200.0\n",
      "Attack reconstruction:  73639118503936.0\n",
      "Attack Probability:  883668247642112.0\n",
      "Attack reconstruction:  65810156486656.0\n",
      "Attack Probability:  789723287126016.0\n",
      "Attack reconstruction:  44472331141120.0\n",
      "Attack Probability:  533667973693440.0\n",
      "Attack reconstruction:  56008063844352.0\n",
      "Attack Probability:  672096615137280.0\n",
      "Attack reconstruction:  86114522103808.0\n",
      "Attack Probability:  1033373761929216.0\n",
      "Attack reconstruction:  85744769040384.0\n",
      "Attack Probability:  1028938671325184.0\n",
      "Attack reconstruction:  74770674614272.0\n",
      "Attack Probability:  897247659163648.0\n",
      "Attack reconstruction:  60706422521856.0\n",
      "Attack Probability:  728476382396416.0\n",
      "Attack reconstruction:  76023949426688.0\n",
      "Attack Probability:  912289104396288.0\n",
      "Attack reconstruction:  56772689657856.0\n",
      "Attack Probability:  681270866608128.0\n",
      "Attack reconstruction:  54032651517952.0\n",
      "Attack Probability:  648391348453376.0\n",
      "Attack reconstruction:  72257590263808.0\n",
      "Attack Probability:  867092760887296.0\n",
      "Attack reconstruction:  76147832389632.0\n",
      "Attack Probability:  913773015597056.0\n",
      "Attack reconstruction:  66585842679808.0\n",
      "Attack Probability:  799030883909632.0\n",
      "Attack reconstruction:  49917485645824.0\n",
      "Attack Probability:  599009559314432.0\n",
      "Attack reconstruction:  81874214977536.0\n",
      "Attack Probability:  982490680393728.0\n",
      "Attack reconstruction:  80127497076736.0\n",
      "Attack Probability:  961529226723328.0\n",
      "Attack reconstruction:  72767777013760.0\n",
      "Attack Probability:  873213022175232.0\n",
      "Attack reconstruction:  53429690957824.0\n",
      "Attack Probability:  641157147131904.0\n",
      "Attack reconstruction:  72227483549696.0\n",
      "Attack Probability:  866730104586240.0\n",
      "Attack reconstruction:  80149080965120.0\n",
      "Attack Probability:  961790078877696.0\n",
      "Attack reconstruction:  52609884880896.0\n",
      "Attack Probability:  631318920560640.0\n",
      "Attack reconstruction:  74156720783360.0\n",
      "Attack Probability:  889879575658496.0\n",
      "Attack reconstruction:  75491809689600.0\n",
      "Attack Probability:  905900407652352.0\n",
      "Attack reconstruction:  54154059841536.0\n",
      "Attack Probability:  649850496483328.0\n",
      "Attack reconstruction:  54683964014592.0\n",
      "Attack Probability:  656207450734592.0\n",
      "Attack reconstruction:  50209035911168.0\n",
      "Attack Probability:  602507675959296.0\n",
      "Attack reconstruction:  53874211684352.0\n",
      "Attack Probability:  646491563622400.0\n",
      "Attack reconstruction:  101888359923712.0\n",
      "Attack Probability:  1222658674917376.0\n",
      "Attack reconstruction:  50010599194624.0\n",
      "Attack Probability:  600127056117760.0\n",
      "Attack reconstruction:  52234268180480.0\n",
      "Attack Probability:  626812493234176.0\n",
      "Attack reconstruction:  52695389962240.0\n",
      "Attack Probability:  632345954615296.0\n",
      "Attack reconstruction:  76433531600896.0\n",
      "Attack Probability:  917201204805632.0\n",
      "Attack reconstruction:  72313886212096.0\n",
      "Attack Probability:  867768144494592.0\n",
      "Attack reconstruction:  50855290077184.0\n",
      "Attack Probability:  610263715807232.0\n",
      "Attack reconstruction:  67608409800704.0\n",
      "Attack Probability:  811298518466560.0\n",
      "Attack reconstruction:  76693679112192.0\n",
      "Attack Probability:  920324518445056.0\n",
      "Attack reconstruction:  82347810619392.0\n",
      "Attack Probability:  988174801174528.0\n",
      "Attack reconstruction:  77677746716672.0\n",
      "Attack Probability:  932132927045632.0\n",
      "Attack reconstruction:  51454547066880.0\n",
      "Attack Probability:  617454229258240.0\n",
      "Attack reconstruction:  53301072625664.0\n",
      "Attack Probability:  639613173497856.0\n",
      "Attack reconstruction:  67591825522688.0\n",
      "Attack Probability:  811102157930496.0\n",
      "Attack reconstruction:  54895474376704.0\n",
      "Attack Probability:  658746380386304.0\n",
      "Attack reconstruction:  75927698538496.0\n",
      "Attack Probability:  911132013363200.0\n",
      "Attack reconstruction:  53330755715072.0\n",
      "Attack Probability:  639968447823872.0\n",
      "Attack reconstruction:  74556303736832.0\n",
      "Attack Probability:  894675242188800.0\n",
      "Attack reconstruction:  52025786105856.0\n",
      "Attack Probability:  624309064171520.0\n",
      "Attack reconstruction:  56437388607488.0\n",
      "Attack Probability:  677249435041792.0\n",
      "Attack reconstruction:  52861304045568.0\n",
      "Attack Probability:  634335329779712.0\n",
      "Attack reconstruction:  75443776520192.0\n",
      "Attack Probability:  905324210946048.0\n",
      "Attack reconstruction:  73462110486528.0\n",
      "Attack Probability:  881544587640832.0\n",
      "Attack reconstruction:  54164428161024.0\n",
      "Attack Probability:  649973372813312.0\n",
      "Attack reconstruction:  53889025966080.0\n",
      "Attack Probability:  646667053301760.0\n",
      "Attack reconstruction:  80733615947776.0\n",
      "Attack Probability:  968803089383424.0\n",
      "Attack reconstruction:  57242728529920.0\n",
      "Attack Probability:  686913312784384.0\n",
      "Attack reconstruction:  76562816827392.0\n",
      "Attack Probability:  918754372354048.0\n",
      "Attack reconstruction:  57218711945216.0\n",
      "Attack Probability:  686624945995776.0\n",
      "Attack reconstruction:  46414969176064.0\n",
      "Attack Probability:  556980619968512.0\n",
      "Attack reconstruction:  54359979196416.0\n",
      "Attack Probability:  652318894718976.0\n",
      "Attack reconstruction:  76027137097728.0\n",
      "Attack Probability:  912325074747392.0\n",
      "Attack reconstruction:  70510922694656.0\n",
      "Attack Probability:  846130837454848.0\n",
      "Attack reconstruction:  77157929844736.0\n",
      "Attack Probability:  925895158136832.0\n",
      "Attack reconstruction:  77728581681152.0\n",
      "Attack Probability:  932743617708032.0\n",
      "Attack reconstruction:  54415784411136.0\n",
      "Attack Probability:  652988708290560.0\n",
      "Attack reconstruction:  72657122885632.0\n",
      "Attack Probability:  871883864014848.0\n",
      "Attack reconstruction:  74943605768192.0\n",
      "Attack Probability:  899322463911936.0\n",
      "Attack reconstruction:  64008237350912.0\n",
      "Attack Probability:  768098663661568.0\n",
      "Attack reconstruction:  55632489086976.0\n",
      "Attack Probability:  667590322028544.0\n",
      "Attack reconstruction:  74841717735424.0\n",
      "Attack Probability:  898101351022592.0\n",
      "Attack reconstruction:  69078370746368.0\n",
      "Attack Probability:  828940700614656.0\n",
      "Attack reconstruction:  78187337875456.0\n",
      "Attack Probability:  938248759148544.0\n",
      "Attack reconstruction:  72551720026112.0\n",
      "Attack Probability:  870621747609600.0\n",
      "Attack reconstruction:  54557107290112.0\n",
      "Attack Probability:  654684280848384.0\n",
      "Attack reconstruction:  66633691299840.0\n",
      "Attack Probability:  799606141091840.0\n",
      "Attack reconstruction:  51642535772160.0\n",
      "Attack Probability:  619710563483648.0\n",
      "Attack reconstruction:  57934583169024.0\n",
      "Attack Probability:  695215283240960.0\n",
      "Attack reconstruction:  76362010329088.0\n",
      "Attack Probability:  916344694374400.0\n",
      "Attack reconstruction:  64637286481920.0\n",
      "Attack Probability:  775647672664064.0\n",
      "Attack reconstruction:  61617781866496.0\n",
      "Attack Probability:  739413583724544.0\n",
      "Attack reconstruction:  67936492453888.0\n",
      "Attack Probability:  815238815416320.0\n",
      "Attack reconstruction:  76993773174784.0\n",
      "Attack Probability:  923926251175936.0\n",
      "Attack reconstruction:  73805925974016.0\n",
      "Attack Probability:  885671916994560.0\n",
      "Attack reconstruction:  81824395034624.0\n",
      "Attack Probability:  981892606197760.0\n",
      "Attack reconstruction:  56301811924992.0\n",
      "Attack Probability:  675620434477056.0\n",
      "Attack reconstruction:  86080103645184.0\n",
      "Attack Probability:  1032962116157440.0\n",
      "Attack reconstruction:  52370817941504.0\n",
      "Attack Probability:  628450083733504.0\n",
      "Attack reconstruction:  75887827484672.0\n",
      "Attack Probability:  910653661380608.0\n",
      "Attack reconstruction:  56241657217024.0\n",
      "Attack Probability:  674900020822016.0\n",
      "Attack reconstruction:  77954126184448.0\n",
      "Attack Probability:  935448037818368.0\n",
      "Attack reconstruction:  55349776220160.0\n",
      "Attack Probability:  664197029429248.0\n",
      "Attack reconstruction:  77284555882496.0\n",
      "Attack Probability:  927416381865984.0\n",
      "Attack reconstruction:  67934160420864.0\n",
      "Attack Probability:  815209891495936.0\n",
      "Attack reconstruction:  51888548478976.0\n",
      "Attack Probability:  622662145540096.0\n",
      "Attack reconstruction:  53275269267456.0\n",
      "Attack Probability:  639303197655040.0\n",
      "Attack reconstruction:  55183845359616.0\n",
      "Attack Probability:  662206647631872.0\n",
      "Attack reconstruction:  79836471099392.0\n",
      "Attack Probability:  958037955182592.0\n",
      "Attack reconstruction:  53212455370752.0\n",
      "Attack Probability:  638548357152768.0\n",
      "Attack reconstruction:  63581047488512.0\n",
      "Attack Probability:  762971546451968.0\n",
      "Attack reconstruction:  75597137051648.0\n",
      "Attack Probability:  907167087460352.0\n",
      "Attack reconstruction:  73050800259072.0\n",
      "Attack Probability:  876608126713856.0\n",
      "Attack reconstruction:  76184314445824.0\n",
      "Attack Probability:  914213987942400.0\n",
      "Attack reconstruction:  73436978216960.0\n",
      "Attack Probability:  881243403059200.0\n",
      "Attack reconstruction:  44836463837184.0\n",
      "Attack Probability:  538037800927232.0\n",
      "Attack reconstruction:  65279287623680.0\n",
      "Attack Probability:  783351501815808.0\n",
      "Attack reconstruction:  70781983784960.0\n",
      "Attack Probability:  849383000113152.0\n",
      "Attack reconstruction:  77319595098112.0\n",
      "Attack Probability:  927834470088704.0\n",
      "Attack reconstruction:  83512031969280.0\n",
      "Attack Probability:  1002143511216128.0\n",
      "Attack reconstruction:  55412707557376.0\n",
      "Attack Probability:  664953077891072.0\n",
      "Attack reconstruction:  53956843667456.0\n",
      "Attack Probability:  647481285148672.0\n",
      "Attack reconstruction:  53278385635328.0\n",
      "Attack Probability:  639340711510016.0\n",
      "Attack reconstruction:  57255730872320.0\n",
      "Attack Probability:  687068468477952.0\n",
      "Attack reconstruction:  53633592852480.0\n",
      "Attack Probability:  643602459918336.0\n",
      "Attack reconstruction:  52247438295040.0\n",
      "Attack Probability:  626969259540480.0\n",
      "Attack reconstruction:  56774077972480.0\n",
      "Attack Probability:  681289254436864.0\n",
      "Attack reconstruction:  55185594384384.0\n",
      "Attack Probability:  662226444746752.0\n",
      "Attack reconstruction:  50126986936320.0\n",
      "Attack Probability:  601524598210560.0\n",
      "Attack reconstruction:  52672942047232.0\n",
      "Attack Probability:  632075975655424.0\n",
      "Attack reconstruction:  63492539285504.0\n",
      "Attack Probability:  761910689529856.0\n",
      "Attack reconstruction:  53551820701696.0\n",
      "Attack Probability:  642621529653248.0\n",
      "Attack reconstruction:  51241207988224.0\n",
      "Attack Probability:  614895099838464.0\n",
      "Attack reconstruction:  50648905154560.0\n",
      "Attack Probability:  607785721004032.0\n",
      "Attack reconstruction:  78113266466816.0\n",
      "Attack Probability:  937359566700544.0\n",
      "Attack reconstruction:  57590797041664.0\n",
      "Attack Probability:  691089094737920.0\n",
      "Attack reconstruction:  76681146531840.0\n",
      "Attack Probability:  920173724827648.0\n",
      "Attack reconstruction:  57644467355648.0\n",
      "Attack Probability:  691733138505728.0\n",
      "Attack reconstruction:  72669621911552.0\n",
      "Attack Probability:  872035395829760.0\n",
      "Attack reconstruction:  80996506533888.0\n",
      "Attack Probability:  971956937555968.0\n",
      "Attack reconstruction:  77639712768000.0\n",
      "Attack Probability:  931676385443840.0\n",
      "Attack reconstruction:  54646848618496.0\n",
      "Attack Probability:  655762116313088.0\n",
      "Attack reconstruction:  50739258851328.0\n",
      "Attack Probability:  608871139770368.0\n",
      "Attack reconstruction:  58942570889216.0\n",
      "Attack Probability:  707311790194688.0\n",
      "Attack reconstruction:  60039037452288.0\n",
      "Attack Probability:  720468080328704.0\n",
      "Attack reconstruction:  57149560455168.0\n",
      "Attack Probability:  685793534279680.0\n",
      "Attack reconstruction:  55627271372800.0\n",
      "Attack Probability:  667526300172288.0\n",
      "Attack reconstruction:  82318643429376.0\n",
      "Attack Probability:  987824090251264.0\n",
      "Attack reconstruction:  68492900433920.0\n",
      "Attack Probability:  821914469662720.0\n",
      "Attack reconstruction:  49167753805824.0\n",
      "Attack Probability:  590011669938176.0\n",
      "Attack reconstruction:  82415917727744.0\n",
      "Attack Probability:  988990643634176.0\n",
      "Attack reconstruction:  52432969138176.0\n",
      "Attack Probability:  629195596103680.0\n",
      "Attack reconstruction:  63054217740288.0\n",
      "Attack Probability:  756650092789760.0\n",
      "Attack reconstruction:  75294954225664.0\n",
      "Attack Probability:  903540524449792.0\n",
      "Attack reconstruction:  69678189772800.0\n",
      "Attack Probability:  836137522298880.0\n",
      "Attack reconstruction:  55808402391040.0\n",
      "Attack Probability:  669701499781120.0\n",
      "Attack reconstruction:  72231921123328.0\n",
      "Attack Probability:  866783657459712.0\n",
      "Attack reconstruction:  72703981649920.0\n",
      "Attack Probability:  872447712690176.0\n",
      "Attack reconstruction:  75925358116864.0\n",
      "Attack Probability:  911105169817600.0\n",
      "Attack reconstruction:  79602730926080.0\n",
      "Attack Probability:  955231529598976.0\n",
      "Attack reconstruction:  59749190074368.0\n",
      "Attack Probability:  716990230560768.0\n",
      "Attack reconstruction:  53392189685760.0\n",
      "Attack Probability:  640706376892416.0\n",
      "Attack reconstruction:  70067685425152.0\n",
      "Attack Probability:  840811923111936.0\n",
      "Attack reconstruction:  75143288193024.0\n",
      "Attack Probability:  901717646376960.0\n",
      "Attack reconstruction:  72769622507520.0\n",
      "Attack Probability:  873235235209216.0\n",
      "Attack reconstruction:  51271507640320.0\n",
      "Attack Probability:  615258628554752.0\n",
      "Attack reconstruction:  66715824160768.0\n",
      "Attack Probability:  800589621493760.0\n",
      "Attack reconstruction:  80321231978496.0\n",
      "Attack Probability:  963855958147072.0\n",
      "Attack reconstruction:  56610374287360.0\n",
      "Attack Probability:  679325246423040.0\n",
      "Attack reconstruction:  76457497853952.0\n",
      "Attack Probability:  917487625437184.0\n",
      "Attack reconstruction:  62750906646528.0\n",
      "Attack Probability:  753009772462080.0\n",
      "Attack reconstruction:  55023740387328.0\n",
      "Attack Probability:  660284045787136.0\n",
      "Attack reconstruction:  67226711359488.0\n",
      "Attack Probability:  806720620199936.0\n",
      "Attack reconstruction:  55194633109504.0\n",
      "Attack Probability:  662336838828032.0\n",
      "Attack reconstruction:  55139297656832.0\n",
      "Attack Probability:  661672125530112.0\n",
      "Attack reconstruction:  76384810565632.0\n",
      "Attack Probability:  916617156362240.0\n",
      "Attack reconstruction:  79760948461568.0\n",
      "Attack Probability:  957132522389504.0\n",
      "Attack reconstruction:  84192499073024.0\n",
      "Attack Probability:  1010308982243328.0\n",
      "Attack reconstruction:  74102253551616.0\n",
      "Attack Probability:  889226606411776.0\n",
      "Attack reconstruction:  78213711659008.0\n",
      "Attack Probability:  938564774789120.0\n",
      "Attack reconstruction:  73192408350720.0\n",
      "Attack Probability:  878310208831488.0\n",
      "Attack reconstruction:  50805117812736.0\n",
      "Attack Probability:  609661279535104.0\n",
      "Attack reconstruction:  76212575666176.0\n",
      "Attack Probability:  914551679746048.0\n",
      "Attack reconstruction:  54245105598464.0\n",
      "Attack Probability:  650940075999232.0\n",
      "Attack reconstruction:  53647090122752.0\n",
      "Attack Probability:  643765534457856.0\n",
      "Attack reconstruction:  53798189924352.0\n",
      "Attack Probability:  645578211983360.0\n",
      "Attack reconstruction:  75151232204800.0\n",
      "Attack Probability:  901816699060224.0\n",
      "Attack reconstruction:  57700884938752.0\n",
      "Attack Probability:  692409931399168.0\n",
      "Attack reconstruction:  53394857263104.0\n",
      "Attack Probability:  640738253602816.0\n",
      "Attack reconstruction:  82092000018432.0\n",
      "Attack Probability:  985104369319936.0\n",
      "Attack reconstruction:  51198686134272.0\n",
      "Attack Probability:  614383193423872.0\n",
      "Attack reconstruction:  51864410259456.0\n",
      "Attack Probability:  622372973445120.0\n",
      "Attack reconstruction:  54458591477760.0\n",
      "Attack Probability:  653504305692672.0\n",
      "Attack reconstruction:  81631910035456.0\n",
      "Attack Probability:  979582115119104.0\n",
      "Attack reconstruction:  57274148061184.0\n",
      "Attack Probability:  687288921096192.0\n",
      "Attack reconstruction:  51172610146304.0\n",
      "Attack Probability:  614070868770816.0\n",
      "Attack reconstruction:  69915620933632.0\n",
      "Attack Probability:  838988239732736.0\n",
      "Attack reconstruction:  77911822434304.0\n",
      "Attack Probability:  934940560588800.0\n",
      "Attack reconstruction:  56152993824768.0\n",
      "Attack Probability:  673835540021248.0\n",
      "Attack reconstruction:  75658189340672.0\n",
      "Attack Probability:  907898372751360.0\n",
      "Attack reconstruction:  58639943467008.0\n",
      "Attack Probability:  703680529563648.0\n",
      "Attack reconstruction:  83431635550208.0\n",
      "Attack Probability:  1001179895037952.0\n",
      "Attack reconstruction:  78865976262656.0\n",
      "Attack Probability:  946390003875840.0\n",
      "Attack reconstruction:  87009301364736.0\n",
      "Attack Probability:  1044111247278080.0\n",
      "Attack reconstruction:  73289883975680.0\n",
      "Attack Probability:  879478775480320.0\n",
      "Attack reconstruction:  79908009148416.0\n",
      "Attack Probability:  958894532722688.0\n",
      "Attack reconstruction:  80234955145216.0\n",
      "Attack Probability:  962819126198272.0\n",
      "Attack reconstruction:  68379784249344.0\n",
      "Attack Probability:  820556991561728.0\n",
      "Attack reconstruction:  75367104643072.0\n",
      "Attack Probability:  904406631448576.0\n",
      "Attack reconstruction:  52765707468800.0\n",
      "Attack Probability:  633189110382592.0\n",
      "Attack reconstruction:  77832734638080.0\n",
      "Attack Probability:  933992983429120.0\n",
      "Attack reconstruction:  66945445527552.0\n",
      "Attack Probability:  803346185191424.0\n",
      "Attack reconstruction:  51614169694208.0\n",
      "Attack Probability:  619370254434304.0\n",
      "Attack reconstruction:  66722757345280.0\n",
      "Attack Probability:  800672836485120.0\n",
      "Attack reconstruction:  46715814019072.0\n",
      "Attack Probability:  560590506426368.0\n",
      "Attack reconstruction:  72742174982144.0\n",
      "Attack Probability:  872905260924928.0\n",
      "Attack reconstruction:  76616730411008.0\n",
      "Attack Probability:  919400563605504.0\n",
      "Attack reconstruction:  82291346898944.0\n",
      "Attack Probability:  987495189708800.0\n",
      "Attack reconstruction:  51741315825664.0\n",
      "Attack Probability:  620896377110528.0\n",
      "Attack reconstruction:  67984089415680.0\n",
      "Attack Probability:  815808703889408.0\n",
      "Attack reconstruction:  75781157945344.0\n",
      "Attack Probability:  909372955820032.0\n",
      "Attack reconstruction:  73436634284032.0\n",
      "Attack Probability:  881239644962816.0\n",
      "Attack reconstruction:  76256783630336.0\n",
      "Attack Probability:  915080162050048.0\n",
      "Attack reconstruction:  85719594827776.0\n",
      "Attack Probability:  1028635741913088.0\n",
      "Attack reconstruction:  71272935456768.0\n",
      "Attack Probability:  855274621501440.0\n",
      "Attack reconstruction:  52839254589440.0\n",
      "Attack Probability:  634071189291008.0\n",
      "Attack reconstruction:  66508482936832.0\n",
      "Attack Probability:  798102164340736.0\n",
      "Attack reconstruction:  58107489157120.0\n",
      "Attack Probability:  697289416900608.0\n",
      "Attack reconstruction:  76572497281024.0\n",
      "Attack Probability:  918868658749440.0\n",
      "Attack reconstruction:  70178754789376.0\n",
      "Attack Probability:  842145980219392.0\n",
      "Attack reconstruction:  76146565709824.0\n",
      "Attack Probability:  913759996477440.0\n",
      "Attack reconstruction:  57726390501376.0\n",
      "Attack Probability:  692717558431744.0\n",
      "Attack reconstruction:  54710052585472.0\n",
      "Attack Probability:  656520110931968.0\n",
      "Attack reconstruction:  66039350034432.0\n",
      "Attack Probability:  792474146570240.0\n",
      "Attack reconstruction:  51161008701440.0\n",
      "Attack Probability:  613931215224832.0\n",
      "Attack reconstruction:  54410679943168.0\n",
      "Attack Probability:  652927035244544.0\n",
      "Attack reconstruction:  52394679336960.0\n",
      "Attack Probability:  628738114977792.0\n",
      "Attack reconstruction:  68460377800704.0\n",
      "Attack Probability:  821522889441280.0\n",
      "Attack reconstruction:  79108297981952.0\n",
      "Attack Probability:  949299374456832.0\n",
      "Attack reconstruction:  82522704707584.0\n",
      "Attack Probability:  990273026916352.0\n",
      "Attack reconstruction:  87573544304640.0\n",
      "Attack Probability:  1050883404070912.0\n",
      "Attack reconstruction:  54811726708736.0\n",
      "Attack Probability:  657741425147904.0\n",
      "Attack reconstruction:  80060178497536.0\n",
      "Attack Probability:  960721705762816.0\n",
      "Attack reconstruction:  76352875134976.0\n",
      "Attack Probability:  916235038490624.0\n",
      "Attack reconstruction:  83211921129472.0\n",
      "Attack Probability:  998543590424576.0\n",
      "Attack reconstruction:  76052177092608.0\n",
      "Attack Probability:  912626863308800.0\n",
      "Attack reconstruction:  77579667111936.0\n",
      "Attack Probability:  930955837571072.0\n",
      "Attack reconstruction:  57722259111936.0\n",
      "Attack Probability:  692667293892608.0\n",
      "Attack reconstruction:  53792930267136.0\n",
      "Attack Probability:  645515330977792.0\n",
      "Attack reconstruction:  81009978638336.0\n",
      "Attack Probability:  972120079204352.0\n",
      "Attack reconstruction:  75501917962240.0\n",
      "Attack Probability:  906023686635520.0\n",
      "Attack reconstruction:  51979325800448.0\n",
      "Attack Probability:  623750382878720.0\n",
      "Attack reconstruction:  53548679168000.0\n",
      "Attack Probability:  642584552669184.0\n",
      "Attack reconstruction:  81837565149184.0\n",
      "Attack Probability:  982051788423168.0\n",
      "Attack reconstruction:  53600545931264.0\n",
      "Attack Probability:  643206651838464.0\n",
      "Attack reconstruction:  52309069398016.0\n",
      "Attack Probability:  627710342725632.0\n",
      "Attack reconstruction:  53567750668288.0\n",
      "Attack Probability:  642812722806784.0\n",
      "Attack reconstruction:  73723935719424.0\n",
      "Attack Probability:  884686624653312.0\n",
      "Attack reconstruction:  65591788437504.0\n",
      "Attack Probability:  787102149115904.0\n",
      "Attack reconstruction:  69168044965888.0\n",
      "Attack Probability:  830017261010944.0\n",
      "Attack reconstruction:  46278247448576.0\n",
      "Attack Probability:  555339405590528.0\n",
      "Attack reconstruction:  81741414924288.0\n",
      "Attack Probability:  980896643547136.0\n",
      "Attack reconstruction:  67721425321984.0\n",
      "Attack Probability:  812657137418240.0\n",
      "Attack reconstruction:  56357600362496.0\n",
      "Attack Probability:  676291590225920.0\n",
      "Attack reconstruction:  65357943406592.0\n",
      "Attack Probability:  784295387987968.0\n",
      "Attack reconstruction:  77141278457856.0\n",
      "Attack Probability:  925696583008256.0\n",
      "Attack reconstruction:  66559334678528.0\n",
      "Attack Probability:  798709902213120.0\n",
      "Attack reconstruction:  55228141404160.0\n",
      "Attack Probability:  662738284052480.0\n",
      "Attack reconstruction:  67430357401600.0\n",
      "Attack Probability:  809163651284992.0\n",
      "Attack reconstruction:  60229660180480.0\n",
      "Attack Probability:  722756626808832.0\n",
      "Attack reconstruction:  86876073492480.0\n",
      "Attack Probability:  1042512714137600.0\n",
      "Attack reconstruction:  54929595039744.0\n",
      "Attack Probability:  659154804932608.0\n",
      "Attack reconstruction:  76322726477824.0\n",
      "Attack Probability:  915872382189568.0\n",
      "Attack reconstruction:  77692300951552.0\n",
      "Attack Probability:  932307611418624.0\n",
      "Attack reconstruction:  57744128212992.0\n",
      "Attack Probability:  692929823768576.0\n",
      "Attack reconstruction:  78502766313472.0\n",
      "Attack Probability:  942035175473152.0\n",
      "Attack reconstruction:  73865686417408.0\n",
      "Attack Probability:  886388773879808.0\n",
      "Attack reconstruction:  78554473693184.0\n",
      "Attack Probability:  942653583654912.0\n",
      "Attack reconstruction:  52189816946688.0\n",
      "Attack Probability:  626275890757632.0\n",
      "Attack reconstruction:  52030928322560.0\n",
      "Attack Probability:  624371274088448.0\n",
      "Attack reconstruction:  91549878714368.0\n",
      "Attack Probability:  1098597672157184.0\n",
      "Attack reconstruction:  80462017986560.0\n",
      "Attack Probability:  965544954036224.0\n",
      "Attack reconstruction:  78904077320192.0\n",
      "Attack Probability:  946848290308096.0\n",
      "Attack reconstruction:  56845985120256.0\n",
      "Attack Probability:  682153482387456.0\n",
      "Attack reconstruction:  73843808927744.0\n",
      "Attack Probability:  886124432064512.0\n",
      "Attack reconstruction:  76553304145920.0\n",
      "Attack Probability:  918639884632064.0\n",
      "Attack reconstruction:  80403339673600.0\n",
      "Attack Probability:  964840176746496.0\n",
      "Attack reconstruction:  72478554587136.0\n",
      "Attack Probability:  869742084620288.0\n",
      "Attack reconstruction:  53181761454080.0\n",
      "Attack Probability:  638181003231232.0\n",
      "Attack reconstruction:  53620124942336.0\n",
      "Attack Probability:  643441801297920.0\n",
      "Attack reconstruction:  77693785735168.0\n",
      "Attack Probability:  932325395267584.0\n",
      "Attack reconstruction:  68150838165504.0\n",
      "Attack Probability:  817810024431616.0\n",
      "Attack reconstruction:  58483571425280.0\n",
      "Attack Probability:  701802622222336.0\n",
      "Attack reconstruction:  55395443802112.0\n",
      "Attack Probability:  664745375956992.0\n",
      "Attack reconstruction:  76472890949632.0\n",
      "Attack Probability:  917674993385472.0\n",
      "Attack reconstruction:  72735422152704.0\n",
      "Attack Probability:  872826743554048.0\n",
      "Attack reconstruction:  49915292024832.0\n",
      "Attack Probability:  598984192163840.0\n",
      "Attack reconstruction:  81796117037056.0\n",
      "Attack Probability:  981554377523200.0\n",
      "Attack reconstruction:  75783934574592.0\n",
      "Attack Probability:  909406980014080.0\n",
      "Attack reconstruction:  66427121827840.0\n",
      "Attack Probability:  797126133022720.0\n",
      "Attack reconstruction:  51762765496320.0\n",
      "Attack Probability:  621152665862144.0\n",
      "Attack reconstruction:  66421648261120.0\n",
      "Attack Probability:  797060567662592.0\n",
      "Attack reconstruction:  73898796253184.0\n",
      "Attack Probability:  886785588592640.0\n",
      "Attack reconstruction:  54558101340160.0\n",
      "Attack Probability:  654697903947776.0\n",
      "Attack reconstruction:  84076501401600.0\n",
      "Attack Probability:  1008918889234432.0\n",
      "Attack reconstruction:  53792884129792.0\n",
      "Attack Probability:  645514458562560.0\n",
      "Attack reconstruction:  79632552427520.0\n",
      "Attack Probability:  955589689606144.0\n",
      "Attack reconstruction:  52768886751232.0\n",
      "Attack Probability:  633226288693248.0\n",
      "Attack reconstruction:  72991257919488.0\n",
      "Attack Probability:  875895699013632.0\n",
      "Attack reconstruction:  70230168567808.0\n",
      "Attack Probability:  842762912006144.0\n",
      "Attack reconstruction:  48261821890560.0\n",
      "Attack Probability:  579141778800640.0\n",
      "Attack reconstruction:  55916833538048.0\n",
      "Attack Probability:  670999854972928.0\n",
      "Attack reconstruction:  56934602375168.0\n",
      "Attack Probability:  683214607745024.0\n",
      "Attack reconstruction:  50368528515072.0\n",
      "Attack Probability:  604423298482176.0\n",
      "Attack reconstruction:  46603469586432.0\n",
      "Attack Probability:  559241584705536.0\n",
      "Attack reconstruction:  64063585386496.0\n",
      "Attack Probability:  768764450701312.0\n",
      "Attack reconstruction:  72863063212032.0\n",
      "Attack Probability:  874357563850752.0\n",
      "Attack reconstruction:  74250329260032.0\n",
      "Attack Probability:  891005393960960.0\n",
      "Attack reconstruction:  56474596278272.0\n",
      "Attack Probability:  677694433918976.0\n",
      "Attack reconstruction:  75554078326784.0\n",
      "Attack Probability:  906648671485952.0\n",
      "Attack reconstruction:  66194820300800.0\n",
      "Attack Probability:  794337222852608.0\n",
      "Attack reconstruction:  80611595255808.0\n",
      "Attack Probability:  967338237100032.0\n",
      "Attack reconstruction:  52349531848704.0\n",
      "Attack Probability:  628194398961664.0\n",
      "Attack reconstruction:  51980772835328.0\n",
      "Attack Probability:  623769643122688.0\n",
      "Attack reconstruction:  82991988604928.0\n",
      "Attack Probability:  995904735674368.0\n",
      "Attack reconstruction:  51340466192384.0\n",
      "Attack Probability:  616084738670592.0\n",
      "Attack reconstruction:  52925627891712.0\n",
      "Attack Probability:  635105940865024.0\n",
      "Attack reconstruction:  71611440955392.0\n",
      "Attack Probability:  859338130325504.0\n",
      "Attack reconstruction:  78883994992640.0\n",
      "Attack Probability:  946607369486336.0\n",
      "Attack reconstruction:  57596803284992.0\n",
      "Attack Probability:  691162176290816.0\n",
      "Attack reconstruction:  65335554211840.0\n",
      "Attack Probability:  784026080116736.0\n",
      "Attack reconstruction:  70801244028928.0\n",
      "Attack Probability:  849615062564864.0\n",
      "Attack reconstruction:  55026743508992.0\n",
      "Attack Probability:  660321291206656.0\n",
      "Attack reconstruction:  65641805512704.0\n",
      "Attack Probability:  787700558856192.0\n",
      "Attack reconstruction:  69439231885312.0\n",
      "Attack Probability:  833269826322432.0\n",
      "Attack reconstruction:  50860570705920.0\n",
      "Attack Probability:  610327267901440.0\n",
      "Attack reconstruction:  51880528969728.0\n",
      "Attack Probability:  622566381191168.0\n",
      "Attack reconstruction:  53720951816192.0\n",
      "Attack Probability:  644650901700608.0\n",
      "Attack reconstruction:  73693921280000.0\n",
      "Attack Probability:  884326384271360.0\n",
      "Attack reconstruction:  55879357431808.0\n",
      "Attack Probability:  670552775720960.0\n",
      "Attack reconstruction:  50460538961920.0\n",
      "Attack Probability:  605526433988608.0\n",
      "Attack reconstruction:  64110112800768.0\n",
      "Attack Probability:  769321387163648.0\n",
      "Attack reconstruction:  52672963018752.0\n",
      "Attack Probability:  632074096607232.0\n",
      "Attack reconstruction:  77228880691200.0\n",
      "Attack Probability:  926746299858944.0\n",
      "Attack reconstruction:  53435986608128.0\n",
      "Attack Probability:  641231839297536.0\n",
      "Attack reconstruction:  79776123453440.0\n",
      "Attack Probability:  957312776798208.0\n",
      "Attack reconstruction:  68864201195520.0\n",
      "Attack Probability:  826370364014592.0\n",
      "Attack reconstruction:  52585943793664.0\n",
      "Attack Probability:  631032499929088.0\n",
      "Attack reconstruction:  73006491631616.0\n",
      "Attack Probability:  876077631143936.0\n",
      "Attack reconstruction:  75642729136128.0\n",
      "Attack Probability:  907712346980352.0\n",
      "Attack reconstruction:  77307976876032.0\n",
      "Attack Probability:  927696292937728.0\n",
      "Attack reconstruction:  66853070176256.0\n",
      "Attack Probability:  802236473016320.0\n",
      "Attack reconstruction:  69875015876608.0\n",
      "Attack Probability:  838498949005312.0\n",
      "Attack reconstruction:  51510155149312.0\n",
      "Attack Probability:  618122767761408.0\n",
      "Attack reconstruction:  89290541367296.0\n",
      "Attack Probability:  1071486899060736.0\n",
      "Attack reconstruction:  58300670410752.0\n",
      "Attack Probability:  699608900567040.0\n",
      "Attack reconstruction:  54232858230784.0\n",
      "Attack Probability:  650795590615040.0\n",
      "Attack reconstruction:  64286017716224.0\n",
      "Attack Probability:  771432363589632.0\n",
      "Attack reconstruction:  78956480954368.0\n",
      "Attack Probability:  947478039887872.0\n",
      "Attack reconstruction:  53513832890368.0\n",
      "Attack Probability:  642165256486912.0\n",
      "Attack reconstruction:  92507203436544.0\n",
      "Attack Probability:  1110088857157632.0\n",
      "Attack reconstruction:  51026572869632.0\n",
      "Attack Probability:  612320266944512.0\n",
      "Attack reconstruction:  76505967230976.0\n",
      "Attack Probability:  918070197485568.0\n",
      "Attack reconstruction:  73246238048256.0\n",
      "Attack Probability:  878956131647488.0\n",
      "Attack reconstruction:  74009685262336.0\n",
      "Attack Probability:  888117967978496.0\n",
      "Attack reconstruction:  76446970150912.0\n",
      "Attack Probability:  917363541147648.0\n",
      "Attack reconstruction:  81463710056448.0\n",
      "Attack Probability:  977565896409088.0\n",
      "Attack reconstruction:  73036984221696.0\n",
      "Attack Probability:  876444515303424.0\n",
      "Attack reconstruction:  75250528157696.0\n",
      "Attack Probability:  903006941872128.0\n",
      "Attack reconstruction:  53701578326016.0\n",
      "Attack Probability:  644418235269120.0\n",
      "Attack reconstruction:  47255063101440.0\n",
      "Attack Probability:  567062183280640.0\n",
      "Attack reconstruction:  57213401956352.0\n",
      "Attack Probability:  686561259683840.0\n",
      "Attack reconstruction:  75651755278336.0\n",
      "Attack Probability:  907821600210944.0\n",
      "Attack reconstruction:  54055103627264.0\n",
      "Attack Probability:  648660454998016.0\n",
      "Attack reconstruction:  52075375362048.0\n",
      "Attack Probability:  624903782924288.0\n",
      "Attack reconstruction:  79338842095616.0\n",
      "Attack Probability:  952065400504320.0\n",
      "Attack reconstruction:  77186157510656.0\n",
      "Attack Probability:  926234125008896.0\n",
      "Attack reconstruction:  82452466892800.0\n",
      "Attack Probability:  989431481761792.0\n",
      "Attack reconstruction:  44356757094400.0\n",
      "Attack Probability:  532281336791040.0\n",
      "Attack reconstruction:  57939133988864.0\n",
      "Attack Probability:  695270580944896.0\n",
      "Attack reconstruction:  52792135778304.0\n",
      "Attack Probability:  633505193132032.0\n",
      "Attack reconstruction:  53386296688640.0\n",
      "Attack Probability:  640636650782720.0\n",
      "Attack reconstruction:  79219270877184.0\n",
      "Attack Probability:  950630612992000.0\n",
      "Attack reconstruction:  77091693395968.0\n",
      "Attack Probability:  925099985207296.0\n",
      "Attack reconstruction:  52873194897408.0\n",
      "Attack Probability:  634479613837312.0\n",
      "Attack reconstruction:  77912703238144.0\n",
      "Attack Probability:  934952036204544.0\n",
      "Attack reconstruction:  74089200877568.0\n",
      "Attack Probability:  889071652044800.0\n",
      "Anomaly Detection completed in : 3.3177 seconds\n",
      "######### Anomalies & Threshold Summary ############\n",
      "------ Reconstruction Error -------\n",
      "Anomaly reconstruction ERROR threshold:  978036.4780\n",
      "Number of anomalies detected using ERROR: 1599.0\n",
      "------ ------------------- -------\n",
      "------ Reconstruction Probability -------\n",
      "Anomaly reconstruction PROBABILITY threshold:  11736459.1796\n",
      "Number of anomalies detected using PROBABILITY: 1599.0\n",
      "------ ------------------- -------\n",
      "######### ############################ ############\n",
      "######### Reconstruction Error Performance ############\n",
      "Confusion Matrix:\n",
      "[[  0 799]\n",
      " [  0 800]]\n",
      "\n",
      "Performance Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       799\n",
      "           1       0.50      1.00      0.67       800\n",
      "\n",
      "    accuracy                           0.50      1599\n",
      "   macro avg       0.25      0.50      0.33      1599\n",
      "weighted avg       0.25      0.50      0.33      1599\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### ############################ ############\n",
      "######### Reconstruction Probability Performance ############\n",
      "Confusion Matrix:\n",
      "[[  0 799]\n",
      " [  0 800]]\n",
      "\n",
      "Performance Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       799\n",
      "           1       0.50      1.00      0.67       800\n",
      "\n",
      "    accuracy                           0.50      1599\n",
      "   macro avg       0.25      0.50      0.33      1599\n",
      "weighted avg       0.25      0.50      0.33      1599\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### ############################ ############\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Resources/Models/SEMI-Supervised-VAE_EPOCHS17_LD10_BETA3_NT50000_INPUT24_12-42.keras\"\n",
    "model_name = \"EmbeddingBetaVAE\"\n",
    "#model_path = best_model_path\n",
    "reconstruction_AD = True\n",
    "latent_AD = False\n",
    "n_rows_train = 50000\n",
    "reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_test(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD,val_dataset2 = test_threshold_dataset)\n",
    "results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5e603307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Mean and Variances completed in: 2.0629 seconds\n",
      "Get Mean and Variances completed in: 1.2815 seconds\n",
      "Get Mean and Variances completed in: 0.5997 seconds\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Resources/Models/BetaVAE_EPOCHS98_LD100_BETA50_NT50000_INPUT24_13-22.keras\"\n",
    "\n",
    "mean_train, variance_train = get_mean_variances(train_dataset, test = False, load_vae= None, model_path= model_path)\n",
    "mixed_means, mixed_variances, mixed_labels = get_mean_variances(test_dataset, test = True, load_vae= None, model_path= model_path)\n",
    "mixed2_means, mixed2_variances, mixed2_labels = get_mean_variances(test_threshold_dataset, test = True, load_vae= None, model_path= model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b198ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_means = mixed_means[:5000]\n",
    "mixed_variances = mixed_variances[:5000]\n",
    "mixed_labels = mixed_labels[:5000]\n",
    "\n",
    "print(len(mean_train))\n",
    "print(sum(mixed_labels), len(mixed_labels) - sum(mixed_labels))\n",
    "print(sum(mixed2_labels), len(mixed2_labels) - sum(mixed2_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290f04a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[490], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m scaled_features, scaler \u001b[38;5;241m=\u001b[39m prepare_features(np\u001b[38;5;241m.\u001b[39marray(mean_train), np\u001b[38;5;241m.\u001b[39marray(variance_train))\n\u001b[0;32m      3\u001b[0m mixed_scaled_features, mixed_scaler \u001b[38;5;241m=\u001b[39m prepare_features(np\u001b[38;5;241m.\u001b[39marray(mixed_means), np\u001b[38;5;241m.\u001b[39marray(mixed_variances))\n\u001b[1;32m----> 5\u001b[0m iso_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_isolation_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontamination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#svm_model = train_one_class_svm(scaled_features, nu=0.00001, kernel='rbf', gamma='scale')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#hdbscan_model = train_hdbscan_detector(scaled_features,min_cluster_size=500,min_samples=500, metric='cosine')\u001b[39;00m\n\u001b[0;32m     10\u001b[0m iso_anomaly_mask, iso_anomaly_scores \u001b[38;5;241m=\u001b[39m detect_anomalies_isolation_forest(iso_model, mixed_scaled_features)\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\Desktop\\MasterThesis\\github_USE_THIS!!\\MasterThesis\\clustering.py:66\u001b[0m, in \u001b[0;36mtrain_isolation_forest\u001b[1;34m(normal_features, contamination, n_estimators, random_state)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Initialize and fit Isolation Forest\u001b[39;00m\n\u001b[0;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m IsolationForest(\n\u001b[0;32m     60\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     61\u001b[0m     contamination\u001b[38;5;241m=\u001b[39mcontamination,\n\u001b[0;32m     62\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m     63\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Use all available cores\u001b[39;00m\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 66\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsolation Forest training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:317\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    Fit estimator.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# Pre-sort indices to avoid that each individual tree of the\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;66;03m# ensemble sorts the indices.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m         X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\SCHUGD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1093\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1088\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1092\u001b[0m             )\n\u001b[1;32m-> 1093\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#print(np.array(mean_train).min())\n",
    "scaled_features, scaler = prepare_features(np.array(mean_train), np.array(variance_train))\n",
    "mixed_scaled_features, mixed_scaler = prepare_features(np.array(mixed_means), np.array(mixed_variances))\n",
    "\n",
    "iso_model = train_isolation_forest(scaled_features, contamination=0.05, n_estimators=10000, random_state=42)\n",
    "#svm_model = train_one_class_svm(scaled_features, nu=0.00001, kernel='rbf', gamma='scale')\n",
    "hdbscan_model = train_hdbscan_detector(scaled_features,min_cluster_size=500,min_samples=500, metric='cosine')\n",
    "\n",
    "\n",
    "iso_anomaly_mask, iso_anomaly_scores = detect_anomalies_isolation_forest(iso_model, mixed_scaled_features)\n",
    "#svm_anomaly_mask, svm_anomaly_scores = detect_anomalies_one_class_svm(svm_model, mixed_scaled_features)\n",
    "#hdbscan_outliers, test_labels, strengths = detect_anomalies_hdbscan(hdbscan_model , mixed_scaled_features)\n",
    "\n",
    "iso_pca = visualize_anomalies(mixed_scaled_features, iso_anomaly_mask, \"Isolation Forest tSNE\")\n",
    "#svm_pca = visualize_anomalies(mixed_scaled_features, svm_anomaly_mask, \"SVM TSNE\")\n",
    "\n",
    "#hdbscan_pca = visualize_anomalies(mixed_scaled_features, hdbscan_outliers,\"HDBSCAN TSNE\")\n",
    "#visualize_results(scaled_features, mixed_scaled_features, hdbscan_outliers)\n",
    "\n",
    "evaluate_anomaly_detector(iso_anomaly_scores, mixed_labels, \"Isolation Forest Accuracy\")\n",
    "#evaluate_anomaly_detector(svm_anomaly_scores, mixed_labels, \"SVM Accuracy\")\n",
    "#evaluate_anomaly_detector(hdbscan_outliers, mixed_labels, \"HDBSCAN Accuracy\")\n",
    "\n",
    "#evaluation = evaluate_hdbscan_detector(mixed_labels, anomaly_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89c3ee",
   "metadata": {},
   "source": [
    "**BENCHMARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "526a994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate model size in memory: 0.78 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.load_model('./Resources/Models/SEMI-Supervised-VAE_EPOCHS36_LD10_BETA3_NT50000_INPUT24_14-57.keras')\n",
    "\n",
    "total_params = model.count_params()\n",
    "memory_in_bytes = total_params * 4  # float32 = 4 bytes\n",
    "memory_in_mb = memory_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Approximate model size in memory: {memory_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c98f8187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 205,357\n",
      "Non-trainable parameters: 4\n",
      "Total parameters: 205,361\n",
      "Failed to calculate FLOPs: Exception encountered when calling Sequential.call().\n",
      "\n",
      "\u001b[1mInvalid input shape for input [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]. Expected shape (None, 50, 24), but input has incompatible shape (1, 24)\u001b[0m\n",
      "\n",
      "Arguments received by Sequential.call():\n",
      "  • inputs=tf.Tensor(shape=(1, 24), dtype=float32)\n",
      "  • training=None\n",
      "  • mask=None\n",
      "Returning parameter counts only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(205357), np.int64(4), None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_params_and_estimate_flops(model, input_shape):\n",
    "    # Count parameters\n",
    "    trainable_params = sum(np.prod(v.shape) for v in model.trainable_variables)\n",
    "    non_trainable_params = sum(np.prod(v.shape) for v in model.non_trainable_variables)\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # For FLOPs estimation, we need to create a proper input shape\n",
    "    # If input_shape is just a dimension, we need to create a proper shape\n",
    "    if isinstance(input_shape, int):\n",
    "        # For VAE models, we typically need to know the input structure\n",
    "        # Let's try to infer it from the model's input spec if possible\n",
    "        try:\n",
    "            # Try to get input shape from model specs\n",
    "            input_spec = model.input_spec\n",
    "            if input_spec:\n",
    "                full_input_shape = input_spec.shape\n",
    "            else:\n",
    "                # Assume a batch dimension and the provided dimension\n",
    "                full_input_shape = (1, input_shape)\n",
    "        except:\n",
    "            # If we can't determine, use a default batch shape\n",
    "            full_input_shape = (1, input_shape)\n",
    "    else:\n",
    "        full_input_shape = input_shape\n",
    "    \n",
    "    try:\n",
    "        # Create a concrete function for the model\n",
    "        dummy_input = tf.ones(full_input_shape, dtype=tf.float32)\n",
    "        \n",
    "        # Call the model once to ensure all variables are created\n",
    "        _ = model(dummy_input)\n",
    "        \n",
    "        concrete_func = tf.function(model).get_concrete_function(\n",
    "            tf.TensorSpec(full_input_shape, tf.float32)\n",
    "        )\n",
    "        \n",
    "        # For newer TF versions, use the recommended approach\n",
    "        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "        frozen_func, _ = convert_variables_to_constants_v2(concrete_func)\n",
    "        \n",
    "        # Calculate FLOPs\n",
    "        from tensorflow.compat.v1.profiler import profile, ProfileOptionBuilder\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = ProfileOptionBuilder.float_operation()\n",
    "        flops = profile(\n",
    "            graph=frozen_func.graph,\n",
    "            run_meta=run_meta, \n",
    "            cmd='op', \n",
    "            options=opts\n",
    "        )\n",
    "        \n",
    "        print(f\"FLOPs: {flops.total_float_ops:,}\")\n",
    "        return trainable_params, non_trainable_params, flops.total_float_ops\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate FLOPs: {e}\")\n",
    "        print(\"Returning parameter counts only\")\n",
    "        return trainable_params, non_trainable_params, None\n",
    "\n",
    "\n",
    "count_params_and_estimate_flops(vae,  input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def monitor_memory(training_function, *args, **kwargs):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage = []\n",
    "    \n",
    "    # Start a background thread to monitor memory\n",
    "    def memory_monitor():\n",
    "        while monitoring:\n",
    "            memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    import threading\n",
    "    monitoring = True\n",
    "    monitor_thread = threading.Thread(target=memory_monitor)\n",
    "    monitor_thread.start()\n",
    "    \n",
    "    # Run the training function\n",
    "    start_time = time.time()\n",
    "    result = training_function(*args, **kwargs)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Stop monitoring\n",
    "    monitoring = False\n",
    "    monitor_thread.join()\n",
    "    \n",
    "    # Plot memory usage\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(memory_usage)\n",
    "    plt.title('Memory Usage During Training')\n",
    "    plt.xlabel('Time (0.1s intervals)')\n",
    "    plt.ylabel('Memory (MB)')\n",
    "    plt.savefig('memory_usage.png')\n",
    "    \n",
    "    print(f\"Peak memory usage: {max(memory_usage):.2f} MB\")\n",
    "    print(f\"Average memory usage: {sum(memory_usage)/len(memory_usage):.2f} MB\")\n",
    "    print(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    return result, memory_usage, execution_time\n",
    "\n",
    "# Usage example:\n",
    "# result, memory_data, exec_time = monitor_memory(train_model_semi, vae, optimizer, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c6f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

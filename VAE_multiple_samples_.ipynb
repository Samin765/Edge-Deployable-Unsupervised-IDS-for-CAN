{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545506f7",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861000fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Changes AWS to True if on SageMaker Instance and set S3 BUCKET and Key accordingly\n",
    "AWS = False\n",
    "BUCKET = 'eu-west-1'\n",
    "REGION = 'ml-can-ids-logs'\n",
    "\n",
    "# Import Functions\n",
    "import setuptools.dist\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "import anomaly_detection_functions\n",
    "import feature_selection\n",
    "import train\n",
    "\n",
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "reload(feature_selection)\n",
    "reload(train)\n",
    "from utils import plot_loss_curve, plot_pca, plot_tsne, get_confusion_matrix, get_latent_representations_label, analyze_latent_variance, analyze_kl_divergence, linear_annealing, save_results_to_excel, save_trained_model, get_s3_client, check_dataset\n",
    "from anomaly_detection_functions import get_threshold_from_train, anomaly_detection, get_anomaly_detection_accuracy\n",
    "from feature_selection import feature_selection_preparation, convert_to_tensorflow\n",
    "from train import train_model\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import keras\n",
    "#from tsfresh import extract_features\n",
    "#from tsfresh.feature_extraction.settings import EfficientFCParameters\n",
    "#from tsfresh import select_features\n",
    "#from tsfresh.utilities.dataframe_functions import impute\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from importlib import reload\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "if AWS:\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # No wrapping, long rows won't be cut off\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content (especially useful for long strings)\n",
    "\n",
    "# Remove this after testing/debugging\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  \n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"✅ Using GPU\")\n",
    "    device = \"/GPU:0\"\n",
    "else:\n",
    "    print(\"❌ Using CPU\")\n",
    "    device = \"/CPU:0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715700",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2753d",
   "metadata": {},
   "source": [
    "**PATH FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS:\n",
    "    s3 = get_s3_client(REGION, BUCKET, True)\n",
    "    \n",
    "    # Get S3 Object \n",
    "    channel2logs = s3.get_object(Bucket = BUCKET, Key= 'channel2Logs.csv')\n",
    "    dos_attack_channel2 = s3.get_object(Bucket = BUCKET, Key= 'dos_attack_channel2.csv')\n",
    "    replay_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'replay_attack_channel2.csv') \n",
    "    spoofing_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'new_spoofing_attack_channel2.csv') \n",
    "\n",
    "    channel2logs = channel2logs['Body'].read().decode('utf-8')\n",
    "    dos_attack_channel2 = dos_attack_channel2['Body'].read().decode('utf-8')\n",
    "    replay_attack_channel2 = replay_attack_channel2['Body'].read().decode('utf-8')\n",
    "    spoofing_attack_channel2 = spoofing_attack_channel2['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Get Content\n",
    "    channel2logs = StringIO(channel2logs)\n",
    "    dos_attack_channel2 = StringIO(dos_attack_channel2)\n",
    "    replay_attack_channel2 = StringIO(replay_attack_channel2)\n",
    "    spoofing_attack_channel2 = StringIO(spoofing_attack_channel2)\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = dos_attack_channel2 # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = replay_attack_channel2 # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = spoofing_attack_channel2 # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel2_csv_path = channel2logs # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = \"\" # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = \"\" # Green Channel\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"\"\n",
    "else:\n",
    "    # Unprocessed Normal and Attack Data\n",
    "    preprocessed_normal_csv_path = './Dataset/Tw22206_L003_with_ecu_channel.csv'  # Normal Unprocessed\n",
    "    preprocessed_DoS_csv_path = './Dataset/Attack_Logs/dos_attack.csv'  # Dos Unprocessed\n",
    "    preprocessed_Fuzzy_csv_path = './Dataset/Attack_Logs/fuzzy_attack.csv'  # Fuzzy Unprocessed\n",
    "    preprocessed_Replay_csv_path = './Dataset/Attack_Logs/replay_attack.csv'  # Replay Unprocessed - Test\n",
    "    preprocessed_Spoofing_csv_path = './Dataset/Attack_Logs/spoofing_attack.csv'  # Spoofing Unprocessed\n",
    "    preprocessed_Suspension_csv_path = './Dataset/Attack_Logs/suspension_attack.csv'  # Suspension Unprocessed - Hardest Attack Type\n",
    "\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = './Dataset/Attack_Logs/dos_attack_channel2.csv'  # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = './Dataset/Attack_Logs/replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Suspension_channel2_csv_path = './Dataset/Attack_Logs/suspension_attack_channel2.csv'  # Suspension on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_new_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/new_spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel0_csv_path = './Dataset/Channel_Logs/channel0Logs.csv'\n",
    "    preprocessed_normal_channel2_csv_path = './Dataset/Channel_Logs/channel2Logs.csv'  # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = './Dataset/Channel_Logs/channel4Logs.csv'  # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = './Dataset/Channel_Logs/channel5Logs.csv'  # Green Channel\n",
    "\n",
    "\n",
    "    # Preprocessed Dataframe Data\n",
    "    processeddataframe_normal_csv_path = './Dataset/Processed_Dataframes/train_dataframe.csv'  # Normal CSV Dataframe (Turns Lists into Strings)\n",
    "    processeddataframe_DoS_csv_path = './Dataset/Processed_Dataframes/test_DoS_dataframe.csv'  # DoS CSV Dataframe (Turns Lists into Strings)\n",
    "\n",
    "    # Preprocessed Pickle Data\n",
    "    processeddataframe_normal_pickle_path = './Dataset/Processed_Dataframes/train_Normal_dataframePickle.pkl'  # Normal Pickle Dataframe\n",
    "    processeddataframe_DoS_pickle_path = './Dataset/Processed_Dataframes/test_DoS_dataframePickle.pkl'  # DoS Pickle Dataframe\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = './Code/Models/BEST_LSTM_VAE_LD30_Beta25_NT800000_21-37.keras'\n",
    "\n",
    "\n",
    "\n",
    "    # PRELOAD Dataframe for Debug\n",
    "    DEBUG = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff978e0",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49866c78",
   "metadata": {},
   "source": [
    "Tsfresh parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fc_parameters = {\n",
    "    # Statistical Features\n",
    "    \"mean\": None,\n",
    "    \"skewness\": None,\n",
    "    \"kurtosis\": None,\n",
    "    \n",
    "    # Temporal Dependency Features\n",
    "    \"autocorrelation\": [{\"lag\": 1}],  # Detects repeated patterns (useful for replay attacks)\n",
    "    \"agg_autocorrelation\": [{\"f_agg\": \"mean\", \"maxlag\": 5}],  # Aggregate autocorrelation over multiple lags\n",
    "    \"agg_linear_trend\": [{\"attr\": \"slope\", \"chunk_len\": 5, \"f_agg\": \"mean\"}],  # Detects trends in timing\n",
    "    \n",
    "    # Frequency Features\n",
    "    \"fft_coefficient\": [{\"coeff\": 1, \"attr\": \"real\"}],  # Extract 1st FFT coefficient (frequency analysis)\n",
    "    \"fft_aggregated\": [{\"aggtype\": \"centroid\"}],  # Spectral centroid (detects abnormal frequency changes)\n",
    "\n",
    "    # Entropy Features (Detects randomness or spoofing)\n",
    "    \"approximate_entropy\": [{\"m\": 2, \"r\": 0.1}],  # Measures unpredictability (spoofed messages might increase entropy)\n",
    "    \"binned_entropy\": [{\"max_bins\": 10}],  # Useful for detecting shifts in data distribution\n",
    "\n",
    "    # Complexity & Energy-Based Features\n",
    "    \"cid_ce\": [{\"normalize\": True}],  # Measures signal complexity (can detect spoofing)\n",
    "    \"absolute_sum_of_changes\": None,  # Captures sudden payload/timing shifts (useful for injection attacks)\n",
    "    \"energy_ratio_by_chunks\": [{\"num_segments\": 5, \"segment_focus\": 2}],  # Detects anomalies in energy distribution\n",
    "\n",
    "    # Stationarity & Distribution-Based Features\n",
    "    \"augmented_dickey_fuller\": [{\"attr\": \"teststat\"}],  # Tests stationarity (useful for detecting sudden changes)\n",
    "    \"benford_correlation\": None,  # Checks if data follows natural distribution (detects synthetic spoofing)\n",
    "\n",
    "    }\n",
    "\n",
    "\"\"\"  \n",
    "    ALL Features extracted ATM, add parameters above in 'custom_fc_parameters':\n",
    "\n",
    "    'iat', 'msg_frequency', 'rolling_mean_iat', 'rolling_std_iat', \n",
    "    'value__binned_entropy__max_bins_10', 'value__skewness', \n",
    "    'value__autocorrelation__lag_1', 'value__agg_autocorrelation__f_agg_\"mean\"__maxlag_5', \n",
    "    'value__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', \n",
    "    'value__fft_coefficient__attr_\"real\"__coeff_1', \n",
    "    'value__approximate_entropy__m_2__r_0.1', 'value__binned_entropy__max_bins_10', \n",
    "    'value__benford_correlation'\n",
    "\"\"\"\n",
    "# List of selected tsfresh features\n",
    "tsfresh_features = [\n",
    "    'iat', 'msg_frequency', 'rolling_mean_iat', 'rolling_std_iat', \n",
    "    'value__binned_entropy__max_bins_10', 'value__skewness', \n",
    "    'value__autocorrelation__lag_1', 'value__agg_autocorrelation__f_agg_\"mean\"__maxlag_5', \n",
    "    'value__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', \n",
    "    'value__fft_coefficient__attr_\"real\"__coeff_1', \n",
    "    'value__approximate_entropy__m_2__r_0.1', 'value__binned_entropy__max_bins_10', \n",
    "    'value__benford_correlation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c046ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(feature_selection)\n",
    "LOAD_DATAFRAME = False\n",
    "TS_FRESH = False\n",
    "\n",
    "n_rows_train = 500    # select how many rows to load. None if whole train datasset\n",
    "n_rows_test = 500   # select how many rows to load. None if whole test datasset\n",
    "batch_size = 8\n",
    "window_size = 80    # increase window size\n",
    "stride = 5           # increase stride as a buffer\n",
    "split_ratio = 0.8     # % of training data to use for training\n",
    "window_anomaly_ratio = 0.1   # For 1 anomaly per window do: 1 / window_size\n",
    "if LOAD_DATAFRAME:\n",
    "    # Load training data\n",
    "    processeddataframe = pd.read_pickle(processeddataframe_normal_pickle_path)\n",
    "    train_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size)\n",
    "\n",
    "    # Load test data\n",
    "    processeddataframe_test = pd.read_pickle(processeddataframe_DoS_pickle_path)\n",
    "    test_dataset = convert_to_tensorflow(processeddataframe_test['features'] ,processeddataframe_test['type'], batch_size= batch_size )\n",
    "else:\n",
    "    # Preprocess and load training data\n",
    "    processeddataframe = feature_selection_preparation(preprocessed_normal_channel2_csv_path, 'training', rows=n_rows_train, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    train_dataset, val_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size, window_size = window_size, stride = stride, split_ratio= split_ratio)\n",
    "\n",
    "    processeddataframe_test = feature_selection_preparation(preprocessed_new_Spoofing_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    test_dataset, test_threshold_dataset = convert_to_tensorflow(processeddataframe_test['features'], processeddataframe_test['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)\n",
    "\n",
    "    #processeddataframe_dos_baseline = feature_selection_preparation(preprocessed_DoS_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    #dos_test_dataset, dos_threshold_dataset = convert_to_tensorflow(processeddataframe_dos_baseline['features'], processeddataframe_dos_baseline['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8eed2",
   "metadata": {},
   "source": [
    "**SAVE Dataset to npz [FIX]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/Channel2_Full_43Features_WindowFeatures_1024Batch_50Window_5Stride.npz\"\n",
    "\n",
    "# Save your datasets\n",
    "save_datasets(train_dataset, val_dataset, path)\n",
    "\n",
    "# Later, load them back\n",
    "train_dataset_loaded, val_dataset_loaded = load_datasets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check that all values normalized or there's nans in the sliding windows\n",
    "# todo: check whole processing of df\n",
    "print(processeddataframe['features'].head(4))\n",
    "print(np.size(processeddataframe['features'][0]))\n",
    "\n",
    "processeddataframe['has_nan'] = processeddataframe['features'].apply(lambda x: any(pd.isna(x)) if isinstance(x, list) else np.nan)\n",
    "valid_lists = [lst for lst in processeddataframe['features'] if isinstance(lst, list)]\n",
    "all_values = sum(valid_lists, [])\n",
    "has_nan = any(pd.isna(all_values))\n",
    "has_out_of_bounds = any(x < 0 or x > 1 for x in all_values if isinstance(x, (int, float)))\n",
    "\n",
    "print(f\"Contains NaN: {has_nan}\")\n",
    "print(f\"Contains values <0 or >1: {has_out_of_bounds}\")\n",
    "\n",
    "# Run checks for both datasets\n",
    "check_dataset(train_dataset, \"Train Dataset\")\n",
    "check_dataset(val_dataset, \"Validation Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70758afc",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6f1cb",
   "metadata": {},
   "source": [
    "**VAE SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamplesOld\")\n",
    "class VAE_multiplesamplesOld(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size, **kwargs):\n",
    "        super(VAE_multiplesamplesOld, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim)\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),  \n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(window_size * input_dim, activation='sigmoid'),  \n",
    "            layers.Reshape((window_size, input_dim))  \n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamplesOld, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"Base_VAE_multiplesamples\")\n",
    "class Base_VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size, **kwargs):\n",
    "        super(Base_VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            layers.BatchNormalization(), # Experiment: BN\n",
    "            layers.LSTM(64, activation='relu', return_sequences= False),\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size),  \n",
    "            layers.LSTM(64, activation='relu', return_sequences=True),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "\n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(Base_VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamples\")\n",
    "class VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(32, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(32, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_weak_generator\")\n",
    "class VAE_weakGenerator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_weakGenerator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(32, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(16, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_weakGenerator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x\")\n",
    "class VAE_2x(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(64, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(64, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x_weak_generator\")\n",
    "class VAE_2x_weak_generator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x_weak_generator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(64, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(32, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x_weak_generator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2d2d4",
   "metadata": {},
   "source": [
    "Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model1(vae,optimizer,discriminator_optimizer, epochs, n_samples, input_dim, latent_dim, batch_size,beta, gamma, n_critic, steps_anneal, patience, time,beta_tc = 0, validation_method = 'None', model_path = \"\"):   \n",
    "    wait = 0  \n",
    "    model_name =\"LSTM_VAE\"\n",
    "\n",
    "    print(f\"Latent Dimension = {latent_dim}, \"\n",
    "        f\"Beta = {beta}, Gamma = {gamma}, N_critic = {n_critic}, Beta_TC = {beta_tc}, Validation Method = {validation_method}, \"\n",
    "        f\"Rows in Training Data = {n_rows_train}, \"\n",
    "        f\"Batch Size = {batch_size}\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    disc_losses = []\n",
    "    beta_tc_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    real_epochs = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        real_epochs += 1\n",
    "        epoch_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        epoch_beta_tc_loss = 0\n",
    "        for step, batch in enumerate(train_dataset): # window size , features\n",
    "            global_step = epoch * len(train_dataset) + step  # Total training step count\n",
    "            # First get latent representations\n",
    "            #_, mu, logvar = vae(batch, n_samples=1, latent_only=True)\n",
    "            #z = vae.reparameterize(mu, logvar, n_samples=1)\n",
    "            #z = tf.squeeze(z, axis=0)\n",
    "            #print(batch.shape)\n",
    "\n",
    "            # Split the batch according to FactorVAE Paper\n",
    "            anneal_coeff = linear_annealing(0, 1, global_step, steps_anneal)\n",
    "\n",
    "            if gamma > 0 and n_critic > 0:\n",
    "\n",
    "                half_batch_size = tf.shape(batch)[0] // 2\n",
    "                first_half_batch = batch[:half_batch_size]\n",
    "                second_half_batch = batch[half_batch_size:]\n",
    "\n",
    "                # Compute Annealing Coefficient\n",
    "                #print(anneal_coeff)\n",
    "            else:\n",
    "                first_half_batch = batch\n",
    "\n",
    "            # 1. Train VAE\n",
    "            with tf.GradientTape() as tape:\n",
    "                reconstructed, mu, logvar = vae(first_half_batch, n_samples=n_samples, latent_only = False)  # Use multiple samples\n",
    "\n",
    "                #Compute reconstruction MSE error for each sample\n",
    "                reconstruction_errors = tf.reduce_mean(\n",
    "                    tf.square(tf.expand_dims(first_half_batch, axis=0) - reconstructed), axis=-1\n",
    "                )  # Shape: (n_samples, batch_size, window_size)\n",
    "\n",
    "                # Aggregate errors (mean over samples and window_size)\n",
    "                mean_reconstruction_error = tf.reduce_mean(reconstruction_errors, axis=(2, 0))\n",
    "\n",
    "                # Compute VAE loss\n",
    "                reconstruction_loss = tf.reduce_mean(mean_reconstruction_error)\n",
    "\n",
    "                #print(f\"Final loss {reconstruction_loss}\")\n",
    "                kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "                                \n",
    "                if gamma > 0:\n",
    "                    # Get latent samples for TC loss\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, axis=0)  # Remove sample dimension for discriminator\n",
    "                    \n",
    "                    # Total Correlation loss\n",
    "                    tc = vae.tc_loss(z)\n",
    "                    loss = reconstruction_loss + beta * kl_divergence + gamma*tc*anneal_coeff\n",
    "                elif beta_tc > 0:\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, 0) # aggregate over latent samples\n",
    "                    #print(\"z shape directly after reparam: \", z.shape)\n",
    "                    tc_beta_loss = vae.b_tcvae_total_correlation_loss(z, mu, logvar)\n",
    "                    b_tcvae_loss = (1 - beta_tc) * tc_beta_loss\n",
    "                    #print(\"tc beta\" ,tc_beta_loss.numpy())\n",
    "                    #print(\"tc beta * b_tc\", b_tcvae_loss.numpy())\n",
    "                    #print(\"anneal\" , anneal_coeff.numpy())\n",
    "                    #print(\" tc beta * anneal\", (tc_beta_loss * anneal_coeff).numpy())\n",
    "\n",
    "                    #print(\"recon\", reconstruction_loss.numpy() )\n",
    "                    #print(\"kl\", kl_divergence.numpy())\n",
    "                    loss = reconstruction_loss + kl_divergence + b_tcvae_loss\n",
    "                else:\n",
    "                    #print(reconstruction_loss)\n",
    "                    #print(kl_divergence)\n",
    "                    loss = reconstruction_loss + beta * kl_divergence\n",
    "\n",
    "\n",
    "\n",
    "            gradients = tape.gradient(loss, vae.encoder.trainable_variables + vae.decoder.trainable_variables)\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "            optimizer.apply_gradients(zip(gradients, vae.encoder.trainable_variables + vae.decoder.trainable_variables))\n",
    "\n",
    "            epoch_loss += loss.numpy()\n",
    "            #epoch_beta_tc_loss += b_tcvae_loss.numpy()\n",
    "            # 2. Train discriminator\n",
    "            disc_loss = 0\n",
    "            if gamma > 0 and n_critic >0:\n",
    "                for _ in range(n_critic):\n",
    "                    # Discriminator update\n",
    "                    with tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        # First get latent representations on second half batch\n",
    "                        _, mu, logvar = vae(second_half_batch, n_samples=n_samples, latent_only=True)\n",
    "                        z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                        z = tf.reduce_mean(z, axis=0)\n",
    "                        z_perm = vae.permute_dims(z)\n",
    "                        current_disc_loss = vae.discriminator_loss(z, z_perm)\n",
    "                        disc_loss += current_disc_loss\n",
    "\n",
    "                    # Update discriminator\n",
    "                    disc_gradients = disc_tape.gradient(current_disc_loss, vae.discriminator.trainable_variables)\n",
    "                    # Clip gradients for stability\n",
    "                    disc_gradients, _ = tf.clip_by_global_norm(disc_gradients, 1.0)\n",
    "                    discriminator_optimizer.apply_gradients(zip(disc_gradients, vae.discriminator.trainable_variables))\n",
    "\n",
    "                disc_loss = disc_loss / n_critic\n",
    "                epoch_disc_loss += disc_loss\n",
    "        \n",
    "        # VALIDATION\n",
    "        epoch_val_loss = 0\n",
    "        total_disc_acc = 0\n",
    "        show_val = validation_method in [\"B_VAE\",\"B_TCVAE\", \"TC\"]\n",
    "        latent_only = validation_method == \"TC\"\n",
    "\n",
    "        if validation_method == \"None\" or validation_method == \"PLOT\":\n",
    "            val_loss = 0\n",
    "            avg_disc_acc = 0\n",
    "            early_stop = False\n",
    "        else:\n",
    "            early_stop = True\n",
    "            for batch in val_dataset:\n",
    "                reconstructed, mu, logvar = vae(batch, n_samples=n_samples, latent_only=latent_only)\n",
    "\n",
    "                if validation_method == \"B_VAE\":\n",
    "                    reconstruction_errors = tf.reduce_mean(\n",
    "                        tf.square(tf.expand_dims(batch, axis=0) - reconstructed), axis=-1\n",
    "                    )\n",
    "                    mean_reconstruction_error = tf.reduce_mean(reconstruction_errors, axis=(2, 0))\n",
    "                    reconstruction_loss = tf.reduce_mean(mean_reconstruction_error)\n",
    "                    kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "\n",
    "                    val_loss = reconstruction_loss + beta * kl_divergence\n",
    "                    epoch_val_loss += val_loss.numpy()\n",
    "                elif validation_method == \"B_TCVAE\":\n",
    "                    reconstruction_errors = tf.reduce_mean(\n",
    "                        tf.square(tf.expand_dims(batch, axis=0) - reconstructed), axis=-1\n",
    "                    )\n",
    "                    mean_reconstruction_error = tf.reduce_mean(reconstruction_errors, axis=(2, 0))\n",
    "                    reconstruction_loss = tf.reduce_mean(mean_reconstruction_error)\n",
    "                    kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, axis= 0)\n",
    "                    tc_beta_loss = vae.b_tcvae_total_correlation_loss(z, mu, logvar)\n",
    "                    b_tcvae_loss = (1 - beta_tc) * tc_beta_loss\n",
    "                    \n",
    "                    val_loss = reconstruction_loss + kl_divergence + b_tcvae_loss\n",
    "\n",
    "                    epoch_val_loss += val_loss.numpy()\n",
    "                elif validation_method == \"TC\":\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, axis= 0)\n",
    "                    tc = vae.tc_loss(z)\n",
    "                    \n",
    "                    disc_acc = vae.discriminator_acc(z)\n",
    "                    #disc_acc = 0  # Assuming this is optional or calculated elsewhere\n",
    "                    total_disc_acc += disc_acc\n",
    "                    val_loss = gamma * tc\n",
    "                    epoch_val_loss += val_loss.numpy()\n",
    "\n",
    "        if validation_method == \"PLOT\" and epoch % 50 == 0 and epoch > 0:\n",
    "            print(\"PLOT AT EPOCH: \", {epoch + 1})\n",
    "            get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = epoch, name = model_name,type = 'TSNE', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "\n",
    "        # Store the loss for this epoch\n",
    "        train_loss = epoch_loss / len(train_dataset)\n",
    "        val_loss = epoch_val_loss / len(val_dataset) \n",
    "        disc_loss = epoch_disc_loss / len(train_dataset)\n",
    "        avg_disc_acc = total_disc_acc / len(val_dataset)\n",
    "        beta_tc_loss = epoch_beta_tc_loss / len(train_dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        disc_losses.append(disc_loss)\n",
    "        beta_tc_losses.append(beta_tc_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.6f}, \"\n",
    "            f\"Discriminator Loss: {disc_loss:.6f} Disc Acc: {avg_disc_acc:.4f}, Beta TC Loss: {beta_tc_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if early_stop:\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                wait = 0\n",
    "                # Save best model\n",
    "                best_model_path = model_path\n",
    "                vae.compile(optimizer = optimizer)\n",
    "                vae.save(best_model_path)\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    epochs = epoch + 1\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        # Save the trained model\n",
    "        #vae.compile(optimizer = optimizer)\n",
    "        #vae.save(model_path)\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            save_trained_model(vae, optimizer, model_path, model_name, latent_dim,beta,n_rows_train,time, AWS = AWS, s3 = s3 , BUCKET = BUCKET)\n",
    "            print( f'💽Saved Model at Epoch {epoch+1}💽')\n",
    "\n",
    "    #Save the trained model\n",
    "    #vae.save(model_path)\n",
    "\n",
    "    print(\"💽Loss Plot Saved.💽\")\n",
    "    print(\"💽VAE model saved.💽\")\n",
    "    #vae.summary()\n",
    "    return train_losses, val_losses, real_epochs, time, show_val, model_path, vae\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863771e",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99324f19",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9a54",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 2\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-03\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"None\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 5  \n",
    "beta = 1 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"LSTM_VAE\"\n",
    "model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs, name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = True\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path,reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e510e",
   "metadata": {},
   "source": [
    "**Test Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_path = \"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_LSTM_VAE_LD30_Beta25_NT800000_21-37.keras\"\n",
    "#loaded_vae = keras.models.load_model(\"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_BEST_VAE_11-15.keras\")\n",
    "loaded_vae = keras.models.load_model(model_path)\n",
    "loaded_vae.trainable = False  # Freeze model weights\n",
    "\n",
    "get_latent_representations_label(loaded_vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010100",
   "metadata": {},
   "source": [
    "**HyperParamter Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 100\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 2  \n",
    "beta = 1 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "latent_dims = [6,9,25,14]  # Example values for latent dimension\n",
    "beta_values = [3,15,7]\n",
    "\n",
    "\n",
    "# Iterate over all combinations\n",
    "for latent_dim, beta in itertools.product(latent_dims, beta_values):\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"BASE_LSTM_VAE\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "    print(f\"Training with: latent_dim={latent_dim}, beta={beta} validation_method={validation_method}\")\n",
    "\n",
    "    vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_weakGen = VAE_weakGenerator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_2x = VAE_2x(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    VAE_2x_weakGen = VAE_2x_weak_generator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_weakGen = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_2x = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_2x_weakGen = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "    print(\"VAE--------------\")\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae, optimizer\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE WEAK GENERATOR-------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_weakGen = train_model(vae_weakGen,optimizer_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    del vae_weakGen, optimizer_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X----------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_2x = train_model(vae_2x,optimizer_2x,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_2x, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae_2x, optimizer_2x\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X WEAK GENERATOR------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, VAE_2x_weakGen = train_model(VAE_2x_weakGen,optimizer_2x_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "   \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(VAE_2x_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type='TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del VAE_2x_weakGen, optimizer_2x_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dea3cc",
   "metadata": {},
   "source": [
    "**Iterative Training on Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular HYPERPARAMETERS \n",
    "#input_dim = np.size(processeddataframe['features'][0])\n",
    "input_dim = 42\n",
    "epochs = 50\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 30  \n",
    "beta = 0 # 20\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"BEST_VAE\"\n",
    "new_model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_{model_name}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = keras.models.load_model(best_model_path)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=weight_decay)\n",
    "optimizer = vae.optimizer\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path = new_model_path)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time, show_val= show_val)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = True)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = False\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path, reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

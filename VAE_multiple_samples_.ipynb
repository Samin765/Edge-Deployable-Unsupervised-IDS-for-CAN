{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a57642",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545506f7",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861000fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Changes AWS to True if on SageMaker Instance and set S3 BUCKET and Key accordingly\n",
    "AWS = False\n",
    "REGION = 'eu-west-1'\n",
    "BUCKET = 'ml-can-ids-logs'\n",
    "s3 = None\n",
    "\n",
    "# Import Functions\n",
    "import setuptools.dist\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "import anomaly_detection_functions\n",
    "import clustering\n",
    "import feature_selection\n",
    "import train\n",
    "import id_embedding\n",
    "\n",
    "reload(utils)\n",
    "reload(train)\n",
    "reload(anomaly_detection_functions)\n",
    "reload(clustering)\n",
    "reload(feature_selection)\n",
    "reload(id_embedding)\n",
    "\n",
    "from utils import plot_loss_curve, plot_pca, plot_tsne, get_confusion_matrix, get_latent_representations_label, analyze_latent_variance, analyze_kl_divergence, linear_annealing, save_results_to_excel, save_trained_model, get_s3_client, check_dataset\n",
    "from anomaly_detection_functions import get_threshold_from_train, get_threshold_from_test, anomaly_detection, get_anomaly_detection_accuracy, get_mean_variances\n",
    "from clustering import visualize_anomalies, evaluate_anomaly_detector, detect_anomalies_one_class_svm_with_threshold, train_hdbscan_detector, evaluate_hdbscan_detector, detect_anomalies_hdbscan, hdbscan_set_threshold, visualize_results\n",
    "from clustering import prepare_features, train_isolation_forest, train_one_class_svm, detect_anomalies_isolation_forest, detect_anomalies_one_class_svm, evaluate_anomaly_detector_verbose\n",
    "from feature_selection import feature_selection_preparation, convert_to_tensorflow, feature_selection_preparation_new\n",
    "from train import train_model, train_model_factor , train_model_btc, train_model_bernoulli, train_model_semi\n",
    "from id_embedding import train_embedding\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers, Model\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import keras\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from importlib import reload\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "if AWS:\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # No wrapping, long rows won't be cut off\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content (especially useful for long strings)\n",
    "\n",
    "# Remove this after testing/debugging\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  \n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"✅ Using GPU\")\n",
    "    device = \"/GPU:0\"\n",
    "else:\n",
    "    print(\"❌ Using CPU\")\n",
    "    device = \"/CPU:0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715700",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2753d",
   "metadata": {},
   "source": [
    "**PATH FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS:\n",
    "    s3 = get_s3_client(REGION, BUCKET, True)\n",
    "    \n",
    "    # Get S3 Object \n",
    "    channel2logs = s3.get_object(Bucket = BUCKET, Key= 'channel2Logs.csv')\n",
    "    dos_attack_channel2 = s3.get_object(Bucket = BUCKET, Key= 'dos_attack_channel2.csv')\n",
    "    replay_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'replay_attack_channel2.csv') \n",
    "    spoofing_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'new_spoofing_attack_channel2.csv') \n",
    "\n",
    "    channel2logs = channel2logs['Body'].read().decode('utf-8')\n",
    "    dos_attack_channel2 = dos_attack_channel2['Body'].read().decode('utf-8')\n",
    "    replay_attack_channel2 = replay_attack_channel2['Body'].read().decode('utf-8')\n",
    "    spoofing_attack_channel2 = spoofing_attack_channel2['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Get Content\n",
    "    channel2logs = StringIO(channel2logs)\n",
    "    dos_attack_channel2 = StringIO(dos_attack_channel2)\n",
    "    replay_attack_channel2 = StringIO(replay_attack_channel2)\n",
    "    spoofing_attack_channel2 = StringIO(spoofing_attack_channel2)\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = dos_attack_channel2 # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = replay_attack_channel2 # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = spoofing_attack_channel2 # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel2_csv_path = channel2logs # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = \"\" # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = \"\" # Green Channel\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"\"\n",
    "else:\n",
    "    # Unprocessed Normal and Attack Data\n",
    "    preprocessed_normal_csv_path = './Dataset/Tw22206_L003_with_ecu_channel.csv'  # Normal Unprocessed\n",
    "    preprocessed_DoS_csv_path = './Dataset/Attack_Logs/dos_attack.csv'  # Dos Unprocessed\n",
    "    preprocessed_Fuzzy_csv_path = './Dataset/Attack_Logs/fuzzy_attack.csv'  # Fuzzy Unprocessed\n",
    "    preprocessed_Replay_csv_path = './Dataset/Attack_Logs/replay_attack.csv'  # Replay Unprocessed - Test\n",
    "    preprocessed_Spoofing_csv_path = './Dataset/Attack_Logs/spoofing_attack.csv'  # Spoofing Unprocessed\n",
    "    preprocessed_Suspension_csv_path = './Dataset/Attack_Logs/suspension_attack.csv'  # Suspension Unprocessed - Hardest Attack Type\n",
    "\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = './Dataset/Attack_Logs/dos_attack_channel2.csv'  # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = './Dataset/Attack_Logs/replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_new_Replay_channel2_csv_path = './Dataset/Attack_Logs/new_replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Suspension_channel2_csv_path = './Dataset/Attack_Logs/suspension_attack_channel2.csv'  # Suspension on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_new_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/new_spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Replay_channel2_csv_path = './Dataset/Attack_Logs/Replay_Attack_Channel2_50K_33s_56s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Replay_channel2_csv_path = './Dataset/Attack_Logs/Replay_Attack_Channel2_200K_33s_169s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_1000k_Replay_channel2_csv_path = './Dataset/Attack_Logs/Replay_Attack_Channel2_1000K_33s_776s.csv'  # Replay on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Replay_channel2_part1_csv_path = './Dataset/Attack_Logs/Train_Replay_Attack_Channel2_50K_33s_56s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Replay_channel2_part1_csv_path = './Dataset/Attack_Logs/Train_Replay_Attack_Channel2_200K_33s_169s.csv'  # Replay on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Replay_channel2_part2_csv_path = './Dataset/Attack_Logs/Test_Replay_Attack_Channel2_50K_33s_56s.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Replay_channel2_part2_csv_path = './Dataset/Attack_Logs/Test_Replay_Attack_Channel2_200K_33s_169s.csv'  # Replay on channel 2 (Red Channel)\n",
    "\n",
    "\n",
    "    preprocessed_50K_ParkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB_Spoofing_Attack_Channel2_50K_33s_56s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_200K_ParkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB_Spoofing_Attack_Channel2_200K_33s_169s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_1000k_ParkingBrake_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/ParkingBrakeController_EPB_Spoofing_Attack_Channel2_1000K_33s_775s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    preprocessed_50K_Coordinator_K_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/Coordinator_K__Spoofing_Attack_Channel2_50K_33s_56s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_200K_Coordinator_K_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/Coordinator_K_Spoofing_Attack_Channel2_200K_33s_169s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_1000K_Coordinator_K_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/Coordinator_K_Spoofing_Attack_Channel2_1000K_33s_776s.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "\n",
    "    # Open Source Datasets:\n",
    "     \n",
    "    # Car Hacking\n",
    "    HCRL_CarHacking_Normal_csv_path = './Dataset/HCRL_CarHacking/Normal_dataset.csv' \n",
    "    HCRL_CarHacking_DoS_csv_path = './Dataset/HCRL_CarHacking/DoS_fixed_dataset.csv' \n",
    "    HCRL_CarHacking_Fuzzy_csv_path = './Dataset/HCRL_CarHacking/Fuzzy_fixed_dataset.csv' \n",
    "    HCRL_CarHacking_Spoofing_Gear_csv_path = './Dataset/HCRL_CarHacking/Gear_fixed_dataset.csv' \n",
    "    HCRL_CarHacking_Spoofing_RPM_csv_path = './Dataset/HCRL_CarHacking/RPM_fixed_dataset.csv' \n",
    "\n",
    "    # OTIDS\n",
    "    HCRL_OTIDS_Normal_csv_path = './Dataset/HCRL_OTIDS/Normal_dataset.csv' \n",
    "    HCRL_OTIDS_DoS_csv_path = './Dataset/HCRL_OTIDS/DoS_dataset.csv' \n",
    "    HCRL_OTIDS_Fuzzy_channel2_csv_path = './Dataset/HCRL_OTIDS/Fuzzy_dataset.csv' \n",
    "    HCRL_OTIDS_Impersonation_channel2_csv_path = './Dataset/HCRL_OTIDS/Impersonation_dataset.csv' \n",
    "\n",
    "    # Competition\n",
    "    HCRL_Competition_Pre_S_Train_1_csv_path = './Dataset/HCRL_Competition/0_Preliminary/0_Training/Pre_train_S_0' \n",
    "    HCRL_Competition_Pre_S_Train_2_csv_path = './Dataset/HCRL_Competition/0_Preliminary/0_Training/Pre_train_S_1' \n",
    "    HCRL_Competition_Pre_S_Train_3_csv_path = './Dataset/HCRL_Competition/0_Preliminary/0_Training/Pre_train_S_2' \n",
    "\n",
    "    HCRL_Competition_Pre_D_Train_1_csv_path = './Dataset/HCRL_Competition/0_Preliminary/0_Training/Pre_train_D_0'\n",
    "    HCRL_Competition_Pre_D_Train_2_csv_path = './Dataset/HCRL_Competition/0_Preliminary/0_Training/Pre_train_D_1'\n",
    "    HCRL_Competition_Pre_D_Train_3_csv_path = './Dataset/HCRL_Competition/0_Preliminary/0_Training/Pre_train_D_2'\n",
    "\n",
    "    HCRL_Competition_Pre_S_Submission_3_csv_path = './Dataset/HCRL_Competition/0_Preliminary/1_Submission/Pre_submit_S' \n",
    "    HCRL_Competition_Pre_D_Submission_1_csv_path = './Dataset/HCRL_Competition/0_Preliminary/1_Submission/Pre_submit_D'\n",
    "\n",
    "    HCRL_Competition_Final_S_Submission_1_csv_path = './Dataset/HCRL_Competition/0_Preliminary/1_Final/Fin_host_session_submit_S'\n",
    "\n",
    "\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel0_csv_path = './Dataset/Channel_Logs/channel0Logs.csv'  \n",
    "    preprocessed_normal_channel2_csv_path = './Dataset/Channel_Logs/channel2Logs.csv'  # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = './Dataset/Channel_Logs/channel4Logs.csv'  # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = './Dataset/Channel_Logs/channel5Logs.csv'  # Green Channel\n",
    "\n",
    "\n",
    "    # Preprocessed Dataframe Data\n",
    "    processeddataframe_normal_csv_path = './Dataset/Processed_Dataframes/train_dataframe.csv'  # Normal CSV Dataframe (Turns Lists into Strings)\n",
    "    processeddataframe_DoS_csv_path = './Dataset/Processed_Dataframes/test_DoS_dataframe.csv'  # DoS CSV Dataframe (Turns Lists into Strings)\n",
    "\n",
    "    # Preprocessed Pickle Data\n",
    "    processeddataframe_normal_pickle_path = './Dataset/Processed_Dataframes/train_Normal_dataframePickle.pkl'  # Normal Pickle Dataframe\n",
    "    processeddataframe_DoS_pickle_path = './Dataset/Processed_Dataframes/test_DoS_dataframePickle.pkl'  # DoS Pickle Dataframe\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"./Resources/Models/SOA_VAE_E6_LD38_EP30_NT100000_B1024_I42.keras\"\n",
    "\n",
    "\n",
    "\n",
    "    # PRELOAD Dataframe for Debug\n",
    "    DEBUG = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff978e0",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c046ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(feature_selection)\n",
    "LOAD_DATAFRAME = False\n",
    "BINARY = False\n",
    "BINARY_ID = False\n",
    "\n",
    "n_rows_train = 50000  # select how many rows to load. None if whole train datasset\n",
    "n_rows_test = 50000   # select how many rows to load. None if whole test datasset\n",
    "batch_size = 1024 \n",
    "window_size = 50    # increase window size\n",
    "stride = 25     # increase stride as a buffer\n",
    "split_ratio = 0.8     # % of training data to use for training\n",
    "window_anomaly = 5  # For 1 anomaly per window do: 1 / window_size\n",
    "\n",
    "if LOAD_DATAFRAME:\n",
    "    # Load training data\n",
    "    processeddataframe = pd.read_pickle(processeddataframe_normal_pickle_path)\n",
    "    train_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size)\n",
    "\n",
    "    # Load test data\n",
    "    processeddataframe_test = pd.read_pickle(processeddataframe_DoS_pickle_path)\n",
    "    test_dataset = convert_to_tensorflow(processeddataframe_test['features'] ,processeddataframe_test['type'], batch_size= batch_size )\n",
    "else:\n",
    "    \n",
    "    # Preprocess and load training data\n",
    "    processeddataframe , embedding_model, id_to_embedding, scalers = feature_selection_preparation(HCRL_CarHacking_Normal_csv_path, phase = 'training', rows=n_rows_train, binary = BINARY, binary_id= BINARY_ID)\n",
    "    train_dataset, val_dataset, val_dataset2 = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size, window_size = window_size, stride = stride, split_ratio= split_ratio)\n",
    "\n",
    "    processeddataframe_test = feature_selection_preparation(HCRL_CarHacking_Fuzzy_csv_path, phase = 'test', rows=n_rows_test, binary = BINARY, binary_id = BINARY_ID, embedding_model = embedding_model, id_to_embedding = id_to_embedding, scalers = scalers)\n",
    "    test_dataset, test_threshold_dataset = convert_to_tensorflow(processeddataframe_test['features'], processeddataframe_test['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly = window_anomaly)\n",
    "    \"\"\"\n",
    "\n",
    "    processeddataframe , embedding_model, id_to_embedding, scalers = feature_selection_preparation_new(preprocessed_50K_Replay_channel2_part1_csv_path, phase = 'test', rows=n_rows_train, binary = BINARY, binary_id= BINARY_ID, train_embedding_scaler = True)\n",
    "    train_dataset, val_dataset = convert_to_tensorflow(processeddataframe['features'], processeddataframe['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly = window_anomaly)\n",
    "\n",
    "    processeddataframe_test = feature_selection_preparation_new(preprocessed_50K_Replay_channel2_part2_csv_path, phase = 'test', rows=n_rows_test, binary = BINARY, binary_id = BINARY_ID, embedding_model = embedding_model, id_to_embedding = id_to_embedding, scalers = scalers, train_embedding_scaler = False)\n",
    "    test_dataset, test_threshold_dataset = convert_to_tensorflow(processeddataframe_test['features'], processeddataframe_test['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly = window_anomaly)\n",
    "\n",
    "    processeddataframe_normal = feature_selection_preparation_new(preprocessed_normal_channel2_csv_path, phase = 'training', rows=n_rows_test, binary = BINARY, binary_id = BINARY_ID, embedding_model = embedding_model, id_to_embedding = id_to_embedding, scalers = scalers, train_embedding_scaler = False)\n",
    "    train_dataset_normal, val_dataset_normal, val_dataset2_normal = convert_to_tensorflow(processeddataframe_normal['features'], batch_size= batch_size, window_size = window_size, stride = stride, split_ratio= split_ratio)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check that all values normalized or there's nans in the sliding windows\n",
    "# todo: check whole processing of df\n",
    "print(processeddataframe['data'].head(1))\n",
    "print(processeddataframe['features'].head(4))\n",
    "print(processeddataframe_test['features'].head(4))\n",
    "print(np.size(processeddataframe['features'][0]))\n",
    "\n",
    "processeddataframe['has_nan'] = processeddataframe['features'].apply(lambda x: any(pd.isna(x)) if isinstance(x, list) else np.nan)\n",
    "valid_lists = [lst for lst in processeddataframe['features'] if isinstance(lst, list)]\n",
    "all_values = sum(valid_lists, [])\n",
    "has_nan = any(pd.isna(all_values))\n",
    "has_out_of_bounds = any(x < 0 or x > 1 for x in all_values if isinstance(x, (int, float)))\n",
    "\n",
    "print(f\"Contains NaN: {has_nan}\")\n",
    "print(f\"Contains values <0 or >1: {has_out_of_bounds}\")\n",
    "\n",
    "# Run checks for both datasets\n",
    "check_dataset(train_dataset, \"Train Dataset\")\n",
    "check_dataset(val_dataset, \"Validation Dataset\")\n",
    "print(processeddataframe_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70758afc",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6f1cb",
   "metadata": {},
   "source": [
    "**VAE SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_Mean_Variance_Decoder\")\n",
    "class VAE_Mean_Variance_Decoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_Mean_Variance_Decoder, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(128, activation = 'relu', return_sequences = True), \n",
    "            #layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "            #layers.TimeDistributed(layers.Dense(input_dim))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        # Split into mean and logvar outputs\n",
    "        self.decoder_mean = layers.TimeDistributed(layers.Dense(input_dim))\n",
    "        self.decoder_logvar = layers.TimeDistributed(layers.Dense(input_dim))\n",
    "\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        #print(\"Z Shape at Decode: \" , z.shape)\n",
    "\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        #print(\"Z_FLAT Shape at Decode: \" , z.shape)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Get mean and logvar\n",
    "        mean_flat = self.decoder_mean(reconstructed_flat)\n",
    "        logvar_flat = self.decoder_logvar(reconstructed_flat)\n",
    "    \n",
    "        # Reshape back to include samples dimension\n",
    "        mean = tf.reshape(mean_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "        logvar = tf.reshape(logvar_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "        \n",
    "        return mean, logvar\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed_mean, reconstructed_logvar = self.decode(z)\n",
    "\n",
    "        #print(\"Reconstructed Mean: \" , reconstructed_mean.shape)\n",
    "        #print(\"Reconstructed Logvar: \" , reconstructed_logvar.shape)\n",
    "\n",
    "\n",
    "        return reconstructed_mean, reconstructed_logvar, mu, logvar\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_Mean_Variance_Decoder, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamples\")\n",
    "class VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(128, activation = 'relu', return_sequences = True), \n",
    "            #layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "            layers.TimeDistributed(layers.Dense(input_dim))  # Output must match (window_size, input_dim)\n",
    "        ]) \n",
    "        \n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        #print(\"AT ENCODE x shape: \" , x.shape)\n",
    "        #print(\"AT ENCODE hidden shape: \", hidden.shape)\n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        #print(\"AT DECODE: z shape \" , z.shape )\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        #print(\"AT DECODE: z flat shape \" , z_flat.shape )\n",
    "\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        #print(\"AT DECODE: reconstructed flat shape \" , reconstructed_flat.shape )\n",
    "\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        model_outputs = {}\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "\n",
    "            model_outputs['reconstructed'] = None\n",
    "            model_outputs['mu'] = mu\n",
    "            model_outputs['logvar'] = logvar\n",
    "            \n",
    "            return model_outputs\n",
    "        mu, logvar = self.encode(x)\n",
    "        #print(\"AT CALL: mu shape \" , mu.shape )\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        #print(\"RECONSTRUCTED shape: \", reconstructed.shape)\n",
    "\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        print(\"Z shape: \", z.shape)\n",
    "        print(\"z_mean shape: \", z_mean.shape)\n",
    "        print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_weak_generator\")\n",
    "class VAE_weakGenerator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_weakGenerator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences= True),\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(50, activation='relu', return_sequences= True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        model_outputs = {}\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_weakGenerator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x\")\n",
    "class VAE_2x(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(50, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(50, activation = 'relu', return_sequences = True), \n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        model_outputs = {}\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x_weak_generator\")\n",
    "class VAE_2x_weak_generator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x_weak_generator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(50, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            #layers.BatchNormalization(),\n",
    "            layers.LSTM(5, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        \"\"\"\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        \"\"\"\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        model_outputs = {}\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "\n",
    "        return model_outputs\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "    \"Disentangling by Factorising\"\n",
    "    (https://arxiv.org/abs/1802.05983)\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "\n",
    "        from: ..\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x_weak_generator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af9b3e",
   "metadata": {},
   "source": [
    "**Bernoulli VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"Bernoulli_VAE\")\n",
    "class Bernoulli_VAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size,  bernoulli_prior_p=0.5 ,**kwargs):\n",
    "        super(Bernoulli_VAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "        self.bernoulli_prior_p = bernoulli_prior_p\n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = tf.keras.Sequential([\n",
    "            layers.Dense(latent_dim),\n",
    "            layers.Activation('softplus')\n",
    "        ])\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(128, activation = 'relu', return_sequences = True), \n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "       \n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        eps = tf.random.normal(shape=(tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.sqrt(logvar)  # More stable alternative to exp UNSTABLE FIX\n",
    "        #print(std)\n",
    "        #print(\"mu shape at reparm: \", mu.shape)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z shape at deocde \", z.shape)\n",
    "        reconstructed = self.decoder(z)\n",
    "        #print(\"Reconstructed X at deocde \", reconstructed.shape)\n",
    "        return reconstructed\n",
    "\n",
    "    def call(self, x, n_samples = 1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        #print(\"Reconstructed X after decode and reshape \", reconstructed.shape)\n",
    "\n",
    "        return reconstructed, mu, logvar, z\n",
    "    \n",
    "    def bernoulli_prior_logpdf(self, z):\n",
    "        \"\"\"Calculate log probability of z under a Bernoulli prior\n",
    "        \n",
    "        For continuous latent variables, we use a \"relaxed\" Bernoulli prior:\n",
    "        p(z) ~ Bernoulli(p) where z is \"pushed\" toward 0 or 1\n",
    "        \"\"\"\n",
    "        # Compute the log probability of z under a Bernoulli-like prior\n",
    "        # p(z) ~ Bernoulli(p) where p is self.bernoulli_prior_p\n",
    "        # For each dimension, we compute the probability based on how close z is to 0 or 1\n",
    "        \n",
    "        # Component that favors values close to 0\n",
    "        log_prob_0 = tf.math.log(1 - self.bernoulli_prior_p + 1e-10) - tf.square(z) / 0.1\n",
    "        \n",
    "        # Component that favors values close to 1\n",
    "        log_prob_1 = tf.math.log(self.bernoulli_prior_p + 1e-10) - tf.square(z - 1) / 0.1\n",
    "        \n",
    "        # Combine (use log-sum-exp trick for numerical stability)\n",
    "        max_val = tf.maximum(log_prob_0, log_prob_1)\n",
    "        log_prob = max_val + tf.math.log(\n",
    "            tf.exp(log_prob_0 - max_val) + tf.exp(log_prob_1 - max_val)\n",
    "        )\n",
    "        \n",
    "        return log_prob\n",
    "    #####################################################################################\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(Bernoulli_VAE, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size,  # Window size\n",
    "            \"bernoulli_prior_p\" : self.bernoulli_prior_p\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'], bernoulli_prior_p = config['bernoulli_prior_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb85df8",
   "metadata": {},
   "source": [
    "**VQ-VAE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88900688",
   "metadata": {},
   "source": [
    "**SEMI SUPERVISED: LR-SEMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\"\"\"\n",
    "LR-SEMI is based on the paper:\n",
    "\n",
    "\"Semisupervised anomaly detection of multivariate time series based on a variational autoencoder\"\n",
    "(https://doi.org/10.1007/s10489-022-03829-1)\n",
    "\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package=\"LR_SEMIVAE\")\n",
    "class LR_SEMIVAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size,num_classes, lambda_cls, **kwargs):\n",
    "        super(LR_SEMIVAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_cls = lambda_cls \n",
    "\n",
    "        # FLOPs: \n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim + 1,)), # + 1 for the z + label concat\n",
    "            layers.RepeatVector(window_size),  \n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "\n",
    "        self.LR_classifier = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(128,)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.Dense(128, activation='tanh'),  # Linear → Tanh\n",
    "            layers.Dense(128, activation='tanh'),  # Final softmax layer? wrong\n",
    "            layers.Dense(128),  \n",
    "            layers.Dense(1, activation = 'sigmoid'),  # Final softmax layer\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        return mu, logvar, hidden\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0) \n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "\n",
    "    def decode(self, z, y = None, hidden = None):\n",
    "        #print(\"AT DECODE: z shape \" , z.shape )\n",
    "        num_samples = tf.shape(z)[0]\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        #print(\"AT DECODE: z flat shape \" , z_flat.shape )\n",
    "\n",
    "        y_pred = None\n",
    "\n",
    "        if y == None:\n",
    "            #print(\"y is none\")\n",
    "            #print(\"AT DECODE: hidden shape \" , hidden.shape)\n",
    "            y_pred = self.LR_classifier(hidden) \n",
    "            #print(\"AT DECODE: y_pred shape: \", y_pred.shape)\n",
    "            y_pred = tf.cast(y_pred, tf.float32)          # ensure dtype is compatible\n",
    "            #y_pred = tf.expand_dims(y_pred, axis=-1)      # shape: (batch_size, 1)\n",
    "            y_repeated = tf.repeat(y_pred, repeats=num_samples, axis=0)  # shape: (n_samples * batch_size, num_classes)  \n",
    "            #print(\"AT DECODE: y_pred_repeated shape: \", y_repeated.shape)\n",
    "            ## LEFT OFF HERE: How should labels be generated if not provided, check the shapes should probably be: y_pred = [n_samples, batch_size, ] or \n",
    "\n",
    "        else:\n",
    "            #print(\"AT DECODE: y shape \" , y.shape )\n",
    "            y = tf.cast(y, tf.float32)          # ensure dtype is compatible\n",
    "            y = tf.expand_dims(y, axis=-1)      # shape: (batch_size, 1)\n",
    "            y_repeated = tf.repeat(y, repeats=num_samples, axis=0)  # shape: (n_samples * batch_size, num_classes)  \n",
    "\n",
    "\n",
    "     \n",
    "        #print(\"AT DECODE: y repeated shape \" , y_repeated.shape )\n",
    "        z_y_flat = tf.concat([z_flat, y_repeated], axis=-1)  # shape: (n_samples * batch_size, latent_dim + num_classes)        \n",
    "\n",
    "        #print(\"AT DECODE: z_y flat shape \" , z_y_flat.shape )\n",
    "\n",
    "        reconstructed_flat = self.decoder(z_y_flat)\n",
    "        #print(\"AT DECODE: reconstructed_flat shape \" , reconstructed_flat.shape )\n",
    "\n",
    "        z_reshaped = tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "        return z_reshaped, y_pred\n",
    "\n",
    "    def call(self, batch_x, batch_y = None, n_samples=1, latent_only = False):\n",
    "        #print(\"AT CALL batch_X shape: \", batch_x.shape)\n",
    "        model_outputs = {}\n",
    "\n",
    "        if batch_y == None:\n",
    "            y_hat = None\n",
    "            #print(\"AT CALL batch_y = None\")\n",
    "        else:\n",
    "            y_hat = batch_y\n",
    "            #print(\"AT CALL batch_y shape: \", batch_y.shape)\n",
    "\n",
    "\n",
    "        mu , logvar, hidden = self.encode(batch_x)\n",
    "        #print(\"AT CALL: mu shape \" , mu.shape )\n",
    "        #print(\"AT CALL: logvar shape \" , logvar.shape )\n",
    "\n",
    "        z = self.reparameterize(mu, logvar, n_samples)\n",
    "        #print(\"AT REPARAM: z shape \" , z.shape )\n",
    "\n",
    "        reconstructed, y_pred = self.decode(z, y_hat, hidden)\n",
    "        #print(\"Reconstruction Shape : \" , reconstructed.shape)\n",
    "\n",
    "        model_outputs['reconstructed'] = reconstructed\n",
    "        model_outputs['mu'] = mu\n",
    "        model_outputs['logvar'] = logvar\n",
    "        model_outputs['hidden'] = hidden\n",
    "        model_outputs['y_pred'] = y_pred\n",
    "\n",
    "        return model_outputs\n",
    "    \n",
    "    def compute_loss(self , y, reconstruction_loss, hidden = None, y_pred = None, AD = True):\n",
    "        # Determine if data is labeled\n",
    "        losses = {}\n",
    "        is_labeled = y is not None\n",
    "        #print(y_pred)\n",
    "        # Mask for Normal Data\n",
    "        if is_labeled:\n",
    "            a_t = 1.0 - tf.cast(y, tf.float32)  # 1 for normal, 0 for abnormal [batch_size, 1]\n",
    "            #print(\"AT COMPUTE Loss: a_t shape BEFORE squeezed \" , a_t.shape )\n",
    "\n",
    "            # Count the number of zeros and ones in y and a_t\n",
    "            \"\"\"\n",
    "            num_zeros_y = tf.reduce_sum(tf.cast(y == 0, tf.float32))\n",
    "            num_ones_y = tf.reduce_sum(tf.cast(y == 1, tf.float32))\n",
    "            num_zeros_a_t = tf.reduce_sum(tf.cast(a_t == 0, tf.float32))\n",
    "            num_ones_a_t = tf.reduce_sum(tf.cast(a_t == 1, tf.float32))\n",
    "\n",
    "\n",
    "            print(f\"Number of zeros in y: {num_zeros_y.numpy()}\")\n",
    "            print(f\"Number of ones in y: {num_ones_y.numpy()}\")\n",
    "            print(f\"Number of zeros in a_t: {num_zeros_a_t.numpy()}\")\n",
    "            print(f\"Number of ones in y: {num_ones_a_t.numpy()}\")\"\n",
    "            \"\"\"\n",
    "            kappa = tf.reduce_mean(a_t, axis = -1 , keepdims = True) #[batch_size , ]\n",
    "\n",
    "        else:\n",
    "            if y_pred is not None:\n",
    "                y_pred = tf.cast(y_pred > 0.5, tf.float32)  # binarize probs\n",
    "\n",
    "                a_t = 1.0 - tf.cast(y_pred, tf.float32)  # 1 for normal, 0 for abnormal [batch_size, 1]\n",
    "                a_t = tf.squeeze(a_t, axis=-1)  # Now a_t shape is (1024,) [batch_size,]\n",
    "                #print(\"AT COMPUTE Loss y_pred is not none: a_t shape \" , a_t.shape )\n",
    "                \n",
    "                \"\"\"\n",
    "                num_zeros_y = tf.reduce_sum(tf.cast(y_pred == 0, tf.float32))\n",
    "                num_ones_y = tf.reduce_sum(tf.cast(y_pred == 1, tf.float32))\n",
    "                num_zeros_a_t = tf.reduce_sum(tf.cast(a_t == 0, tf.float32))\n",
    "                num_ones_a_t = tf.reduce_sum(tf.cast(a_t == 1, tf.float32))\n",
    "\n",
    "                \n",
    "                print(f\"Number of zeros in y: {num_zeros_y.numpy()}\")\n",
    "                print(f\"Number of ones in y: {num_ones_y.numpy()}\")\n",
    "                print(f\"Number of zeros in a_t: {num_zeros_a_t.numpy()}\")\n",
    "                print(f\"Number of ones in y: {num_ones_a_t.numpy()}\")\n",
    "                \"\"\"\n",
    "                kappa = tf.reduce_mean(a_t, axis = -1 , keepdims = True) #[batch_size , ]\n",
    "                #print(kappa)\n",
    "                \n",
    "        #print(\"AT COMPUTE loss: reconstruction loss before masking\" , reconstruction_loss.shape)\n",
    "        masked_recon_loss_batch = reconstruction_loss * a_t\n",
    "        #print(\"AT COMPUTE Loss: masked_recon_loss \" , masked_recon_loss.shape )\n",
    "\n",
    "        masked_recon_loss = tf.reduce_mean(masked_recon_loss_batch)\n",
    "        # 3. Classification Loss (for labeled data only)\n",
    "        classification_loss = 0.0\n",
    "        if is_labeled and self.lambda_cls > 0:\n",
    "        # Use the latent representation to predict the label\n",
    "            #z_mean = tf.reduce_mean(z, axis=0)  # Average over samples\n",
    "            y_pred = self.LR_classifier(hidden) \n",
    "            y_pred = tf.cast(y_pred, tf.float32)          # ensure dtype is compatible\n",
    "            y_logits = y_pred\n",
    "            y_logits = tf.squeeze(y_logits, axis=-1)  # Now a_t shape is (1024,) [batch_size,]\n",
    "\n",
    "            \n",
    "            #print(\"AT COMPUTE Loss: y \" , y.shape )\n",
    "            #print(\"AT COMPUTE Loss: y_logits \" , y_logits.shape )\n",
    "\n",
    "            #y_logits = self.LR_classifier(z_mean)\n",
    "            # Binary cross-entropy loss for classification\n",
    "            if AD:\n",
    "                classification_loss = tf.keras.losses.binary_crossentropy(\n",
    "                    y, \n",
    "                    y_logits,\n",
    "                    from_logits=False\n",
    "                )\n",
    "            else: \n",
    "                classification_loss = tf.reduce_mean(\n",
    "                    tf.keras.losses.binary_crossentropy(\n",
    "                        y, \n",
    "                        y_logits,\n",
    "                        from_logits=False\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if AD:\n",
    "            losses['classification_loss'] = classification_loss * self.lambda_cls\n",
    "            losses['masked_recon_loss'] = masked_recon_loss_batch\n",
    "            losses['kappa'] = kappa\n",
    "        else: \n",
    "            losses['classification_loss'] = classification_loss * self.lambda_cls\n",
    "            losses['masked_recon_loss'] = masked_recon_loss\n",
    "            losses['kappa'] = kappa\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LR_SEMIVAE, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'window_size': self.window_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'lambda_cls': self.lambda_cls\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863771e",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9a54",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(train)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "epochs = 100\n",
    "n_samples = 11\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "learning_rate_disc = 5e-5 # FactorVAE\n",
    "\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "print(\"steps: \", steps_anneal) \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"B_TCVAE\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 10\n",
    "beta = 0.8 # 50\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"LSTM_VAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(train)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "epochs = 100\n",
    "n_samples = 11\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "learning_rate_disc = 5e-5 # FactorVAE\n",
    "\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "print(\"steps: \", steps_anneal) \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"B_TCVAE\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 10\n",
    "beta = 2 # 50\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = True\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"LSTM_VAE\"\n",
    "#model_path s= f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "#vae = Bernoulli_VAE(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size, bernoulli_prior_p= 0.5)\n",
    "#vae = VAE_Mean_Variance_Decoder(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "vae_semi = LR_SEMIVAE(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size, num_classes= 2, lambda_cls = 1)\n",
    "\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= \"\", train_dataset = train_dataset,test_dataset = test_dataset,\n",
    "                                                                      val_dataset = val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "reducer = None\n",
    "reducer = get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False, reducer = reducer)\n",
    "reducer = get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs, name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = True\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= None)\n",
    "  results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "  reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010100",
   "metadata": {},
   "source": [
    "**HyperParamter Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 100\n",
    "n_samples = 8\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5 # increase this\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"B_VAE\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 2  \n",
    "beta = 20 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "latent_dims = [30, 70]  # Example values for latent dimension\n",
    "beta_values = [50,100]\n",
    "\n",
    "it = 0\n",
    "reconstruction_AD = True\n",
    "latent_AD = False\n",
    "# Iterate over all combinations\n",
    "for latent_dim, beta in itertools.product(latent_dims, beta_values):\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"BASE_LSTM_VAE\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "    print(f\"Training with: latent_dim={latent_dim}, beta={beta} validation_method={validation_method}\")\n",
    "\n",
    "    vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_weakGen = VAE_weakGenerator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_2x = VAE_2x(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    VAE_2x_weakGen = VAE_2x_weak_generator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    optimizer_weakGen = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    optimizer_2x = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    optimizer_2x_weakGen = tf.keras.optimizers.AdamW(learning_rate=cosine_decay_schedule)\n",
    "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "    print(\"VAE--------------\")\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae, optimizer\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE WEAK GENERATOR-------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_weakGen = train_model(vae_weakGen,optimizer_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    del vae_weakGen, optimizer_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X----------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_2x = train_model(vae_2x,optimizer_2x,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_2x, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae_2x, optimizer_2x\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X WEAK GENERATOR------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, VAE_2x_weakGen = train_model(VAE_2x_weakGen,optimizer_2x_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "   \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(VAE_2x_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type='TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "    reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= val_dataset2)\n",
    "    results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "    reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del VAE_2x_weakGen, optimizer_2x_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dea3cc",
   "metadata": {},
   "source": [
    "**Iterative Training on Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular HYPERPARAMETERS \n",
    "#input_dim = np.size(processeddataframe['features'][0])\n",
    "input_dim = 42\n",
    "epochs = 50\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 30  \n",
    "beta = 0 # 20\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"BEST_VAE\"\n",
    "new_model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_{model_name}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = keras.models.load_model(best_model_path)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=weight_decay)\n",
    "optimizer = vae.optimizer\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path = new_model_path)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time, show_val= show_val)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = True)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = False\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path, reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e510e",
   "metadata": {},
   "source": [
    "**Test Saved Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55000e",
   "metadata": {},
   "source": [
    "Latent Dimension = 38, Beta = 30, Gamma = 0, N_critic = 0, Beta_TC = 0, Validation Method = PLOT, Rows in Training Data = 100000, Batch Size = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./Resources/Models/LAT_HCRL_CarHacking_BetaVAE_EPOCHS21_LD10_BETA2_NT50000_INPUT19_13-43.keras\"\n",
    "load_vae = keras.models.load_model(best_model_path)\n",
    "load_vae.trainable = False  # Freeze model weights\n",
    "reducer = None\n",
    "reducer = get_latent_representations_label(load_vae, train_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "reducer = get_latent_representations_label(load_vae, val_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "reducer = get_latent_representations_label(load_vae, test_threshold_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "reducer = get_latent_representations_label(load_vae, test_dataset, 0, 0 ,0,0,0,epoch = 0,name = \"-\",type='PCA', save = False, AWS = AWS, s3 = s3, BUCKET = BUCKET, reducer = reducer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./Resources/Models/LAT_HCRL_CarHacking_BetaVAE_EPOCHS21_LD10_BETA2_NT50000_INPUT19_13-43.keras\"\n",
    "model_name = \"EmbeddingBetaVAE\"\n",
    "#model_path = best_model_path\n",
    "reconstruction_AD = False\n",
    "latent_AD = True\n",
    "reconstruction_threshold, probability_threshold, latent_threshold, mean_train, variance_train, loaded_vae, tree = get_threshold_from_train(model_path,train_dataset, val_dataset,reconstruction_AD, latent_AD, val_dataset2= None)\n",
    "results, results_probs, distances = anomaly_detection(loaded_vae, test_dataset , reconstruction_AD, latent_AD, mean_train, variance_train, tree = tree, debug = True)\n",
    "reconstruction_error_accuracy , reconstruction_probs_accuracy, latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,results_probs,reconstruction_threshold,probability_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time,n_rows_train, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e603307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./Resources/Models/LAT_HCRL_CarHacking_BetaVAE_EPOCHS21_LD10_BETA2_NT50000_INPUT19_13-43.keras\"\n",
    "\n",
    "mean_train, variance_train = get_mean_variances(train_dataset, test = False, load_vae= None, model_path= model_path)\n",
    "mixed_means, mixed_variances, mixed_labels = get_mean_variances(test_dataset, test = True, load_vae= None, model_path= model_path)\n",
    "mixed2_means, mixed2_variances, mixed2_labels = get_mean_variances(test_threshold_dataset, test = True, load_vae= None, model_path= model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mixed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b198ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_means = mixed_means[:5000]\n",
    "mixed_variances = mixed_variances[:5000]\n",
    "mixed_labels = mixed_labels[:5000]\n",
    "\n",
    "print(len(mean_train))\n",
    "print(sum(mixed_labels), len(mixed_labels) - sum(mixed_labels))\n",
    "print(sum(mixed2_labels), len(mixed2_labels) - sum(mixed2_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.array(mean_train).min())\n",
    "scaled_features, scaler = prepare_features(np.array(mean_train), np.array(variance_train))\n",
    "mixed_scaled_features, mixed_scaler = prepare_features(np.array(mixed_means), np.array(mixed_variances))\n",
    "\n",
    "iso_model = train_isolation_forest(scaled_features, contamination=0.005, n_estimators=10000, random_state=42)\n",
    "#svm_model = train_one_class_svm(scaled_features, nu=0.00001, kernel='rbf', gamma='scale')\n",
    "#hdbscan_model = train_hdbscan_detector(np.array(mixed_means),min_cluster_size=500,min_samples=500, metric='cosine')\n",
    "\n",
    "\n",
    "iso_anomaly_mask, iso_anomaly_scores = detect_anomalies_isolation_forest(iso_model, mixed_scaled_features)\n",
    "#svm_anomaly_mask, svm_anomaly_scores = detect_anomalies_one_class_svm(svm_model, mixed_scaled_features)\n",
    "#hdbscan_outliers, test_labels, strengths = detect_anomalies_hdbscan(hdbscan_model , mixed_scaled_features)\n",
    "\n",
    "iso_pca = visualize_anomalies(mixed_scaled_features, iso_anomaly_mask, \"Isolation Forest tSNE\")\n",
    "#svm_pca = visualize_anomalies(mixed_scaled_features, svm_anomaly_mask, \"SVM TSNE\")\n",
    "\n",
    "#hdbscan_pca = visualize_anomalies(mixed_scaled_features, hdbscan_outliers,\"HDBSCAN TSNE\")\n",
    "#visualize_results(scaled_features, mixed_scaled_features, hdbscan_outliers)\n",
    "\n",
    "evaluate_anomaly_detector_verbose(iso_anomaly_scores, np.array(mixed_labels), \"Isolation Forest Accuracy\")\n",
    "#evaluate_anomaly_detector(svm_anomaly_scores, mixed_labels, \"SVM Accuracy\")\n",
    "#evaluate_anomaly_detector(hdbscan_outliers, mixed_labels, \"HDBSCAN Accuracy\")\n",
    "\n",
    "#evaluation = evaluate_hdbscan_detector(mixed_labels, anomaly_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89c3ee",
   "metadata": {},
   "source": [
    "**BENCHMARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model('./Resources/Models/SEMI-Supervised-VAE_EPOCHS36_LD10_BETA3_NT50000_INPUT24_14-57.keras')\n",
    "\n",
    "total_params = model.count_params()\n",
    "memory_in_bytes = total_params * 4  # float32 = 4 bytes\n",
    "memory_in_mb = memory_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Approximate model size in memory: {memory_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params_and_estimate_flops(model, input_shape):\n",
    "    # Count parameters\n",
    "    trainable_params = sum(np.prod(v.shape) for v in model.trainable_variables)\n",
    "    non_trainable_params = sum(np.prod(v.shape) for v in model.non_trainable_variables)\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # For FLOPs estimation, we need to create a proper input shape\n",
    "    # If input_shape is just a dimension, we need to create a proper shape\n",
    "    if isinstance(input_shape, int):\n",
    "        # For VAE models, we typically need to know the input structure\n",
    "        # Let's try to infer it from the model's input spec if possible\n",
    "        try:\n",
    "            # Try to get input shape from model specs\n",
    "            input_spec = model.input_spec\n",
    "            if input_spec:\n",
    "                full_input_shape = input_spec.shape\n",
    "            else:\n",
    "                # Assume a batch dimension and the provided dimension\n",
    "                full_input_shape = (1, input_shape)\n",
    "        except:\n",
    "            # If we can't determine, use a default batch shape\n",
    "            full_input_shape = (1, input_shape)\n",
    "    else:\n",
    "        full_input_shape = input_shape\n",
    "    \n",
    "    try:\n",
    "        # Create a concrete function for the model\n",
    "        dummy_input = tf.ones(full_input_shape, dtype=tf.float32)\n",
    "        \n",
    "        # Call the model once to ensure all variables are created\n",
    "        _ = model(dummy_input)\n",
    "        \n",
    "        concrete_func = tf.function(model).get_concrete_function(\n",
    "            tf.TensorSpec(full_input_shape, tf.float32)\n",
    "        )\n",
    "        \n",
    "        # For newer TF versions, use the recommended approach\n",
    "        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "        frozen_func, _ = convert_variables_to_constants_v2(concrete_func)\n",
    "        \n",
    "        # Calculate FLOPs\n",
    "        from tensorflow.compat.v1.profiler import profile, ProfileOptionBuilder\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = ProfileOptionBuilder.float_operation()\n",
    "        flops = profile(\n",
    "            graph=frozen_func.graph,\n",
    "            run_meta=run_meta, \n",
    "            cmd='op', \n",
    "            options=opts\n",
    "        )\n",
    "        \n",
    "        print(f\"FLOPs: {flops.total_float_ops:,}\")\n",
    "        return trainable_params, non_trainable_params, flops.total_float_ops\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate FLOPs: {e}\")\n",
    "        print(\"Returning parameter counts only\")\n",
    "        return trainable_params, non_trainable_params, None\n",
    "\n",
    "\n",
    "count_params_and_estimate_flops(vae,  input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def monitor_memory(training_function, *args, **kwargs):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage = []\n",
    "    \n",
    "    # Start a background thread to monitor memory\n",
    "    def memory_monitor():\n",
    "        while monitoring:\n",
    "            memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    import threading\n",
    "    monitoring = True\n",
    "    monitor_thread = threading.Thread(target=memory_monitor)\n",
    "    monitor_thread.start()\n",
    "    \n",
    "    # Run the training function\n",
    "    start_time = time.time()\n",
    "    result = training_function(*args, **kwargs)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Stop monitoring\n",
    "    monitoring = False\n",
    "    monitor_thread.join()\n",
    "    \n",
    "    # Plot memory usage\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(memory_usage)\n",
    "    plt.title('Memory Usage During Training')\n",
    "    plt.xlabel('Time (0.1s intervals)')\n",
    "    plt.ylabel('Memory (MB)')\n",
    "    plt.savefig('memory_usage.png')\n",
    "    \n",
    "    print(f\"Peak memory usage: {max(memory_usage):.2f} MB\")\n",
    "    print(f\"Average memory usage: {sum(memory_usage)/len(memory_usage):.2f} MB\")\n",
    "    print(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    return result, memory_usage, execution_time\n",
    "\n",
    "# Usage example:\n",
    "# result, memory_data, exec_time = monitor_memory(train_model_semi, vae, optimizer, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c6f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a57642",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545506f7",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861000fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "❌ Using CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Changes AWS to True if on SageMaker Instance and set S3 BUCKET and Key accordingly\n",
    "AWS = False\n",
    "BUCKET = 'eu-west-1'\n",
    "REGION = 'ml-can-ids-logs'\n",
    "s3 = None\n",
    "\n",
    "# Import Functions\n",
    "import setuptools.dist\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "import anomaly_detection_functions\n",
    "import feature_selection\n",
    "import train\n",
    "\n",
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "reload(feature_selection)\n",
    "reload(train)\n",
    "from utils import plot_loss_curve, plot_pca, plot_tsne, get_confusion_matrix, get_latent_representations_label, analyze_latent_variance, analyze_kl_divergence, linear_annealing, save_results_to_excel, save_trained_model, get_s3_client, check_dataset\n",
    "from anomaly_detection_functions import get_threshold_from_train, anomaly_detection, get_anomaly_detection_accuracy\n",
    "from feature_selection import feature_selection_preparation, convert_to_tensorflow\n",
    "from train import train_model\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers, Model\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import keras\n",
    "#from tsfresh import extract_features\n",
    "#from tsfresh.feature_extraction.settings import EfficientFCParameters\n",
    "#from tsfresh import select_features\n",
    "#from tsfresh.utilities.dataframe_functions import impute\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from importlib import reload\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "if AWS:\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # No wrapping, long rows won't be cut off\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content (especially useful for long strings)\n",
    "\n",
    "# Remove this after testing/debugging\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  \n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"✅ Using GPU\")\n",
    "    device = \"/GPU:0\"\n",
    "else:\n",
    "    print(\"❌ Using CPU\")\n",
    "    device = \"/CPU:0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715700",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2753d",
   "metadata": {},
   "source": [
    "**PATH FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bae76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AWS:\n",
    "    s3 = get_s3_client(REGION, BUCKET, True)\n",
    "    \n",
    "    # Get S3 Object \n",
    "    channel2logs = s3.get_object(Bucket = BUCKET, Key= 'channel2Logs.csv')\n",
    "    dos_attack_channel2 = s3.get_object(Bucket = BUCKET, Key= 'dos_attack_channel2.csv')\n",
    "    replay_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'replay_attack_channel2.csv') \n",
    "    spoofing_attack_channel2 = s3.get_object(Bucket = BUCKET, Key = 'new_spoofing_attack_channel2.csv') \n",
    "\n",
    "    channel2logs = channel2logs['Body'].read().decode('utf-8')\n",
    "    dos_attack_channel2 = dos_attack_channel2['Body'].read().decode('utf-8')\n",
    "    replay_attack_channel2 = replay_attack_channel2['Body'].read().decode('utf-8')\n",
    "    spoofing_attack_channel2 = spoofing_attack_channel2['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Get Content\n",
    "    channel2logs = StringIO(channel2logs)\n",
    "    dos_attack_channel2 = StringIO(dos_attack_channel2)\n",
    "    replay_attack_channel2 = StringIO(replay_attack_channel2)\n",
    "    spoofing_attack_channel2 = StringIO(spoofing_attack_channel2)\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = dos_attack_channel2 # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = replay_attack_channel2 # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = spoofing_attack_channel2 # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel2_csv_path = channel2logs # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = \"\" # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = \"\" # Green Channel\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = \"\"\n",
    "else:\n",
    "    # Unprocessed Normal and Attack Data\n",
    "    preprocessed_normal_csv_path = './Dataset/Tw22206_L003_with_ecu_channel.csv'  # Normal Unprocessed\n",
    "    preprocessed_DoS_csv_path = './Dataset/Attack_Logs/dos_attack.csv'  # Dos Unprocessed\n",
    "    preprocessed_Fuzzy_csv_path = './Dataset/Attack_Logs/fuzzy_attack.csv'  # Fuzzy Unprocessed\n",
    "    preprocessed_Replay_csv_path = './Dataset/Attack_Logs/replay_attack.csv'  # Replay Unprocessed - Test\n",
    "    preprocessed_Spoofing_csv_path = './Dataset/Attack_Logs/spoofing_attack.csv'  # Spoofing Unprocessed\n",
    "    preprocessed_Suspension_csv_path = './Dataset/Attack_Logs/suspension_attack.csv'  # Suspension Unprocessed - Hardest Attack Type\n",
    "\n",
    "\n",
    "    # Attack based on Channel\n",
    "    preprocessed_DoS_channel2_csv_path = './Dataset/Attack_Logs/dos_attack_channel2.csv'  # DoS on channel 2 (Red Channel)\n",
    "    preprocessed_Replay_channel2_csv_path = './Dataset/Attack_Logs/replay_attack_channel2.csv'  # Replay on channel 2 (Red Channel)\n",
    "    preprocessed_Suspension_channel2_csv_path = './Dataset/Attack_Logs/suspension_attack_channel2.csv'  # Suspension on channel 2 (Red Channel)\n",
    "    preprocessed_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "    preprocessed_new_Spoofing_channel2_csv_path = './Dataset/Attack_Logs/new_spoofing_attack_channel2.csv'  # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "\n",
    "    # Unprocessed Channel Data\n",
    "    preprocessed_normal_channel0_csv_path = './Dataset/Channel_Logs/channel0Logs.csv'\n",
    "    preprocessed_normal_channel2_csv_path = './Dataset/Channel_Logs/channel2Logs.csv'  # Red Channel\n",
    "    preprocessed_normal_channel4_csv_path = './Dataset/Channel_Logs/channel4Logs.csv'  # Yellow Channel\n",
    "    preprocessed_normal_channel5_csv_path = './Dataset/Channel_Logs/channel5Logs.csv'  # Green Channel\n",
    "\n",
    "\n",
    "    # Preprocessed Dataframe Data\n",
    "    processeddataframe_normal_csv_path = './Dataset/Processed_Dataframes/train_dataframe.csv'  # Normal CSV Dataframe (Turns Lists into Strings)\n",
    "    processeddataframe_DoS_csv_path = './Dataset/Processed_Dataframes/test_DoS_dataframe.csv'  # DoS CSV Dataframe (Turns Lists into Strings)\n",
    "\n",
    "    # Preprocessed Pickle Data\n",
    "    processeddataframe_normal_pickle_path = './Dataset/Processed_Dataframes/train_Normal_dataframePickle.pkl'  # Normal Pickle Dataframe\n",
    "    processeddataframe_DoS_pickle_path = './Dataset/Processed_Dataframes/test_DoS_dataframePickle.pkl'  # DoS Pickle Dataframe\n",
    "\n",
    "    # Current best model\n",
    "    best_model_path = './Code/Models/BEST_LSTM_VAE_LD30_Beta25_NT800000_21-37.keras'\n",
    "\n",
    "\n",
    "\n",
    "    # PRELOAD Dataframe for Debug\n",
    "    DEBUG = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff978e0",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49866c78",
   "metadata": {},
   "source": [
    "Tsfresh parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fc_parameters = {\n",
    "    # Statistical Features\n",
    "    \"mean\": None,\n",
    "    \"skewness\": None,\n",
    "    \"kurtosis\": None,\n",
    "    \n",
    "    # Temporal Dependency Features\n",
    "    \"autocorrelation\": [{\"lag\": 1}],  # Detects repeated patterns (useful for replay attacks)\n",
    "    \"agg_autocorrelation\": [{\"f_agg\": \"mean\", \"maxlag\": 5}],  # Aggregate autocorrelation over multiple lags\n",
    "    \"agg_linear_trend\": [{\"attr\": \"slope\", \"chunk_len\": 5, \"f_agg\": \"mean\"}],  # Detects trends in timing\n",
    "    \n",
    "    # Frequency Features\n",
    "    \"fft_coefficient\": [{\"coeff\": 1, \"attr\": \"real\"}],  # Extract 1st FFT coefficient (frequency analysis)\n",
    "    \"fft_aggregated\": [{\"aggtype\": \"centroid\"}],  # Spectral centroid (detects abnormal frequency changes)\n",
    "\n",
    "    # Entropy Features (Detects randomness or spoofing)\n",
    "    \"approximate_entropy\": [{\"m\": 2, \"r\": 0.1}],  # Measures unpredictability (spoofed messages might increase entropy)\n",
    "    \"binned_entropy\": [{\"max_bins\": 10}],  # Useful for detecting shifts in data distribution\n",
    "\n",
    "    # Complexity & Energy-Based Features\n",
    "    \"cid_ce\": [{\"normalize\": True}],  # Measures signal complexity (can detect spoofing)\n",
    "    \"absolute_sum_of_changes\": None,  # Captures sudden payload/timing shifts (useful for injection attacks)\n",
    "    \"energy_ratio_by_chunks\": [{\"num_segments\": 5, \"segment_focus\": 2}],  # Detects anomalies in energy distribution\n",
    "\n",
    "    # Stationarity & Distribution-Based Features\n",
    "    \"augmented_dickey_fuller\": [{\"attr\": \"teststat\"}],  # Tests stationarity (useful for detecting sudden changes)\n",
    "    \"benford_correlation\": None,  # Checks if data follows natural distribution (detects synthetic spoofing)\n",
    "\n",
    "    }\n",
    "\n",
    "\"\"\"  \n",
    "    ALL Features extracted ATM, add parameters above in 'custom_fc_parameters':\n",
    "\n",
    "    'iat', 'msg_frequency', 'rolling_mean_iat', 'rolling_std_iat', \n",
    "    'value__binned_entropy__max_bins_10', 'value__skewness', \n",
    "    'value__autocorrelation__lag_1', 'value__agg_autocorrelation__f_agg_\"mean\"__maxlag_5', \n",
    "    'value__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', \n",
    "    'value__fft_coefficient__attr_\"real\"__coeff_1', \n",
    "    'value__approximate_entropy__m_2__r_0.1', 'value__binned_entropy__max_bins_10', \n",
    "    'value__benford_correlation'\n",
    "\"\"\"\n",
    "# List of selected tsfresh features\n",
    "tsfresh_features = [\n",
    "    'iat', 'msg_frequency', 'rolling_mean_iat', 'rolling_std_iat', \n",
    "    'value__binned_entropy__max_bins_10', 'value__skewness', \n",
    "    'value__autocorrelation__lag_1', 'value__agg_autocorrelation__f_agg_\"mean\"__maxlag_5', \n",
    "    'value__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', \n",
    "    'value__fft_coefficient__attr_\"real\"__coeff_1', \n",
    "    'value__approximate_entropy__m_2__r_0.1', 'value__binned_entropy__max_bins_10', \n",
    "    'value__benford_correlation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00c046ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking original data types in ./Dataset/Channel_Logs/channel2Logs.csv...\n",
      "Original inferred data types:\n",
      "  Unnamed: 0: int64\n",
      "  timestamp: float64\n",
      "  arbitration_id: int64\n",
      "  channel: int64\n",
      "  dlc: int64\n",
      "  data: object\n",
      "  ecu: object\n",
      "\n",
      "Sample values for each column:\n",
      "  Unnamed: 0: 133 (type: int64)\n",
      "  timestamp: 6.435292 (type: float64)\n",
      "  arbitration_id: 218068007 (type: int64)\n",
      "  channel: 2 (type: int64)\n",
      "  dlc: 8 (type: int64)\n",
      "  data: fc ff ff ff 03 ff ff ff (type: str)\n",
      "  ecu: {'GW_K', 'Coordinator_K'} (type: str)\n",
      "False\n",
      "Original window that works:  (19985, 80, 42)\n",
      "Feature shape BEFORE sliding window: (100000, 42)\n",
      "Feature shape AFTER sliding window: (19985, 80, 42)\n",
      "Checking original data types in ./Dataset/Attack_Logs/new_spoofing_attack_channel2.csv...\n",
      "Original inferred data types:\n",
      "  timestamp: float64\n",
      "  arbitration_id: float64\n",
      "  dlc: int64\n",
      "  data: object\n",
      "  type: object\n",
      "\n",
      "Sample values for each column:\n",
      "  timestamp: 6.435292 (type: float64)\n",
      "  arbitration_id: 218068007.0 (type: float64)\n",
      "  dlc: 8 (type: int64)\n",
      "  data: fc ff ff ff 03 ff ff ff (type: str)\n",
      "  type: R (type: str)\n",
      "Raw type values: ['R' 'T']\n",
      "Anomalies in 'type' column: 4667\n",
      "False\n",
      "Original window that works:  (7985, 80, 42)\n",
      "Anomalies in 'y' array: 5531.0\n",
      "x before tensor 7985\n",
      "y before tensor 7985\n",
      "Feature shape BEFORE sliding window: (40000, 42)\n",
      "Feature shape AFTER sliding window: (7985, 80, 42)\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "reload(feature_selection)\n",
    "LOAD_DATAFRAME = False\n",
    "TS_FRESH = False\n",
    "\n",
    "n_rows_train = 100000    # select how many rows to load. None if whole train datasset\n",
    "n_rows_test = 40000   # select how many rows to load. None if whole test datasset\n",
    "batch_size = 1024\n",
    "window_size = 80    # increase window size\n",
    "stride = 5           # increase stride as a buffer\n",
    "split_ratio = 0.8     # % of training data to use for training\n",
    "window_anomaly_ratio = 0.1   # For 1 anomaly per window do: 1 / window_size\n",
    "if LOAD_DATAFRAME:\n",
    "    # Load training data\n",
    "    processeddataframe = pd.read_pickle(processeddataframe_normal_pickle_path)\n",
    "    train_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size)\n",
    "\n",
    "    # Load test data\n",
    "    processeddataframe_test = pd.read_pickle(processeddataframe_DoS_pickle_path)\n",
    "    test_dataset = convert_to_tensorflow(processeddataframe_test['features'] ,processeddataframe_test['type'], batch_size= batch_size )\n",
    "else:\n",
    "    # Preprocess and load training data\n",
    "    processeddataframe = feature_selection_preparation(preprocessed_normal_channel2_csv_path, 'training', rows=n_rows_train, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    train_dataset, val_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size, window_size = window_size, stride = stride, split_ratio= split_ratio)\n",
    "\n",
    "    processeddataframe_test = feature_selection_preparation(preprocessed_new_Spoofing_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    test_dataset, test_threshold_dataset = convert_to_tensorflow(processeddataframe_test['features'], processeddataframe_test['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)\n",
    "\n",
    "    #processeddataframe_dos_baseline = feature_selection_preparation(preprocessed_DoS_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    #dos_test_dataset, dos_threshold_dataset = convert_to_tensorflow(processeddataframe_dos_baseline['features'], processeddataframe_dos_baseline['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8eed2",
   "metadata": {},
   "source": [
    "**SAVE Dataset to npz [FIX]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/Channel2_Full_43Features_WindowFeatures_1024Batch_50Window_5Stride.npz\"\n",
    "\n",
    "# Save your datasets\n",
    "save_datasets(train_dataset, val_dataset, path)\n",
    "\n",
    "# Later, load them back\n",
    "train_dataset_loaded, val_dataset_loaded = load_datasets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check that all values normalized or there's nans in the sliding windows\n",
    "# todo: check whole processing of df\n",
    "print(processeddataframe['features'].head(4))\n",
    "print(np.size(processeddataframe['features'][0]))\n",
    "\n",
    "processeddataframe['has_nan'] = processeddataframe['features'].apply(lambda x: any(pd.isna(x)) if isinstance(x, list) else np.nan)\n",
    "valid_lists = [lst for lst in processeddataframe['features'] if isinstance(lst, list)]\n",
    "all_values = sum(valid_lists, [])\n",
    "has_nan = any(pd.isna(all_values))\n",
    "has_out_of_bounds = any(x < 0 or x > 1 for x in all_values if isinstance(x, (int, float)))\n",
    "\n",
    "print(f\"Contains NaN: {has_nan}\")\n",
    "print(f\"Contains values <0 or >1: {has_out_of_bounds}\")\n",
    "\n",
    "# Run checks for both datasets\n",
    "check_dataset(train_dataset, \"Train Dataset\")\n",
    "check_dataset(val_dataset, \"Validation Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70758afc",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6f1cb",
   "metadata": {},
   "source": [
    "**VAE SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ecbbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"Base_VAE_multiplesamples\")\n",
    "class Base_VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size, **kwargs):\n",
    "        super(Base_VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            layers.BatchNormalization(), # Experiment: BN\n",
    "            layers.LSTM(64, activation='relu', return_sequences= False),\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size),  \n",
    "            layers.LSTM(64, activation='relu', return_sequences=True),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "\n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(Base_VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c54e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamples\")\n",
    "class VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(32, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(32, activation = 'relu', return_sequences = True), \n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6797623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_weak_generator\")\n",
    "class VAE_weakGenerator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_weakGenerator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(32, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(16, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_weakGenerator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd89b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x\")\n",
    "class VAE_2x(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(64, activation='relu', return_sequences=True), \n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(64, activation = 'relu', return_sequences = True), \n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e316b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x_weak_generator\")\n",
    "class VAE_2x_weak_generator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x_weak_generator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(64, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(32, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "    \"Disentangling by Factorising\"\n",
    "    (https://arxiv.org/abs/1802.05983)\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## β-TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements β-TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "\n",
    "        from: ..\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x_weak_generator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb85df8",
   "metadata": {},
   "source": [
    "**VQ-VAE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88900688",
   "metadata": {},
   "source": [
    "**SEMI SUPERVISED: LR-SEMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\"\"\"\n",
    "LR-SEMI is based on the paper:\n",
    "\n",
    "\"Semisupervised anomaly detection of multivariate time series based on a variational autoencoder\"\n",
    "(https://doi.org/10.1007/s10489-022-03829-1)\n",
    "\n",
    "Since we only use labeled data we are omitting the unlabeled loss shown in equation 2\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package=\"LR_SEMIVAE\")\n",
    "class LR_SEMIVAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size,num_classes, lambda_cls, **kwargs):\n",
    "        super(LR_SEMIVAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_cls = lambda_cls \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            layers.GlobalAveragePooling1D()\n",
    "\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim + 1,)), # + 1 for the z + label concat\n",
    "            layers.RepeatVector(window_size),  \n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "\n",
    "        self.LR_classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='tanh'),  # Linear → Tanh\n",
    "            tf.keras.layers.Dense(64, activation='tanh'),   # Linear → Tanh\n",
    "            tf.keras.layers.Dense(32),                      # Linear (no activation)\n",
    "            tf.keras.layers.Dense(input_dim, activation='sigmoid')  # Final softmax layer? wrong\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "\n",
    "    def decode(self, z, y = None):\n",
    "        return \n",
    "\n",
    "    def call(self, inputs, n_samples=1, training = None):\n",
    "        return\n",
    "    def compute_loss(self, x , y, model_outputs, n_samples = 1):\n",
    "        return\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LR_SEMIVAE, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'window_size': self.window_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'lambda_cls': self.lambda_cls\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863771e",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Done\n",
    "\n",
    "# Model parameters\n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "latent_dim = 32\n",
    "num_classes = 2\n",
    "lambda_cls = 1.0\n",
    "\n",
    "test_dataset = test_dataset.map(lambda x, y: (x, y))  # No need for one-hot encoding\n",
    "\n",
    "# Check the shape of the dataset\n",
    "for batch, label in test_dataset.take(1):  # Or val_dataset if testing\n",
    "    print(\"Batch shape: \", batch.shape)  # Print the shape of the input data\n",
    "    print(\"Label shape: \", label.shape)  # Print the shape of the labels\n",
    "\n",
    "# Create the model\n",
    "model = LR_SEMIVAE(\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    window_size=window_size,\n",
    "    num_classes=num_classes,\n",
    "    lambda_cls=lambda_cls\n",
    ")\n",
    "optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4)\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_model(model, optimizer, epochs, batch_size, train_dataset):\n",
    "    # Initialize variables for tracking losses and epochs\n",
    "    epoch_loss = 0\n",
    "    real_epochs = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        real_epochs += 1\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Loop through each batch in the training dataset\n",
    "        for x, y in test_dataset:  # Loop through batches\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                model_outputs = model([x,y], n_samples=1, training=True)\n",
    "                # Compute loss\n",
    "                loss_dict = model.compute_loss(x, y, model_outputs)\n",
    "                total_loss = loss_dict['total_loss']\n",
    "\n",
    "            # Compute gradients and apply them\n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)  # Replace `model.trainable_variables`\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Replace `model.trainable_variables`\n",
    "\n",
    "            epoch_loss += total_loss.numpy()  # Accumulate loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_dataset)}\")\n",
    "\n",
    "train_model(model, optimizer,5,batch_size, test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9a54",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "reload(train)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 100\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"None\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 30  \n",
    "beta = 20 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"LSTM_VAE\"\n",
    "model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs, name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = True\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path,reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010100",
   "metadata": {},
   "source": [
    "**HyperParamter Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f09d4d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: latent_dim=25, beta=3 validation_method=PLOT\n",
      "VAE--------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 64\u001b[0m\n\u001b[0;32m     57\u001b[0m discriminator_optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdamW(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate_disc)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAE--------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m train_losses, val_losses, real_epochs, time, show_val, model_path, vae \u001b[38;5;241m=\u001b[39m train_model(vae,optimizer,discriminator_optimizer, epochs,\n\u001b[0;32m     61\u001b[0m                                                                 n_samples, input_dim, latent_dim, batch_size,beta,\n\u001b[0;32m     62\u001b[0m                                                                   gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n\u001b[0;32m     63\u001b[0m                                                                   model_path\u001b[38;5;241m=\u001b[39m model_path, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m---> 64\u001b[0m                                                                   val_dataset\u001b[38;5;241m=\u001b[39mval_dataset,n_rows_train\u001b[38;5;241m=\u001b[39mn_rows_train,AWS \u001b[38;5;241m=\u001b[39m AWS,s3 \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m, BUCKET\u001b[38;5;241m=\u001b[39mBUCKET)\n\u001b[0;32m     67\u001b[0m plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val\u001b[38;5;241m=\u001b[39m show_val, AWS \u001b[38;5;241m=\u001b[39m AWS, s3 \u001b[38;5;241m=\u001b[39m s3, BUCKET \u001b[38;5;241m=\u001b[39m BUCKET)\n\u001b[0;32m     68\u001b[0m get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch \u001b[38;5;241m=\u001b[39m real_epochs,name \u001b[38;5;241m=\u001b[39m model_name,\u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSNE\u001b[39m\u001b[38;5;124m'\u001b[39m, save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, AWS \u001b[38;5;241m=\u001b[39m AWS, s3 \u001b[38;5;241m=\u001b[39m s3, BUCKET \u001b[38;5;241m=\u001b[39m BUCKET)\n",
      "\u001b[1;31mNameError\u001b[0m: name 's3' is not defined"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 80\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 2  \n",
    "beta = 1 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "latent_dims = [25,45,35]  # Example values for latent dimension\n",
    "beta_values = [3,30,15]\n",
    "\n",
    "\n",
    "# Iterate over all combinations\n",
    "for latent_dim, beta in itertools.product(latent_dims, beta_values):\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"BASE_LSTM_VAE\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "    print(f\"Training with: latent_dim={latent_dim}, beta={beta} validation_method={validation_method}\")\n",
    "\n",
    "    vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_weakGen = VAE_weakGenerator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_2x = VAE_2x(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    VAE_2x_weakGen = VAE_2x_weak_generator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_weakGen = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_2x = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_2x_weakGen = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "    print(\"VAE--------------\")\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae, optimizer\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE WEAK GENERATOR-------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_weakGen = train_model(vae_weakGen,optimizer_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    del vae_weakGen, optimizer_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X----------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_2x = train_model(vae_2x,optimizer_2x,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "    \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae_2x, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del vae_2x, optimizer_2x\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n",
    "    \n",
    "    print(\"VAE 2X WEAK GENERATOR------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, VAE_2x_weakGen = train_model(VAE_2x_weakGen,optimizer_2x_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path, train_dataset=train_dataset,test_dataset=test_dataset,\n",
    "                                                                      val_dataset=val_dataset,n_rows_train=n_rows_train,AWS = AWS,s3 = s3, BUCKET=BUCKET)\n",
    "\n",
    "   \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(VAE_2x_weakGen, test_dataset, latent_dim, beta,n_critic,gamma,time,epoch = real_epochs,name = model_name,type='TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    del VAE_2x_weakGen, optimizer_2x_weakGen\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    #tf.config.experimental.reset_memory_stats('/GPU:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dea3cc",
   "metadata": {},
   "source": [
    "**Iterative Training on Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular HYPERPARAMETERS \n",
    "#input_dim = np.size(processeddataframe['features'][0])\n",
    "input_dim = 42\n",
    "epochs = 50\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 30  \n",
    "beta = 0 # 20\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"BEST_VAE\"\n",
    "new_model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_{model_name}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = keras.models.load_model(best_model_path)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=weight_decay)\n",
    "optimizer = vae.optimizer\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path = new_model_path)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time, show_val= show_val)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = True)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = False\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path, reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e510e",
   "metadata": {},
   "source": [
    "**Test Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_path = \"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_LSTM_VAE_LD30_Beta25_NT800000_21-37.keras\"\n",
    "#loaded_vae = keras.models.load_model(\"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_BEST_VAE_11-15.keras\")\n",
    "loaded_vae = keras.models.load_model(model_path)\n",
    "loaded_vae.trainable = False  # Freeze model weights\n",
    "\n",
    "get_latent_representations_label(loaded_vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

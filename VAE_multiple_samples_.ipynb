{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545506f7",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861000fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Functions\n",
    "from importlib import reload\n",
    "import utils\n",
    "import anomaly_detection_functions\n",
    "\n",
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "from utils import plot_loss_curve, plot_pca, plot_tsne, get_confusion_matrix, get_latent_representations_label, analyze_latent_variance, analyze_kl_divergence, linear_annealing, save_results_to_excel, save_trained_model\n",
    "from anomaly_detection_functions import get_threshold_from_train, anomaly_detection, get_anomaly_detection_accuracy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import keras\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.settings import EfficientFCParameters\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from importlib import reload\n",
    "import os\n",
    "#keras.saving.get_custom_objects().clear()\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # No wrapping, long rows won't be cut off\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content (especially useful for long strings)\n",
    "\n",
    "# Remove this after testing/debugging\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  \n",
    "\n",
    "AWS = False\n",
    "s3 = None\n",
    "BUCKET = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715700",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2753d",
   "metadata": {},
   "source": [
    "**PATH FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unprocessed Normal and Attack Data\n",
    "preprocessed_normal_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Tw22206_L003_with_ecu_channel.csv\" # Normal Unprocessed\n",
    "preprocessed_DoS_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/dos_attack.csv\" # Dos Unprocessed\n",
    "preprocessed_Fuzzy_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/fuzzy_attack.csv\" # Fuzzy Unprocessed\n",
    "preprocessed_Replay_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/replay_attack.csv\" # Replay Unprocessed - Test\n",
    "preprocessed_Spoofing_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/spoofing_attack.csv\" # Spoofing Unprocessed\n",
    "preprocessed_Suspension_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/suspension_attack.csv\" # Suspension Unprocessed - Hardest Attack Type\n",
    "\n",
    "# Attack based on Channel\n",
    "preprocessed_DoS_channel2_csv_path = r'/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/dos_attack_channel2.csv' # DoS on channel 2 (Red Channel)\n",
    "preprocessed_Replay_channel2_csv_path = r'/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/replay_attack_channel2.csv' # Replay on channel 2 (Red Channel)\n",
    "preprocessed_Suspension_channel2_csv_path = r'/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/suspension_attack_channel2.csv' # Suspension on channel 2 (Red Channel)\n",
    "preprocessed_Spoofing_channel2_csv_path = r'/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/spoofing_attack_channel2.csv' # Spoofing on channel 2 (Red Channel)\n",
    "preprocessed_new_Spoofing_channel2_csv_path = r'/Users/SCHUGD/Desktop/MasterThesis/Dataset/Attack_Logs/new_spoofing_attack_channel2.csv' # Spoofing on channel 2 (Red Channel)\n",
    "\n",
    "# Unprocessed Channel Data\n",
    "preprocessed_normal_channel0_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Channel_Logs/channel0Logs.csv\"\n",
    "preprocessed_normal_channel2_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Channel_Logs/channel2Logs.csv\" # Red Channel\n",
    "preprocessed_normal_channel4_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Channel_Logs/channel4Logs.csv\" # Yellow Channel\n",
    "preprocessed_normal_channel5_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Channel_Logs/channel5Logs.csv\" # Green Channel\n",
    "\n",
    "# Preprocessed Dataframe Data\n",
    "processeddataframe_normal_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/train_dataframe.csv\" # Normal CSV Dataframe (Turns Lists into Strings)\n",
    "processeddataframe_DoS_csv_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/test_DoS_dataframe.csv\" # DoS CSV Dataframe (Turns Lists into Strings)\n",
    "\n",
    "# Preprocessed Pickle Data\n",
    "processeddataframe_normal_pickle_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/train_Normal_dataframePickle.pkl\" # Normal Pickle Dataframe\n",
    "processeddataframe_DoS_pickle_path = r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/test_DoS_dataframePickle.pkl\" # DoS Pickle Dataframe\n",
    "\n",
    "# Current best model\n",
    "best_model_path = \"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_LSTM_VAE_LD30_Beta25_NT800000_21-37.keras\"\n",
    "\n",
    "\n",
    "# PRELOAD Dataframe for Debug\n",
    "DEBUG = False \n",
    "\n",
    "if DEBUG:\n",
    "    column_names = ['timestamp' , 'arbitration_id' , 'channel' , 'dlc', 'data' , 'ecu']\n",
    "    pre_dataframe = pd.read_csv(preprocessed_normal_csv_path, parse_dates=['timestamp'], header =0, names=column_names, nrows= 50000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff978e0",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7e14a",
   "metadata": {},
   "source": [
    "**FEATURE SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(ids, num_bits):\n",
    "    \"\"\"\n",
    "    Converts a list of CAN IDs (hexadecimal strings) to binary representation.\n",
    "    \n",
    "    Args:\n",
    "        ids (list of str): List of CAN IDs as hexadecimal strings (e.g., ['0x101', '0x102']).\n",
    "        num_bits (int): Number of bits to represent the IDs in binary format.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of binary representations, where each row is a binary vector.\n",
    "    \"\"\"\n",
    "    binary_ids = []\n",
    "    for id_str in ids:\n",
    "        # Convert hexadecimal string to integer\n",
    "        id_int = int(id_str, 16)\n",
    "        # Convert integer to binary and pad with leading zeros\n",
    "        binary_vector = [int(bit) for bit in f\"{id_int:0{num_bits}b}\"]\n",
    "        binary_ids.append(binary_vector)\n",
    "    return np.array(binary_ids)\n",
    "\n",
    "\n",
    "\n",
    "def binary_encode_integers(ids, num_bits):\n",
    "    \"\"\"\n",
    "    Converts a list of CAN IDs (integers) to binary representation.\n",
    "\n",
    "    Args:\n",
    "        ids (list of int): List of CAN IDs as integers (e.g., [452948266, 452946218]).\n",
    "        num_bits (int): Number of bits to represent the IDs in binary format.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of binary representations, where each row is a binary vector.\n",
    "    \"\"\"\n",
    "    binary_ids = []\n",
    "    for id_int in ids:\n",
    "        # Convert integer to binary and pad with leading zeros\n",
    "        binary_vector = [int(bit) for bit in f\"{id_int:0{num_bits}b}\"]\n",
    "        binary_ids.append(binary_vector)\n",
    "    return np.array(binary_ids)\n",
    "\n",
    "def compute_temporal_features_tsfresh(dataframe, custom_fc_parameters = None, ts_fresh = False):\n",
    "    \"\"\"\n",
    "    Computes tsfresh features while preserving message-level granularity.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): CAN data with 'timestamp', 'arbitration_id', 'data'.\n",
    "        window_size (int): Rolling window size for local statistics.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Original dataframe with merged tsfresh features.\n",
    "    \"\"\"\n",
    "    #dataframe = dataframe.copy()\n",
    "    dataframe.sort_values(by=\"timestamp\")\n",
    "    # Check if timestamp is already a float and handle accordingly\n",
    "    if dataframe[\"timestamp\"].dtype != 'float64':\n",
    "        # Convert the 'timestamp' column to datetime format and handle errors\n",
    "        print(\"dates not floats\")\n",
    "        dataframe[\"timestamp\"] = pd.to_datetime(dataframe[\"timestamp\"], errors='coerce')\n",
    "\n",
    "    # Check for NaT values and print those rows for inspection\n",
    "    if dataframe[\"timestamp\"].isna().sum() > 0:\n",
    "        print(f\"Found {dataframe['timestamp'].isna().sum()} NaT values in timestamp. Rows: {dataframe[dataframe['timestamp'].isna()]}\")\n",
    "\n",
    "    # Compute Inter-Arrival Time (IAT)\n",
    "    dataframe[\"iat\"] = dataframe.groupby(\"arbitration_id\")[\"timestamp\"].diff().fillna(0)\n",
    "\n",
    "    # Check for NaN values in 'iat' and handle\n",
    "    if dataframe[\"iat\"].isna().sum() > 0:\n",
    "        print(\"NaN values found in 'iat' column\")\n",
    "        dataframe[\"iat\"] = dataframe[\"iat\"].fillna(0)  # Replace NaNs with 0 or other logic\n",
    "\n",
    "    # Compute Rolling Window Statistics per message\n",
    "    window_size_seconds = 0.2 # interval between log 1 and 50 is approx 0.18seconds\n",
    "    timestamps = dataframe[\"timestamp\"].to_numpy()\n",
    "    arbitration_ids = dataframe[\"arbitration_id\"].to_numpy()\n",
    "\n",
    "    # Initialize the result array\n",
    "    msg_count_last_20ms = np.zeros(len(dataframe), dtype=int)\n",
    "\n",
    "    # Define the window size in seconds (50 milliseconds = 0.050 seconds)\n",
    "\n",
    "    # Iterate through each row and compute the count\n",
    "    for i, (current_time, current_id) in enumerate(zip(timestamps, arbitration_ids)):\n",
    "        mask = (timestamps >= current_time - window_size_seconds) & \\\n",
    "            (timestamps <= current_time) & \\\n",
    "            (arbitration_ids == current_id)\n",
    "        msg_count_last_20ms[i] = np.sum(mask)\n",
    "\n",
    "    # Add the result to the dataframe\n",
    "    dataframe[\"msg_frequency\"] = msg_count_last_20ms\n",
    "\n",
    "    # Normalize msg_count_last_50ms to [0, 1]\n",
    "    max_count = dataframe[\"msg_frequency\"].max()\n",
    "    if max_count > 0:\n",
    "        dataframe[\"msg_frequency\"] = dataframe[\"msg_frequency\"] / max_count\n",
    "    else:\n",
    "        dataframe[\"msg_frequency\"] = 0  # If max_count is 0, set all normalized values to 0\n",
    "\n",
    "    dataframe[\"rolling_mean_iat\"] = dataframe.groupby(\"arbitration_id\")[\"iat\"].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    dataframe[\"rolling_std_iat\"] = dataframe.groupby(\"arbitration_id\")[\"iat\"].transform(lambda x: x.rolling(20, min_periods=1).std().fillna(0))\n",
    "\n",
    "    # Check for NaN values in 'id' or 'time' columns and handle\n",
    "    if dataframe['arbitration_id'].isna().sum() > 0 or dataframe['timestamp'].isna().sum() > 0:\n",
    "        print(\"NaN values found in 'id' or 'time' columns\")\n",
    "        dataframe = dataframe.dropna(subset=['arbitration_id', 'timestamp'])\n",
    "\n",
    "    if ts_fresh:\n",
    "        # Prepare for tsfresh\n",
    "        tsfresh_df = dataframe.rename(columns={\"arbitration_id\": \"id\", \"timestamp\": \"time\", \"iat\": \"value\"})\n",
    "        tsfresh_df = tsfresh_df[['id', 'time', 'value']]\n",
    "\n",
    "\n",
    "        if custom_fc_parameters is None:\n",
    "            custom_fc_parameters = EfficientFCParameters()\n",
    "\n",
    "        \n",
    "        extracted_features = extract_features(tsfresh_df, column_id=\"id\", column_sort=\"time\", \n",
    "                                        default_fc_parameters=custom_fc_parameters)\n",
    "        \n",
    "        # Impute missing values\n",
    "        impute(extracted_features)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_features = scaler.fit_transform(extracted_features)\n",
    "        normalized_extracted_features = pd.DataFrame(normalized_features, columns=extracted_features.columns, index=extracted_features.index)\n",
    "        \n",
    "        # Merge extracted features back to the original dataframe without losing time-series order\n",
    "        dataframe = dataframe.merge(normalized_extracted_features, left_on=\"arbitration_id\", right_index=True, how=\"left\")\n",
    "        dataframe.sort_values(by=\"timestamp\")\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def calculate_entropy(row):\n",
    "    byte_values = [row[f'data[{i}]'] for i in range(8)]\n",
    "    # Compute frequency of each byte value (0-255)\n",
    "    counts = np.bincount(byte_values, minlength=256)\n",
    "    probabilities = counts / np.sum(counts)\n",
    "    return entropy(probabilities, base=2)\n",
    "\n",
    "def hamming_distance(payload1, payload2):\n",
    "    \"\"\"Calculate Hamming distance between two 8-byte payloads.\"\"\"\n",
    "    # Convert payloads to binary strings\n",
    "    bin1 = ''.join(f\"{int(byte, 16):08b}\" for byte in payload1)\n",
    "    bin2 = ''.join(f\"{int(byte, 16):08b}\" for byte in payload2)\n",
    "\n",
    "    # Count differing bits\n",
    "    return sum(b1 != b2 for b1, b2 in zip(bin1, bin2))\n",
    "\n",
    "def compute_hamming_distances(dataframe, scaler, previous_x=1):\n",
    "    \"\"\"\n",
    "    Computes the Hamming distance between each row's payload and the previous X messages.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): CAN dataset with 'data' column.\n",
    "        previous_x (int): Number of previous messages to compare with.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series containing the Hamming distances.\n",
    "    \"\"\"\n",
    "    hamming_distances = []\n",
    "    data_columns = [f'data[{i}]' for i in range(8)]  # Extracted byte columns\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        if i < previous_x:\n",
    "            hamming_distances.append(0)  # No previous message to compare with\n",
    "        else:\n",
    "            prev_payload = dataframe.iloc[i - previous_x][data_columns].astype(str).tolist()\n",
    "            curr_payload = dataframe.iloc[i][data_columns].astype(str).tolist()\n",
    "            hamming_distances.append(hamming_distance(prev_payload, curr_payload))\n",
    "\n",
    "    dataframe['hamming_distance'] = hamming_distances\n",
    "    return dataframe\n",
    "\n",
    "def feature_selection_preparation(file_name, phase ,pre_dataframe = None, rows = None, ts_fresh = False, ts_fresh_parameters = None, ts_fresh_custom_features = None):\n",
    "    column_names_train = ['timestamp' , 'arbitration_id' , 'channel' , 'dlc', 'data' , 'ecu']\n",
    "    column_names_test = ['timestamp', 'arbitration_id', 'dlc', 'data']\n",
    "    if phase =='training':\n",
    "        #dataframe = pd.read_csv(file_name, parse_dates=['timestamp'], header =0, names=column_names_train, nrows = rows)\n",
    "        dataframe = pd.read_csv(file_name, header=0, names=column_names_train, nrows=rows, dtype={'timestamp': float})\n",
    "    elif phase == 'debug':\n",
    "        #print(\"check\")\n",
    "        dataframe = pre_dataframe  \n",
    "    elif phase == 'test':\n",
    "        #dataframe = pd.read_csv(file_name, parse_dates=['timestamp'], header =0, names=column_names_test + ['type'], nrows= rows)\n",
    "        dataframe = pd.read_csv(file_name, header=0, names=column_names_test + ['type'], nrows=rows, dtype={'arbitration_id': int})\n",
    "        # one hot encode type\n",
    "        dataframe['type'] = dataframe['type'].apply(lambda x: 0 if x == 'R' else 1)\n",
    "        # Print the count of 1s in the 'type' column\n",
    "        count_of_ones = dataframe['type'].sum()\n",
    "        print(f\"Anomalies in 'type' column: {count_of_ones}\")\n",
    "    else:        \n",
    "        print(\"invalid phase\")\n",
    "        return None\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    #print(dataframe.head(2))\n",
    "    dataframe = compute_temporal_features_tsfresh(dataframe, custom_fc_parameters= ts_fresh_custom_features, ts_fresh= ts_fresh)\n",
    "    #dataframe = compute_temporal_features(dataframe)\n",
    "    \n",
    "    dataframe = dataframe[dataframe['dlc'] == 8].reset_index(drop=True)\n",
    "\n",
    "    # Extract data to indiviual columns\n",
    "    data_columns = [f'data[{i}]' for i in range(8)]\n",
    "    dataframe[data_columns] = dataframe['data'].str.split(' ', expand=True).iloc[:, :8]\n",
    "\n",
    "    dataframe = compute_hamming_distances(dataframe, scaler,previous_x=5)  # Compare with the last 5 messages\n",
    "    \n",
    "    # Convert Data from Hexadecimal to Integers [0,255]\n",
    "    for col in data_columns:\n",
    "        dataframe[col] = dataframe[col].apply(lambda x: int(x, 16) if isinstance(x, str) else x)\n",
    "\n",
    "    # Entropy feature\n",
    "    dataframe['payload_entropy'] = dataframe.apply(calculate_entropy, axis=1)    \n",
    "\n",
    "    num_bits = 29  # Standard for CAN IDs\n",
    "    binary_encoded_ids = binary_encode_integers(dataframe['arbitration_id'].tolist(), num_bits)\n",
    "    binary_encoded_df = pd.DataFrame(binary_encoded_ids, columns=[f'bit_{i}' for i in range(num_bits)])\n",
    "\n",
    "    # Add binary-encoded IDs to the original DataFrame\n",
    "    #dataframe = dataframe.drop(columns=['arbitration_id']).reset_index(drop=True)\n",
    "    dataframe = pd.concat([binary_encoded_df, dataframe], axis=1)\n",
    "    \n",
    "    dataframe[data_columns] = scaler.fit_transform(dataframe[data_columns])\n",
    "    dataframe['timestamp'] = scaler.fit_transform(dataframe[['timestamp']])\n",
    "    dataframe['payload_entropy'] = scaler.fit_transform(dataframe[['payload_entropy']])\n",
    "    dataframe['hamming_distance'] = scaler.fit_transform(dataframe[['hamming_distance']])\n",
    "\n",
    "\n",
    "    # Create a combined feature column, ensuring everything is a float\n",
    "    #dataframe['features'] = dataframe.apply(lambda row: np.concatenate([row[[f'bit_{i}' for i in range(num_bits)]].values , row[data_columns].values]), axis=1)\n",
    "    if ts_fresh:\n",
    "        dataframe['features'] = dataframe.apply(\n",
    "            lambda row: np.concatenate([\n",
    "                row[[f'bit_{i}' for i in range(num_bits)]].values, \n",
    "                row[data_columns].values, \n",
    "                row[ts_fresh_parameters].values\n",
    "            ]), axis=1\n",
    "        )\n",
    "    else:\n",
    "        dataframe['features'] = dataframe.apply(\n",
    "        lambda row: np.concatenate(\n",
    "            [row[[f'bit_{i}' for i in range(num_bits)]].values,  # Binary data columns\n",
    "            row[data_columns].values,  # Other data columns\n",
    "            np.array([row['msg_frequency'], row['timestamp']])  # Additional features\n",
    "            ]), axis=1)\n",
    "        \n",
    "        # row['iat'], row['msg_frequency'],  row['rolling_mean_iat'] , row['rolling_std_iat'],\n",
    "        #   np.array([ row['payload_entropy'],   \n",
    "        #              row['hamming_distance']\n",
    "        #              ])]), axis=1)\n",
    "        #row['payload_entropy'], row['hamming_distance'], \n",
    "        \n",
    "    nan_counts = dataframe.isna().sum()\n",
    "    if nan_counts.any():\n",
    "        print(\"NaN values found:\\n\", nan_counts[nan_counts > 0])\n",
    "    print(np.any(np.isinf(dataframe.select_dtypes(include=[np.number]))))\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def create_sliding_windows(data, labels=None, window_size=5, stride=1, anomaly_window_ratio = 0.5):\n",
    "    # Generates sliding windows for both features and labels.\n",
    "    X = np.array([data[i:i+window_size] for i in range(0, len(data) - window_size + 1, stride)], dtype=np.float32)\n",
    "    print(\"Original window that works: \", X.shape)  # (991, 50, 40)\n",
    "    \"\"\"\n",
    "    # Create array with the same structure but with added features\n",
    "    num_window_features = 3\n",
    "    augmented_X = np.zeros((X.shape[0], X.shape[1], X.shape[2] + num_window_features), dtype=np.float32)\n",
    "\n",
    "    # Copy all original data (already normalized, so we keep as is)\n",
    "    augmented_X[:, :, :X.shape[2]] = X\n",
    "\n",
    "    # Calculate window features and add to each data point in that window\n",
    "    for i, window in enumerate(X):\n",
    "        # Split window into meaningful components\n",
    "        arbitration_ids = window[:, :29]  # First 29 bits (arbitration ID)\n",
    "        payloads = window[:, 29:37]       # Next 8 bits (payload)\n",
    "        entropy_values = window[:, 37]    # Next value (entropy)\n",
    "        hamming_distances = window[:, 38] # Next value (hamming distance)\n",
    "        timestamps = window[:, 39]        # Last value (timestamp)\n",
    "    \n",
    "        # Calculate window-level features\n",
    "        unique_arbitration_ids = len(np.unique(arbitration_ids, axis=0))\n",
    "        mean_entropy = np.mean(entropy_values)\n",
    "        var_entropy = np.var(entropy_values)\n",
    "        mean_hamming = np.mean(hamming_distances)\n",
    "        inter_arrival_times = np.diff(timestamps)\n",
    "        mean_inter_arrival = np.mean(inter_arrival_times) if len(inter_arrival_times) > 0 else 0\n",
    "        \n",
    "        # Store window features temporarily\n",
    "        window_features = np.array([\n",
    "            #unique_arbitration_ids,\n",
    "            mean_entropy,\n",
    "            #var_entropy,\n",
    "            mean_hamming,\n",
    "            mean_inter_arrival\n",
    "        ])\n",
    "\n",
    "        #print(\"mean entropy\", mean_entropy)\n",
    "        #print(\"mean IAT\", mean_inter_arrival)\n",
    "\n",
    "        # Add these window features to all data points in this window\n",
    "        for j in range(num_window_features):\n",
    "            augmented_X[i, :, X.shape[2] + j] = window_features[j]\n",
    "\n",
    "    print(\"Shape after adding window features:\", augmented_X.shape)\n",
    "    X = augmented_X\n",
    "    \"\"\"\n",
    "    \n",
    "    if labels is not None:\n",
    "        labels = labels.values\n",
    "\n",
    "        # Initialize an empty list to store the labels\n",
    "        y = []\n",
    "\n",
    "        # Define threshold: At least 50% of the window should contain 1s\n",
    "        threshold = anomaly_window_ratio * window_size\n",
    "\n",
    "        for i in range(0, len(labels) - window_size + 1, stride):\n",
    "            # Extract the current window of labels\n",
    "            window = labels[i:i+window_size]\n",
    "            # Check if there is at least one '1' in the window\n",
    "            if np.sum(window == 1) >= threshold:  # Count 1s and compare to threshold\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)  # If there's not enough 1s in window, mark this window as normal\n",
    "\n",
    "        # Convert the list to a numpy array\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "        count_of_ones = np.sum(y)\n",
    "        print(f\"Anomalies in 'y' array: {count_of_ones}\")\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    ids = ['0x101', '0x102', '0x103']\n",
    "    num_bits = 11  # Standard for CAN IDs\n",
    "    binary_encoded_ids = binary_encode(ids, num_bits)\n",
    "\n",
    "    # Print the result\n",
    "    print(binary_encoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensorflow(featureframe, labels=None, batch_size=32, window_size=5, stride=1, split_ratio = 0.8, window_anomaly_ratio = 0.5):\n",
    "    # Convert feature list to NumPy array\n",
    "    input_data = np.array(featureframe.tolist(), dtype=np.float32)\n",
    "    before_window_shape = input_data.shape\n",
    "\n",
    "    # Check if Train or Test dataframe\n",
    "    if labels is not None:\n",
    "        input_data, labels = create_sliding_windows(input_data, labels, window_size, stride, window_anomaly_ratio)\n",
    "        print(\"x before tensor\", len(input_data))\n",
    "        print(\"y before tensor\", len(labels))\n",
    "        # todo: check windows \n",
    "        labels = np.array(labels, dtype=np.float32)  # Ensure labels are NumPy arrays\n",
    "        model_input = tf.data.Dataset.from_tensor_slices((input_data, labels))\n",
    "\n",
    "        dataset_size = len(input_data)\n",
    "        test_size = int(dataset_size*split_ratio)\n",
    "\n",
    "        #Shuffle , decrease size of buffer for faster shuffling\n",
    "        #buffer_size=min(50000, dataset_size)\n",
    "        #model_input = model_input.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=False, seed= SEED)\n",
    "\n",
    "        # Split to train and anomaly threshold set\n",
    "        train_dataset = model_input.take(test_size)\n",
    "        val_dataset = model_input.skip(test_size)\n",
    "\n",
    "        # Batch\n",
    "        test_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        print(f\"Feature shape BEFORE sliding window: {before_window_shape}\")\n",
    "        print(f\"Feature shape AFTER sliding window: {input_data.shape}\")\n",
    "        return test_dataset, val_dataset  \n",
    "\n",
    "    else:\n",
    "        input_data = create_sliding_windows(input_data, labels=None, window_size=window_size, stride=stride)\n",
    "        model_input = tf.data.Dataset.from_tensor_slices(input_data)\n",
    "\n",
    "        # Get Size of training set\n",
    "        dataset_size = len(input_data)\n",
    "        train_size = int(dataset_size * split_ratio)\n",
    "        \n",
    "        #Shuffle , decrease size of buffer for faster shuffling\n",
    "        #buffer_size=min(1000, dataset_size)\n",
    "        #model_input = model_input.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True, seed= SEED)\n",
    "\n",
    "        # Split to train and anomaly threshold set\n",
    "        train_dataset = model_input.take(train_size)\n",
    "        val_dataset = model_input.skip(train_size)\n",
    "\n",
    "        # Batch\n",
    "        train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        print(f\"Feature shape BEFORE sliding window: {before_window_shape}\")\n",
    "        print(f\"Feature shape AFTER sliding window: {input_data.shape}\")\n",
    "        return train_dataset, val_dataset  \n",
    "\n",
    "    # Apply batching\n",
    "    model_input = model_input.batch(batch_size)\n",
    "    print(f\"Feature shape BEFORE sliding window: {before_window_shape}\")\n",
    "    print(f\"Feature shape AFTER sliding window: {input_data.shape}\")\n",
    "    print(f\"Successfully prepared model input data.\")\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(train_dataset, val_dataset, path=r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/new_Channel2_Full_43Features_WindowFeatures_1024Batch_50Window_5Stride.npz\"):\n",
    "    \"\"\"\n",
    "    Save TensorFlow datasets to disk, preserving their exact structure.\n",
    "    Works with batched or unbatched datasets with complex shapes.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: A TensorFlow dataset\n",
    "        val_dataset: A TensorFlow dataset\n",
    "        path: Path to save the file (will be converted to .npz)\n",
    "    \"\"\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Save specs for reconstruction\n",
    "    train_spec = tf.data.experimental.get_structure(train_dataset)\n",
    "    val_spec = tf.data.experimental.get_structure(val_dataset)\n",
    "    \n",
    "    # Create temporary folders for TFRecords\n",
    "    temp_train_path = os.path.join(os.path.dirname(path), \"temp_train\")\n",
    "    temp_val_path = os.path.join(os.path.dirname(path), \"temp_val\")\n",
    "    \n",
    "    # Save datasets to TFRecord format\n",
    "    tf.data.experimental.save(train_dataset, temp_train_path)\n",
    "    tf.data.experimental.save(val_dataset, temp_val_path)\n",
    "    \n",
    "    # Save metadata (paths and specs) to the specified file\n",
    "    np.savez_compressed(\n",
    "        path.replace('.pkl', ''),\n",
    "        train_path=np.array(temp_train_path),\n",
    "        val_path=np.array(temp_val_path),\n",
    "        train_spec_dtype=np.array(train_spec._dtype.as_numpy_dtype),  # Fix dtype saving\n",
    "        val_spec_dtype=np.array(val_spec._dtype.as_numpy_dtype),  # Fix dtype saving\n",
    "        train_spec_shape=np.array(train_spec._shape),\n",
    "        val_spec_shape=np.array(val_spec._shape)\n",
    "    )\n",
    "    \n",
    "    print(f\"Datasets saved. Metadata at {path.replace('.pkl', '.npz')}\")\n",
    "    print(f\"Train dataset at {temp_train_path}\")\n",
    "    print(f\"Val dataset at {temp_val_path}\")\n",
    "\n",
    "def load_datasets(path=r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/Channel2_Full_43Features_WindowFeatures_1024Batch_50Window_5Stride.npz\"):\n",
    "    \"\"\"\n",
    "    Load TensorFlow datasets from disk with their original structure preserved.\n",
    "    \n",
    "    Args:\n",
    "        path: Path where datasets were saved (will be converted to .npz)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "    metadata = np.load(path.replace('.pkl', '.npz'), allow_pickle=True)\n",
    "    # Extract paths from metadata (convert from numpy array to string)\n",
    "    train_path = metadata['train_path'].item()  # Convert numpy array to string\n",
    "    val_path = metadata['val_path'].item()\n",
    "\n",
    "    train_spec = tf.TensorSpec(\n",
    "        shape=tuple(metadata['train_spec_shape']),\n",
    "        dtype=metadata['train_spec_dtype'].item()  # Convert back to NumPy dtype\n",
    "    )\n",
    "\n",
    "    val_spec = tf.TensorSpec(\n",
    "        shape=tuple(metadata['val_spec_shape']),\n",
    "        dtype=metadata['val_spec_dtype'].item()  # Convert back to NumPy dtype\n",
    "    )\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = tf.data.experimental.load(train_path, train_spec)\n",
    "    val_dataset = tf.data.experimental.load(val_path, val_spec)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49866c78",
   "metadata": {},
   "source": [
    "Tsfresh parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fc_parameters = {\n",
    "    # Statistical Features\n",
    "    \"mean\": None,\n",
    "    \"skewness\": None,\n",
    "    \"kurtosis\": None,\n",
    "    \n",
    "    # Temporal Dependency Features\n",
    "    \"autocorrelation\": [{\"lag\": 1}],  # Detects repeated patterns (useful for replay attacks)\n",
    "    \"agg_autocorrelation\": [{\"f_agg\": \"mean\", \"maxlag\": 5}],  # Aggregate autocorrelation over multiple lags\n",
    "    \"agg_linear_trend\": [{\"attr\": \"slope\", \"chunk_len\": 5, \"f_agg\": \"mean\"}],  # Detects trends in timing\n",
    "    \n",
    "    # Frequency Features\n",
    "    \"fft_coefficient\": [{\"coeff\": 1, \"attr\": \"real\"}],  # Extract 1st FFT coefficient (frequency analysis)\n",
    "    \"fft_aggregated\": [{\"aggtype\": \"centroid\"}],  # Spectral centroid (detects abnormal frequency changes)\n",
    "\n",
    "    # Entropy Features (Detects randomness or spoofing)\n",
    "    \"approximate_entropy\": [{\"m\": 2, \"r\": 0.1}],  # Measures unpredictability (spoofed messages might increase entropy)\n",
    "    \"binned_entropy\": [{\"max_bins\": 10}],  # Useful for detecting shifts in data distribution\n",
    "\n",
    "    # Complexity & Energy-Based Features\n",
    "    \"cid_ce\": [{\"normalize\": True}],  # Measures signal complexity (can detect spoofing)\n",
    "    \"absolute_sum_of_changes\": None,  # Captures sudden payload/timing shifts (useful for injection attacks)\n",
    "    \"energy_ratio_by_chunks\": [{\"num_segments\": 5, \"segment_focus\": 2}],  # Detects anomalies in energy distribution\n",
    "\n",
    "    # Stationarity & Distribution-Based Features\n",
    "    \"augmented_dickey_fuller\": [{\"attr\": \"teststat\"}],  # Tests stationarity (useful for detecting sudden changes)\n",
    "    \"benford_correlation\": None,  # Checks if data follows natural distribution (detects synthetic spoofing)\n",
    "\n",
    "    }\n",
    "\n",
    "\"\"\"  \n",
    "    ALL Features extracted ATM, add parameters above in 'custom_fc_parameters':\n",
    "\n",
    "    'iat', 'msg_frequency', 'rolling_mean_iat', 'rolling_std_iat', \n",
    "    'value__binned_entropy__max_bins_10', 'value__skewness', \n",
    "    'value__autocorrelation__lag_1', 'value__agg_autocorrelation__f_agg_\"mean\"__maxlag_5', \n",
    "    'value__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', \n",
    "    'value__fft_coefficient__attr_\"real\"__coeff_1', \n",
    "    'value__approximate_entropy__m_2__r_0.1', 'value__binned_entropy__max_bins_10', \n",
    "    'value__benford_correlation'\n",
    "\"\"\"\n",
    "# List of selected tsfresh features\n",
    "tsfresh_features = [\n",
    "    'iat', 'msg_frequency', 'rolling_mean_iat', 'rolling_std_iat', \n",
    "    'value__binned_entropy__max_bins_10', 'value__skewness', \n",
    "    'value__autocorrelation__lag_1', 'value__agg_autocorrelation__f_agg_\"mean\"__maxlag_5', \n",
    "    'value__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', \n",
    "    'value__fft_coefficient__attr_\"real\"__coeff_1', \n",
    "    'value__approximate_entropy__m_2__r_0.1', 'value__binned_entropy__max_bins_10', \n",
    "    'value__benford_correlation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c046ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "LOAD_DATAFRAME = False\n",
    "TS_FRESH = False\n",
    "\n",
    "n_rows_train = 70000    # select how many rows to load. None if whole train datasset\n",
    "n_rows_test = 25000   # select how many rows to load. None if whole test datasset\n",
    "batch_size = 1024\n",
    "window_size = 50    # increase window size\n",
    "stride = 5           # increase stride as a buffer\n",
    "split_ratio = 0.8     # % of training data to use for training\n",
    "window_anomaly_ratio = 0.1   # For 1 anomaly per window do: 1 / window_size\n",
    "if LOAD_DATAFRAME:\n",
    "    # Load training data\n",
    "    processeddataframe = pd.read_pickle(processeddataframe_normal_pickle_path)\n",
    "    train_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size)\n",
    "\n",
    "    # Load test data\n",
    "    processeddataframe_test = pd.read_pickle(processeddataframe_DoS_pickle_path)\n",
    "    test_dataset = convert_to_tensorflow(processeddataframe_test['features'] ,processeddataframe_test['type'], batch_size= batch_size )\n",
    "else:\n",
    "    # Preprocess and load training data\n",
    "    processeddataframe = feature_selection_preparation(preprocessed_normal_channel2_csv_path, 'training', rows=n_rows_train, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    train_dataset, val_dataset = convert_to_tensorflow(processeddataframe['features'], batch_size= batch_size, window_size = window_size, stride = stride, split_ratio= split_ratio)\n",
    "\n",
    "    processeddataframe_test = feature_selection_preparation(preprocessed_new_Spoofing_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    test_dataset, test_threshold_dataset = convert_to_tensorflow(processeddataframe_test['features'], processeddataframe_test['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)\n",
    "\n",
    "    #processeddataframe_dos_baseline = feature_selection_preparation(preprocessed_DoS_channel2_csv_path, 'test', rows=n_rows_test, ts_fresh= TS_FRESH, ts_fresh_parameters= tsfresh_features, ts_fresh_custom_features= custom_fc_parameters)\n",
    "    #dos_test_dataset, dos_threshold_dataset = convert_to_tensorflow(processeddataframe_dos_baseline['features'], processeddataframe_dos_baseline['type'], batch_size = batch_size, window_size= window_size, stride=stride, window_anomaly_ratio = window_anomaly_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8eed2",
   "metadata": {},
   "source": [
    "**SAVE Dataset to npz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"/Users/SCHUGD/Desktop/MasterThesis/Dataset/Processed_Dataframes/Channel2_Full_43Features_WindowFeatures_1024Batch_50Window_5Stride.npz\"\n",
    "\n",
    "# Save your datasets\n",
    "save_datasets(train_dataset, val_dataset, path)\n",
    "\n",
    "# Later, load them back\n",
    "train_dataset_loaded, val_dataset_loaded = load_datasets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(processeddataframe.head(1))\n",
    "print(processeddataframe['features'].head(4))\n",
    "print(np.size(processeddataframe['features'][0]))\n",
    "# todo: check that all values normalized or there's nans in the sliding windows\n",
    "# todo: check whole processing of df\n",
    "# Assuming 'df' is your DataFrame\n",
    "processeddataframe['has_nan'] = processeddataframe['features'].apply(lambda x: any(pd.isna(x)) if isinstance(x, list) else np.nan)\n",
    "\n",
    "# Ensure all values in 'features' are lists before flattening\n",
    "valid_lists = [lst for lst in processeddataframe['features'] if isinstance(lst, list)]\n",
    "\n",
    "# Flatten the valid lists\n",
    "all_values = sum(valid_lists, [])\n",
    "\n",
    "# Check for NaN values\n",
    "has_nan = any(pd.isna(all_values))\n",
    "\n",
    "# Check for out-of-bounds values\n",
    "has_out_of_bounds = any(x < 0 or x > 1 for x in all_values if isinstance(x, (int, float)))\n",
    "\n",
    "print(f\"Contains NaN: {has_nan}\")\n",
    "print(f\"Contains values <0 or >1: {has_out_of_bounds}\")\n",
    "\n",
    "def check_dataset(dataset, dataset_name=\"Dataset\"):\n",
    "    all_values = []\n",
    "    \n",
    "    for batch in dataset:\n",
    "        # Convert TensorFlow tensors to NumPy\n",
    "        batch_np = batch.numpy() if isinstance(batch, tf.Tensor) else np.array(batch)\n",
    "        \n",
    "        # Flatten batch and store values\n",
    "        all_values.extend(batch_np.flatten())\n",
    "        #print(batch_np.flatten())\n",
    "        #print(len(all_values))\n",
    "    \n",
    "    # Perform checks\n",
    "    has_nan = np.isnan(all_values).any()\n",
    "    has_out_of_bounds = (np.array(all_values) < 0).any() or (np.array(all_values) > 1).any()\n",
    "    all_values = [lst if isinstance(lst, (list, np.ndarray)) else [lst] for lst in all_values]\n",
    "    all_same_size = all(len(lst) == len(all_values[0]) for lst in all_values)\n",
    "    # Final condition combining both checks\n",
    "    if not all_same_size:\n",
    "        print(\"Error: Lists have different sizes.\")\n",
    "    print(f\"{dataset_name} - Contains NaN: {has_nan}\")\n",
    "    print(f\"{dataset_name} - Contains values <0 or >1: {has_out_of_bounds}\")\n",
    "\n",
    "# Run checks for both datasets\n",
    "check_dataset(train_dataset, \"Train Dataset\")\n",
    "check_dataset(val_dataset, \"Validation Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processeddataframe.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70758afc",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6f1cb",
   "metadata": {},
   "source": [
    "**VAE SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamplesOld\")\n",
    "class VAE_multiplesamplesOld(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size, **kwargs):\n",
    "        super(VAE_multiplesamplesOld, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim)\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),  \n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(window_size * input_dim, activation='sigmoid'),  \n",
    "            layers.Reshape((window_size, input_dim))  \n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamplesOld, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"Base_VAE_multiplesamples\")\n",
    "class Base_VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size, **kwargs):\n",
    "        super(Base_VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True), \n",
    "            layers.BatchNormalization(), # Experiment: BN\n",
    "            layers.LSTM(64, activation='relu', return_sequences= False),\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size),  \n",
    "            layers.LSTM(64, activation='relu', return_sequences=True),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "\n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(Base_VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_multiplesamples\")\n",
    "class VAE_multiplesamples(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_multiplesamples, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(84, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(84, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## -TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements -TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_multiplesamples, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_weak_generator\")\n",
    "class VAE_weakGenerator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_weakGenerator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(84, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(32, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## -TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements -TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_weakGenerator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x\")\n",
    "class VAE_2x(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(120, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(120, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## -TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements -TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"VAE_2x_weak_generator\")\n",
    "class VAE_2x_weak_generator(Model):\n",
    "    def __init__(self, input_dim, latent_dim, window_size ,**kwargs):\n",
    "        super(VAE_2x_weak_generator, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.window_size = window_size  \n",
    "\n",
    "        ## Standard VAE ##\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(window_size, input_dim)),  # Sequence input shape (window_size, input_dim) Experiment: HE Init\n",
    "            layers.LSTM(120, activation='relu', return_sequences=True), \n",
    "            #layers.BatchNormalization(), # Experiment: BN\n",
    "            #layers.LSTM(180, activation='relu', return_sequences= True),\n",
    "            #layers.LSTM(80, activation='relu', return_sequences= True), # return sequence?\n",
    "            #layers.Flatten()#layers.flatten 32* 50 , if this is removed u need to uncomment pooling in encode()\n",
    "            layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)  # Latent mean  make input smaller than latent dim?\n",
    "        self.fc_logvar = layers.Dense(latent_dim)  # Latent log variance\n",
    "\n",
    "        # Decoder with LSTM layers                   # Weaker Generator --> Better Encoder --> Better Latent Space?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.RepeatVector(window_size), \n",
    "            layers.LSTM(32, activation = 'relu', return_sequences = True), \n",
    "            #layers.LSTM(80, activation='relu', return_sequences=True),\n",
    "            #layers.BatchNormalization(),\n",
    "            #layers.LSTM(120, activation='relu', return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(input_dim, activation='sigmoid'))  # Output must match (window_size, input_dim)\n",
    "        ])\n",
    "        #####################################################################################\n",
    "        ## FactorVAE ##\n",
    "        # Discriminator for TC estimation\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "                layers.InputLayer(shape=(latent_dim,)),\n",
    "                #layers.Dense(512),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(256),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                \n",
    "                #layers.Dense(128),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dropout(0.2),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                layers.Dense(1500),\n",
    "                layers.LeakyReLU(0.2),\n",
    "                #layers.Dense(1)\n",
    "                layers.Dense(2)\n",
    "                #layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        #####################################################################################\n",
    "\n",
    "    ## Standard VAE Functions ##\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        #hidden = tf.reduce_mean(hidden, axis=1)  # Mean over time (batch_size, hidden_dim) REMOVE\n",
    "        \n",
    "        #hidden_pooled = tf.reduce_max(hidden, axis=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        logvar = tf.clip_by_value(logvar, -10, 5)  # Prevents extreme values UNSTABLE FIX\n",
    "        #print(logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar, n_samples=1):\n",
    "        eps = tf.random.normal(shape=(n_samples, tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "        #std = tf.exp(0.5 * logvar)\n",
    "        std = tf.sqrt(tf.nn.softplus(logvar))  # More stable alternative to exp UNSTABLE FIX\n",
    "        mu = tf.expand_dims(mu, axis=0)  # Expand for n_samples\n",
    "        std = tf.expand_dims(std, axis=0)\n",
    "        \n",
    "        #print(std)\n",
    "        return mu + eps * std  # Shape: (n_samples, batch_size, latent_dim)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        # Flatten the samples for decoding\n",
    "        z_flat = tf.reshape(z, (-1, tf.shape(z)[-1]))  # (n_samples * batch_size, latent_dim)\n",
    "        reconstructed_flat = self.decoder(z_flat)\n",
    "        # Reshape back to (n_samples, batch_size, window_size, input_dim)\n",
    "        return tf.reshape(reconstructed_flat, (tf.shape(z)[0], tf.shape(z)[1], self.window_size, self.input_dim))\n",
    "\n",
    "    def call(self, x, n_samples=1, latent_only = False):\n",
    "        if latent_only:\n",
    "            mu, logvar = self.encode(x)\n",
    "            return _ , mu, logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    #####################################################################################\n",
    "    ## FactorVAE Functions ##\n",
    "    \"\"\"\n",
    "    Following section implements FactorVAE based on:\n",
    "\n",
    "    \"\"\"\n",
    "    def permute_dims(self, z):\n",
    "        \"\"\"Permutes the batch dimension to estimate Total Correlation\"\"\"\n",
    "        \"\"\"Permutes each latent dimension independently to estimate Total Correlation\"\"\"\n",
    "        B, D = tf.shape(z)[0], tf.shape(z)[1]  # Batch size, Latent dimension\n",
    "        z_perm = tf.zeros_like(z)\n",
    "        for j in range(D):\n",
    "            perm = tf.random.shuffle(tf.range(B))\n",
    "            z_perm = tf.tensor_scatter_nd_update(z_perm, tf.stack([tf.range(B), tf.tile([j], [B])], axis=1), tf.gather(z[:, j], perm))\n",
    "        return z_perm\n",
    "    \n",
    "    def discriminator_loss(self, z, z_perm):\n",
    "        \"\"\"Compute discriminator loss for TC estimation using Dense(2)\"\"\"\n",
    "        real_logits = self.discriminator(z)  # Shape: (batch_size, 2)\n",
    "        fake_logits = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        real_labels = tf.zeros((tf.shape(real_logits)[0],), dtype=tf.int32)  # Class 0 for real samples\n",
    "        fake_labels = tf.ones((tf.shape(fake_logits)[0],), dtype=tf.int32)   # Class 1 for permuted samples\n",
    "        \n",
    "        real_loss = tf.keras.losses.sparse_categorical_crossentropy(real_labels, real_logits, from_logits=True)\n",
    "        fake_loss = tf.keras.losses.sparse_categorical_crossentropy(fake_labels, fake_logits, from_logits=True)\n",
    "        \n",
    "        return 0.5 * (tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss))\n",
    "    \n",
    "    def tc_loss(self, z):\n",
    "        \"\"\"Estimate Total Correlation using the discriminator\"\"\"\n",
    "        logits = self.discriminator(z)\n",
    "        #logits_perm = self.discriminator(self.permute_dims(z))\n",
    "        # Density ratio trick\n",
    "        #tc = tf.reduce_mean(logits_real - logits_perm)\n",
    "        #tc_2 = tf.reduce_mean(logits[:, :1] - logits[:,1:])\n",
    "        tc = tf.reduce_mean(logits[:,0] - logits[:,1]) # correct?\n",
    "        return tc\n",
    "    \n",
    "    def discriminator_acc(self, z):\n",
    "        # Permute dimensions to create \"fake\" samples\n",
    "        z_perm = self.permute_dims(z)\n",
    "\n",
    "        # Get discriminator outputs (logits for two classes)\n",
    "        logits_real = self.discriminator(z)       # Shape: (batch_size, 2)\n",
    "        logits_perm = self.discriminator(z_perm)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute predicted class (0 = real, 1 = permuted)\n",
    "        preds_real = tf.argmax(logits_real, axis=1)\n",
    "        preds_perm = tf.argmax(logits_perm, axis=1)\n",
    "\n",
    "        # Compute accuracy: real samples should be classified as 0, permuted as 1\n",
    "        acc_real = tf.reduce_mean(tf.cast(tf.equal(preds_real, 0), tf.float32))\n",
    "        acc_perm = tf.reduce_mean(tf.cast(tf.equal(preds_perm, 1), tf.float32))\n",
    "\n",
    "        # Average accuracy\n",
    "        discriminator_accuracy = 0.5 * (acc_real + acc_perm)\n",
    "\n",
    "        return discriminator_accuracy\n",
    "    #####################################################################################\n",
    "    ## -TCVAE ##\n",
    "    \"\"\"\n",
    "    Following section implements -TCVAE based on:\n",
    "\n",
    "    \"Isolating Sources of Disentanglement in Variational Autoencoders\"\n",
    "    (https://arxiv.org/pdf/1802.04942).\n",
    "\n",
    "    if gamma = alpha = 1 , the original function can be rewritten to only\n",
    "    calculate the Total Correlation similar to FactorVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def gaussian_log_density(self,samples, mean, log_squared_scale):\n",
    "        pi = tf.constant(np.pi)\n",
    "        normalization = tf.math.log(2. * pi)\n",
    "        #inv_sigma = tf.math.exp(-log_squared_scale)\n",
    "            # Use the same transformation as in reparameterize\n",
    "        var = tf.nn.softplus(log_squared_scale)  # Match the softplus used in reparameterize\n",
    "        inv_sigma = 1.0 / var\n",
    "        tmp = (samples - mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + log_squared_scale + normalization)\n",
    "\n",
    "    def b_tcvae_total_correlation_loss(self,z, z_mean, z_log_squared_scale):\n",
    "        \"\"\"Estimate of total correlation on a batch.\n",
    "        We need to compute the expectation over a batch of: E_j [log(q(z(x_j))) -\n",
    "        log(prod_l q(z(x_j)_l))]. We ignore the constants as they do not matter\n",
    "        for the minimization. The constant should be equal to (num_latents - 1) *\n",
    "        log(batch_size * dataset_size)\n",
    "        Args:\n",
    "        z: [batch_size, num_latents]-tensor with sampled representation.\n",
    "        z_mean: [batch_size, num_latents]-tensor with mean of the encoder.\n",
    "        z_log_squared_scale: [batch_size, num_latents]-tensor with log variance of the encoder.\n",
    "        Returns:\n",
    "        Total correlation estimated on a batch.\n",
    "        \"\"\"\n",
    "        #print(\"Z shape: \", z.shape)\n",
    "        #print(\"z_mean shape: \", z_mean.shape)\n",
    "        #print(\"z_logvar shape: \", z_log_squared_scale.shape)\n",
    "        # Compute log(q(z(x_j)|x_i)) for every sample in the batch, which is a\n",
    "        # tensor of size [batch_size, batch_size, num_latents]. In the following\n",
    "        log_qz_prob = self.gaussian_log_density(\n",
    "            tf.expand_dims(z, 1), tf.expand_dims(z_mean, 0),\n",
    "            tf.expand_dims(z_log_squared_scale, 0))\n",
    "        #print(\"[batch_size, batch_size, num_latents]\",log_qz_prob.shape)\n",
    "\n",
    "        # Compute log prod_l p(z(x_j)_l) = sum_l(log(sum_i(q(z(z_j)_l|x_i)))\n",
    "        # + constant) for each sample in the batch, which is a vector of size\n",
    "        # [batch_size,].\n",
    "        log_qz_product = tf.math.reduce_sum(\n",
    "            tf.math.reduce_logsumexp(log_qz_prob, axis=1, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        #print(\"[batch_size,]\",log_qz_product.shape)\n",
    "\n",
    "        # Compute log(q(z(x_j))) as log(sum_i(q(z(x_j)|x_i))) + constant =\n",
    "        # log(sum_i(prod_l q(z(x_j)_l|x_i))) + constant.\n",
    "        log_qz = tf.math.reduce_logsumexp(\n",
    "            tf.math.reduce_sum(log_qz_prob, axis=2, keepdims=False),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "        return tf.math.reduce_mean(log_qz - log_qz_product)\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Return the config of the model including custom parameters\n",
    "        config = super(VAE_2x_weak_generator, self).get_config()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,  # Input dimension\n",
    "            'latent_dim': self.fc_mu.units,  # Latent dimension\n",
    "            'window_size': self.window_size  # Window size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(input_dim=config['input_dim'], latent_dim=config['latent_dim'], window_size=config['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2d2d4",
   "metadata": {},
   "source": [
    "Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(vae,optimizer,discriminator_optimizer, epochs, n_samples, input_dim, latent_dim, batch_size,beta, gamma, n_critic, steps_anneal, patience, time,beta_tc = 0, validation_method = 'None', model_path = \"\"):   \n",
    "    wait = 0  \n",
    "    model_name =\"LSTM_VAE\"\n",
    "\n",
    "    print(f\"Latent Dimension = {latent_dim}, \"\n",
    "        f\"Beta = {beta}, Gamma = {gamma}, N_critic = {n_critic}, Beta_TC = {beta_tc}, Validation Method = {validation_method}, \"\n",
    "        f\"Rows in Training Data = {n_rows_train}, \"\n",
    "        f\"Batch Size = {batch_size}\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    disc_losses = []\n",
    "    beta_tc_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    real_epochs = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        real_epochs += 1\n",
    "        epoch_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        epoch_beta_tc_loss = 0\n",
    "        for step, batch in enumerate(train_dataset): # window size , features\n",
    "            global_step = epoch * len(train_dataset) + step  # Total training step count\n",
    "            # First get latent representations\n",
    "            #_, mu, logvar = vae(batch, n_samples=1, latent_only=True)\n",
    "            #z = vae.reparameterize(mu, logvar, n_samples=1)\n",
    "            #z = tf.squeeze(z, axis=0)\n",
    "            #print(batch.shape)\n",
    "\n",
    "            # Split the batch according to FactorVAE Paper\n",
    "            anneal_coeff = linear_annealing(0, 1, global_step, steps_anneal)\n",
    "\n",
    "            if gamma > 0 and n_critic > 0:\n",
    "\n",
    "                half_batch_size = tf.shape(batch)[0] // 2\n",
    "                first_half_batch = batch[:half_batch_size]\n",
    "                second_half_batch = batch[half_batch_size:]\n",
    "\n",
    "                # Compute Annealing Coefficient\n",
    "                #print(anneal_coeff)\n",
    "            else:\n",
    "                first_half_batch = batch\n",
    "\n",
    "            # 1. Train VAE\n",
    "            with tf.GradientTape() as tape:\n",
    "                reconstructed, mu, logvar = vae(first_half_batch, n_samples=n_samples, latent_only = False)  # Use multiple samples\n",
    "\n",
    "                #Compute reconstruction MSE error for each sample\n",
    "                reconstruction_errors = tf.reduce_mean(\n",
    "                    tf.square(tf.expand_dims(first_half_batch, axis=0) - reconstructed), axis=-1\n",
    "                )  # Shape: (n_samples, batch_size, window_size)\n",
    "\n",
    "                # Aggregate errors (mean over samples and window_size)\n",
    "                mean_reconstruction_error = tf.reduce_mean(reconstruction_errors, axis=(2, 0))\n",
    "\n",
    "                # Compute VAE loss\n",
    "                reconstruction_loss = tf.reduce_mean(mean_reconstruction_error)\n",
    "\n",
    "                #print(f\"Final loss {reconstruction_loss}\")\n",
    "                kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "                                \n",
    "                if gamma > 0:\n",
    "                    # Get latent samples for TC loss\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, axis=0)  # Remove sample dimension for discriminator\n",
    "                    \n",
    "                    # Total Correlation loss\n",
    "                    tc = vae.tc_loss(z)\n",
    "                    loss = reconstruction_loss + beta * kl_divergence + gamma*tc*anneal_coeff\n",
    "                elif beta_tc > 0:\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, 0) # aggregate over latent samples\n",
    "                    #print(\"z shape directly after reparam: \", z.shape)\n",
    "                    tc_beta_loss = vae.b_tcvae_total_correlation_loss(z, mu, logvar)\n",
    "                    b_tcvae_loss = (1 - beta_tc) * tc_beta_loss\n",
    "                    #print(\"tc beta\" ,tc_beta_loss.numpy())\n",
    "                    #print(\"tc beta * b_tc\", b_tcvae_loss.numpy())\n",
    "                    #print(\"anneal\" , anneal_coeff.numpy())\n",
    "                    #print(\" tc beta * anneal\", (tc_beta_loss * anneal_coeff).numpy())\n",
    "\n",
    "                    #print(\"recon\", reconstruction_loss.numpy() )\n",
    "                    #print(\"kl\", kl_divergence.numpy())\n",
    "                    loss = reconstruction_loss + kl_divergence + b_tcvae_loss\n",
    "                else:\n",
    "                    #print(reconstruction_loss)\n",
    "                    #print(kl_divergence)\n",
    "                    loss = reconstruction_loss + beta * kl_divergence\n",
    "\n",
    "\n",
    "\n",
    "            gradients = tape.gradient(loss, vae.encoder.trainable_variables + vae.decoder.trainable_variables)\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "            optimizer.apply_gradients(zip(gradients, vae.encoder.trainable_variables + vae.decoder.trainable_variables))\n",
    "\n",
    "            epoch_loss += loss.numpy()\n",
    "            #epoch_beta_tc_loss += b_tcvae_loss.numpy()\n",
    "            # 2. Train discriminator\n",
    "            disc_loss = 0\n",
    "            if gamma > 0 and n_critic >0:\n",
    "                for _ in range(n_critic):\n",
    "                    # Discriminator update\n",
    "                    with tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        # First get latent representations on second half batch\n",
    "                        _, mu, logvar = vae(second_half_batch, n_samples=n_samples, latent_only=True)\n",
    "                        z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                        z = tf.reduce_mean(z, axis=0)\n",
    "                        z_perm = vae.permute_dims(z)\n",
    "                        current_disc_loss = vae.discriminator_loss(z, z_perm)\n",
    "                        disc_loss += current_disc_loss\n",
    "\n",
    "                    # Update discriminator\n",
    "                    disc_gradients = disc_tape.gradient(current_disc_loss, vae.discriminator.trainable_variables)\n",
    "                    # Clip gradients for stability\n",
    "                    disc_gradients, _ = tf.clip_by_global_norm(disc_gradients, 1.0)\n",
    "                    discriminator_optimizer.apply_gradients(zip(disc_gradients, vae.discriminator.trainable_variables))\n",
    "\n",
    "                disc_loss = disc_loss / n_critic\n",
    "                epoch_disc_loss += disc_loss\n",
    "        \n",
    "        # VALIDATION\n",
    "        epoch_val_loss = 0\n",
    "        total_disc_acc = 0\n",
    "        show_val = validation_method in [\"B_VAE\",\"B_TCVAE\", \"TC\"]\n",
    "        latent_only = validation_method == \"TC\"\n",
    "\n",
    "        if validation_method == \"None\" or validation_method == \"PLOT\":\n",
    "            val_loss = 0\n",
    "            avg_disc_acc = 0\n",
    "            early_stop = False\n",
    "        else:\n",
    "            early_stop = True\n",
    "            for batch in val_dataset:\n",
    "                reconstructed, mu, logvar = vae(batch, n_samples=n_samples, latent_only=latent_only)\n",
    "\n",
    "                if validation_method == \"B_VAE\":\n",
    "                    reconstruction_errors = tf.reduce_mean(\n",
    "                        tf.square(tf.expand_dims(batch, axis=0) - reconstructed), axis=-1\n",
    "                    )\n",
    "                    mean_reconstruction_error = tf.reduce_mean(reconstruction_errors, axis=(2, 0))\n",
    "                    reconstruction_loss = tf.reduce_mean(mean_reconstruction_error)\n",
    "                    kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "\n",
    "                    val_loss = reconstruction_loss + beta * kl_divergence\n",
    "                    epoch_val_loss += val_loss.numpy()\n",
    "                elif validation_method == \"B_TCVAE\":\n",
    "                    reconstruction_errors = tf.reduce_mean(\n",
    "                        tf.square(tf.expand_dims(batch, axis=0) - reconstructed), axis=-1\n",
    "                    )\n",
    "                    mean_reconstruction_error = tf.reduce_mean(reconstruction_errors, axis=(2, 0))\n",
    "                    reconstruction_loss = tf.reduce_mean(mean_reconstruction_error)\n",
    "                    kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, axis= 0)\n",
    "                    tc_beta_loss = vae.b_tcvae_total_correlation_loss(z, mu, logvar)\n",
    "                    b_tcvae_loss = (1 - beta_tc) * tc_beta_loss\n",
    "                    \n",
    "                    val_loss = reconstruction_loss + kl_divergence + b_tcvae_loss\n",
    "\n",
    "                    epoch_val_loss += val_loss.numpy()\n",
    "                elif validation_method == \"TC\":\n",
    "                    z = vae.reparameterize(mu, logvar, n_samples=n_samples)\n",
    "                    z = tf.reduce_mean(z, axis= 0)\n",
    "                    tc = vae.tc_loss(z)\n",
    "                    \n",
    "                    disc_acc = vae.discriminator_acc(z)\n",
    "                    #disc_acc = 0  # Assuming this is optional or calculated elsewhere\n",
    "                    total_disc_acc += disc_acc\n",
    "                    val_loss = gamma * tc\n",
    "                    epoch_val_loss += val_loss.numpy()\n",
    "\n",
    "        if validation_method == \"PLOT\" and epoch % 50 == 0 and epoch > 0:\n",
    "            print(\"PLOT AT EPOCH: \", {epoch + 1})\n",
    "            get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "\n",
    "\n",
    "        # Store the loss for this epoch\n",
    "        train_loss = epoch_loss / len(train_dataset)\n",
    "        val_loss = epoch_val_loss / len(val_dataset) \n",
    "        disc_loss = epoch_disc_loss / len(train_dataset)\n",
    "        avg_disc_acc = total_disc_acc / len(val_dataset)\n",
    "        beta_tc_loss = epoch_beta_tc_loss / len(train_dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        disc_losses.append(disc_loss)\n",
    "        beta_tc_losses.append(beta_tc_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.6f}, \"\n",
    "            f\"Discriminator Loss: {disc_loss:.6f} Disc Acc: {avg_disc_acc:.4f}, Beta TC Loss: {beta_tc_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if early_stop:\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                wait = 0\n",
    "                # Save best model\n",
    "                best_model_path = model_path\n",
    "                vae.compile(optimizer = optimizer)\n",
    "                vae.save(best_model_path)\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    epochs = epoch + 1\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        # Save the trained model\n",
    "        #vae.compile(optimizer = optimizer)\n",
    "        #vae.save(model_path)\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            save_trained_model(vae, optimizer, model_path, model_name, latent_dim,beta,n_rows_train,time, AWS = AWS, s3 = s3 , BUCKET = BUCKET)\n",
    "            print( f'Saved Model at Epoch {epoch+1}')\n",
    "\n",
    "    #Save the trained model\n",
    "    #vae.save(model_path)\n",
    "\n",
    "    print(\"Loss Plot Saved.\")\n",
    "    print(\"VAE model saved.\")\n",
    "    #vae.summary()\n",
    "    return train_losses, val_losses, real_epochs, time, show_val, model_path, vae\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863771e",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99324f19",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9a54",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 2\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-03\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"None\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 5  \n",
    "beta = 1 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"LSTM_VAE\"\n",
    "model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path= model_path)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,model_name,'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = True\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path,reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time, AWS = AWS, s3=s3, BUCKET = BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e510e",
   "metadata": {},
   "source": [
    "**Test Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_path = \"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_LSTM_VAE_LD30_Beta25_NT800000_21-37.keras\"\n",
    "#loaded_vae = keras.models.load_model(\"/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_BEST_VAE_11-15.keras\")\n",
    "loaded_vae = keras.models.load_model(model_path)\n",
    "loaded_vae.trainable = False  # Freeze model weights\n",
    "\n",
    "get_latent_representations_label(loaded_vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010100",
   "metadata": {},
   "source": [
    "**HyperParamter Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(anomaly_detection_functions)\n",
    "# Regular HYPERPARAMETERS \n",
    "input_dim = np.size(processeddataframe['features'][0])\n",
    "#input_dim = 42\n",
    "epochs = 80\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"None\" # None, B_VAE, TC, B_TCVAE\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 2  \n",
    "beta = 1 # 20\n",
    "beta_tc = 0 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "latent_dims = [9,14, 20,25]  # Example values for latent dimension\n",
    "beta_values = [1, 7, 15,30]\n",
    "\n",
    "\n",
    "# Iterate over all combinations\n",
    "for latent_dim, beta in itertools.product(latent_dims, beta_values):\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"BASE_LSTM_VAE\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "\n",
    "    print(f\"Training with: latent_dim={latent_dim}, beta={beta} validation_method={validation_method}\")\n",
    "\n",
    "    vae = VAE_multiplesamples(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_weakGen = VAE_weakGenerator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    vae_2x = VAE_2x(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "    VAE_2x_weakGen = VAE_2x_weak_generator(input_dim=input_dim, latent_dim=latent_dim, window_size= window_size)\n",
    "\n",
    "        #optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_weakGen = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_2x = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    optimizer_2x_weakGen = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "    print(\"VAE--------------\")\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                        n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                        gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                        model_path= model_path)\n",
    "\n",
    "\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    print(\"VAE WEAK GENERATOR-------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_weakGen = train_model(vae_weakGen,optimizer_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                        n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                        gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                        model_path= model_path)\n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    print(\"VAE 2X----------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, vae_2x = train_model(vae_2x,optimizer_2x,discriminator_optimizer, epochs,\n",
    "                                                                        n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                        gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                        model_path= model_path)\n",
    "    \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,model_name,type = 'TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    print(\"VAE 2X WEAK GENERATOR------------------\")\n",
    "    time = datetime.now().strftime(\"%H-%M\")\n",
    "    model_name =\"LSTM_VAE_2x_WEAKGEN\"\n",
    "    model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/BEST_{model_name}_LD{latent_dim}_Beta{beta}_NT{n_rows_train}_{time}.keras'\n",
    "    train_losses, val_losses, real_epochs, time, show_val, model_path, VAE_2x_weakGen = train_model(VAE_2x_weakGen,optimizer_2x_weakGen,discriminator_optimizer, epochs,\n",
    "                                                                        n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                        gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                        model_path= model_path)\n",
    "   \n",
    "    plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time,n_rows_train, show_val= show_val, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,model_name,type='TSNE', save = True, AWS = AWS, s3 = s3, BUCKET = BUCKET)\n",
    "    print(\"-----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dea3cc",
   "metadata": {},
   "source": [
    "**Iterative Training on Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular HYPERPARAMETERS \n",
    "#input_dim = np.size(processeddataframe['features'][0])\n",
    "input_dim = 42\n",
    "epochs = 50\n",
    "n_samples = 1\n",
    "# Best 512 settigns: AdamW with LR=1e-05, WD=1e-06, Beta1=0.85, Beta2=0.98  \n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-06\n",
    "# FactorVAE\n",
    "learning_rate_disc = 5e-5\n",
    "# Annealing and Early stop\n",
    "steps_anneal = epochs * len(train_dataset)  \n",
    "alpha = 0.0  # Minimum learning rate as a fraction of the initial learning rate\n",
    "validation_method = \"PLOT\" # None, B_VAE, TC, B_TCVAE, PLOT\n",
    "patience = 5  \n",
    "\n",
    "# IMPORTANT \n",
    "latent_dim= 30  \n",
    "beta = 0 # 20\n",
    "beta_tc = 1.008 #1.008 # keep in mind this is tuned based on that we get tc loss = -244..\n",
    "\n",
    "gamma = 0  # TC weight (typically between 1 and 10) 6\n",
    "n_critic = 0\n",
    "##################\n",
    "\n",
    "AD = False\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M\")\n",
    "model_name =\"BEST_VAE\"\n",
    "new_model_path = f'/Users/SCHUGD/Desktop/MasterThesis/Code/Models/Iter_BEST_{model_name}_{time}.keras'\n",
    "\n",
    "cosine_decay_schedule = CosineDecay(\n",
    "    1e-03, steps_anneal, alpha=alpha\n",
    ")\n",
    "\n",
    "\n",
    "vae = keras.models.load_model(best_model_path)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay, beta_1=beta1, beta_2=beta2)\n",
    "#optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=weight_decay)\n",
    "optimizer = vae.optimizer\n",
    "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate_disc)\n",
    "\n",
    "train_losses, val_losses, real_epochs, time, show_val, model_path, vae = train_model(vae,optimizer,discriminator_optimizer, epochs,\n",
    "                                                                    n_samples, input_dim, latent_dim, batch_size,beta,\n",
    "                                                                      gamma, n_critic, steps_anneal, patience, time,beta_tc,validation_method,\n",
    "                                                                      model_path = new_model_path)\n",
    "\n",
    "\n",
    "plot_loss_curve(train_losses, val_losses , real_epochs, latent_dim, time, show_val= show_val)\n",
    "#analyze_latent_variance(vae,train_dataset, test_dataset)\n",
    "#analyze_kl_divergence(vae, train_dataset, test_dataset)\n",
    "#get_latent_representations_label(vae, test_dataset,latent_dim, beta,n_critic,gamma,time, 'PCA', save = False)\n",
    "get_latent_representations_label(vae, test_dataset, latent_dim, beta,n_critic,gamma,time,'TSNE', save = True)\n",
    "#get_latent_representations_label(vae, train_dataset, 'TSNE')\n",
    "\n",
    "if AD:\n",
    "  reconstruction_AD = False\n",
    "  latent_AD = True\n",
    "  reconstruction_threshold, latent_threshold, mean_train, variance_train = get_threshold_from_train(model_path, reconstruction_AD, latent_AD)\n",
    "  results, distances = anomaly_detection(vae, reconstruction_AD, latent_AD, mean_train, variance_train)\n",
    "  reconstruction_accuracy , latent_accuracy = get_anomaly_detection_accuracy(reconstruction_AD, latent_AD, results,reconstruction_threshold,distances,latent_threshold,model_name, latent_dim,epochs,time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
